{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "with open(r'C:\\Users\\WMT\\Downloads\\anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "vocab = sorted(set(text))#set将文章中的所有不同字符取出，然后sorted排序\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}#排好序的字符列表进行字典索引\n",
    "int_to_vocab = dict(enumerate(vocab))#与上字典相反，索引号为键，字符为值\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)#把text中所有字符进行数字编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985223\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "\n",
    "    # 用sequence和step计算batch大小，得出batch个数，最后不够一个batch的扔掉\n",
    "    characters_per_batch = n_seqs * n_steps\n",
    "    n_batches = len(arr)//characters_per_batch\n",
    "    arr = arr[:n_batches * characters_per_batch]\n",
    "\n",
    "    # 重新reshape为sequence行，列数自动生成（-1）\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "\n",
    "    # 生成样本特征batch及目标值batch（目标值为样本值的下一个字母）\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        # 目标值往下滚动一个字母，目标batch最后一列可设置为样本特征batch的第一列，不会影响精度\n",
    "        y[:, :-1], y[:,-1] = x[:, 1:], x[:, 0]\n",
    "\n",
    "        # x,y为生成器（generater）\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    '''batch_size是每个batch中sequence的长度（batch行数）\n",
    "        num_steps是batch列数\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs, targets, keep_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "\n",
    "    # 创建LSTM单元\n",
    "\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "\n",
    "    # reshape\n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "\n",
    "    # 将RNN输入连接到softmax层\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, lstm_size=128,\n",
    "                num_layers=2, learning_rate=0.001, grad_clip=5, sampling=False):\n",
    "\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers =2\n",
    "learning_rate = 0.0001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20... Training Step:1... Training loss:4.4201... 5.2489 sec/batch\n",
      "Epoch:1/20... Training Step:2... Training loss:4.4114... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:3... Training loss:4.4018... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:4... Training loss:4.3921... 0.4099 sec/batch\n",
      "Epoch:1/20... Training Step:5... Training loss:4.3817... 0.3930 sec/batch\n",
      "Epoch:1/20... Training Step:6... Training loss:4.3706... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:7... Training loss:4.3553... 0.4039 sec/batch\n",
      "Epoch:1/20... Training Step:8... Training loss:4.3359... 0.3959 sec/batch\n",
      "Epoch:1/20... Training Step:9... Training loss:4.3117... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:10... Training loss:4.2797... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:11... Training loss:4.2364... 0.3940 sec/batch\n",
      "Epoch:1/20... Training Step:12... Training loss:4.1823... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:13... Training loss:4.0914... 0.3820 sec/batch\n",
      "Epoch:1/20... Training Step:14... Training loss:3.9722... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:15... Training loss:3.7914... 0.3870 sec/batch\n",
      "Epoch:1/20... Training Step:16... Training loss:3.6173... 0.3840 sec/batch\n",
      "Epoch:1/20... Training Step:17... Training loss:3.6310... 0.3930 sec/batch\n",
      "Epoch:1/20... Training Step:18... Training loss:3.7537... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:19... Training loss:3.6805... 0.3960 sec/batch\n",
      "Epoch:1/20... Training Step:20... Training loss:3.5178... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:21... Training loss:3.4917... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:22... Training loss:3.4523... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:23... Training loss:3.4652... 0.3959 sec/batch\n",
      "Epoch:1/20... Training Step:24... Training loss:3.4868... 0.3860 sec/batch\n",
      "Epoch:1/20... Training Step:25... Training loss:3.4738... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:26... Training loss:3.4866... 0.3940 sec/batch\n",
      "Epoch:1/20... Training Step:27... Training loss:3.4849... 0.4408 sec/batch\n",
      "Epoch:1/20... Training Step:28... Training loss:3.4351... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:29... Training loss:3.4297... 0.4089 sec/batch\n",
      "Epoch:1/20... Training Step:30... Training loss:3.4125... 0.4039 sec/batch\n",
      "Epoch:1/20... Training Step:31... Training loss:3.4170... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:32... Training loss:3.3852... 0.6134 sec/batch\n",
      "Epoch:1/20... Training Step:33... Training loss:3.3517... 0.4099 sec/batch\n",
      "Epoch:1/20... Training Step:34... Training loss:3.3710... 0.4299 sec/batch\n",
      "Epoch:1/20... Training Step:35... Training loss:3.3509... 0.4368 sec/batch\n",
      "Epoch:1/20... Training Step:36... Training loss:3.3665... 0.4129 sec/batch\n",
      "Epoch:1/20... Training Step:37... Training loss:3.3256... 0.4299 sec/batch\n",
      "Epoch:1/20... Training Step:38... Training loss:3.3358... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:39... Training loss:3.3213... 0.4019 sec/batch\n",
      "Epoch:1/20... Training Step:40... Training loss:3.3282... 0.4109 sec/batch\n",
      "Epoch:1/20... Training Step:41... Training loss:3.3011... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:42... Training loss:3.3031... 0.4009 sec/batch\n",
      "Epoch:1/20... Training Step:43... Training loss:3.2971... 0.3950 sec/batch\n",
      "Epoch:1/20... Training Step:44... Training loss:3.2931... 0.3820 sec/batch\n",
      "Epoch:1/20... Training Step:45... Training loss:3.3082... 0.3859 sec/batch\n",
      "Epoch:1/20... Training Step:46... Training loss:3.3062... 0.3840 sec/batch\n",
      "Epoch:1/20... Training Step:47... Training loss:3.3035... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:48... Training loss:3.3093... 0.3870 sec/batch\n",
      "Epoch:1/20... Training Step:49... Training loss:3.3013... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:50... Training loss:3.2945... 0.4069 sec/batch\n",
      "Epoch:1/20... Training Step:51... Training loss:3.2801... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:52... Training loss:3.2811... 0.4000 sec/batch\n",
      "Epoch:1/20... Training Step:53... Training loss:3.2851... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:54... Training loss:3.2702... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:55... Training loss:3.2842... 0.4448 sec/batch\n",
      "Epoch:1/20... Training Step:56... Training loss:3.2518... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:57... Training loss:3.2669... 0.4109 sec/batch\n",
      "Epoch:1/20... Training Step:58... Training loss:3.2635... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:59... Training loss:3.2531... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:60... Training loss:3.2589... 0.3930 sec/batch\n",
      "Epoch:1/20... Training Step:61... Training loss:3.2585... 0.3989 sec/batch\n",
      "Epoch:1/20... Training Step:62... Training loss:3.2783... 0.4019 sec/batch\n",
      "Epoch:1/20... Training Step:63... Training loss:3.2912... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:64... Training loss:3.2379... 0.4000 sec/batch\n",
      "Epoch:1/20... Training Step:65... Training loss:3.2438... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:66... Training loss:3.2701... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:67... Training loss:3.2647... 0.4019 sec/batch\n",
      "Epoch:1/20... Training Step:68... Training loss:3.2195... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:69... Training loss:3.2376... 0.3850 sec/batch\n",
      "Epoch:1/20... Training Step:70... Training loss:3.2544... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:71... Training loss:3.2464... 0.3969 sec/batch\n",
      "Epoch:1/20... Training Step:72... Training loss:3.2721... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:73... Training loss:3.2517... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:74... Training loss:3.2517... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:75... Training loss:3.2499... 0.5017 sec/batch\n",
      "Epoch:1/20... Training Step:76... Training loss:3.2548... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:77... Training loss:3.2526... 0.4189 sec/batch\n",
      "Epoch:1/20... Training Step:78... Training loss:3.2440... 0.4099 sec/batch\n",
      "Epoch:1/20... Training Step:79... Training loss:3.2427... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:80... Training loss:3.2301... 0.3969 sec/batch\n",
      "Epoch:1/20... Training Step:81... Training loss:3.2352... 0.3840 sec/batch\n",
      "Epoch:1/20... Training Step:82... Training loss:3.2497... 0.3959 sec/batch\n",
      "Epoch:1/20... Training Step:83... Training loss:3.2431... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:84... Training loss:3.2325... 0.4169 sec/batch\n",
      "Epoch:1/20... Training Step:85... Training loss:3.2235... 0.4328 sec/batch\n",
      "Epoch:1/20... Training Step:86... Training loss:3.2314... 0.3979 sec/batch\n",
      "Epoch:1/20... Training Step:87... Training loss:3.2272... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:88... Training loss:3.2302... 0.4049 sec/batch\n",
      "Epoch:1/20... Training Step:89... Training loss:3.2450... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:90... Training loss:3.2322... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:91... Training loss:3.2433... 0.4139 sec/batch\n",
      "Epoch:1/20... Training Step:92... Training loss:3.2262... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:93... Training loss:3.2300... 0.3929 sec/batch\n",
      "Epoch:1/20... Training Step:94... Training loss:3.2328... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:95... Training loss:3.2272... 0.4219 sec/batch\n",
      "Epoch:1/20... Training Step:96... Training loss:3.2280... 0.5046 sec/batch\n",
      "Epoch:1/20... Training Step:97... Training loss:3.2348... 0.4438 sec/batch\n",
      "Epoch:1/20... Training Step:98... Training loss:3.2195... 0.3899 sec/batch\n",
      "Epoch:1/20... Training Step:99... Training loss:3.2287... 0.4069 sec/batch\n",
      "Epoch:1/20... Training Step:100... Training loss:3.2263... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:101... Training loss:3.2289... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:102... Training loss:3.2259... 0.3969 sec/batch\n",
      "Epoch:1/20... Training Step:103... Training loss:3.2256... 0.3820 sec/batch\n",
      "Epoch:1/20... Training Step:104... Training loss:3.2240... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:105... Training loss:3.2302... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:106... Training loss:3.2202... 0.3929 sec/batch\n",
      "Epoch:1/20... Training Step:107... Training loss:3.2053... 0.4019 sec/batch\n",
      "Epoch:1/20... Training Step:108... Training loss:3.2041... 0.3930 sec/batch\n",
      "Epoch:1/20... Training Step:109... Training loss:3.2252... 0.3810 sec/batch\n",
      "Epoch:1/20... Training Step:110... Training loss:3.1978... 0.3959 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20... Training Step:111... Training loss:3.2176... 0.3830 sec/batch\n",
      "Epoch:1/20... Training Step:112... Training loss:3.2218... 0.4049 sec/batch\n",
      "Epoch:1/20... Training Step:113... Training loss:3.2174... 0.4318 sec/batch\n",
      "Epoch:1/20... Training Step:114... Training loss:3.2067... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:115... Training loss:3.2081... 0.4189 sec/batch\n",
      "Epoch:1/20... Training Step:116... Training loss:3.2028... 0.4059 sec/batch\n",
      "Epoch:1/20... Training Step:117... Training loss:3.2089... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:118... Training loss:3.2323... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:119... Training loss:3.2273... 0.3980 sec/batch\n",
      "Epoch:1/20... Training Step:120... Training loss:3.1999... 0.4079 sec/batch\n",
      "Epoch:1/20... Training Step:121... Training loss:3.2396... 0.4079 sec/batch\n",
      "Epoch:1/20... Training Step:122... Training loss:3.2211... 0.3929 sec/batch\n",
      "Epoch:1/20... Training Step:123... Training loss:3.2271... 0.4119 sec/batch\n",
      "Epoch:1/20... Training Step:124... Training loss:3.2270... 0.4139 sec/batch\n",
      "Epoch:1/20... Training Step:125... Training loss:3.1980... 0.4049 sec/batch\n",
      "Epoch:1/20... Training Step:126... Training loss:3.1974... 0.3919 sec/batch\n",
      "Epoch:1/20... Training Step:127... Training loss:3.2106... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:128... Training loss:3.2182... 0.4079 sec/batch\n",
      "Epoch:1/20... Training Step:129... Training loss:3.2004... 0.4259 sec/batch\n",
      "Epoch:1/20... Training Step:130... Training loss:3.2098... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:131... Training loss:3.2208... 0.4099 sec/batch\n",
      "Epoch:1/20... Training Step:132... Training loss:3.2043... 0.4109 sec/batch\n",
      "Epoch:1/20... Training Step:133... Training loss:3.2130... 0.4109 sec/batch\n",
      "Epoch:1/20... Training Step:134... Training loss:3.2075... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:135... Training loss:3.1741... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:136... Training loss:3.1780... 0.4179 sec/batch\n",
      "Epoch:1/20... Training Step:137... Training loss:3.1974... 0.4279 sec/batch\n",
      "Epoch:1/20... Training Step:138... Training loss:3.2027... 0.3969 sec/batch\n",
      "Epoch:1/20... Training Step:139... Training loss:3.2065... 0.3999 sec/batch\n",
      "Epoch:1/20... Training Step:140... Training loss:3.2024... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:141... Training loss:3.1985... 0.4089 sec/batch\n",
      "Epoch:1/20... Training Step:142... Training loss:3.1828... 0.4009 sec/batch\n",
      "Epoch:1/20... Training Step:143... Training loss:3.1970... 0.3850 sec/batch\n",
      "Epoch:1/20... Training Step:144... Training loss:3.1919... 0.4039 sec/batch\n",
      "Epoch:1/20... Training Step:145... Training loss:3.1965... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:146... Training loss:3.1992... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:147... Training loss:3.2129... 0.3860 sec/batch\n",
      "Epoch:1/20... Training Step:148... Training loss:3.2232... 0.3919 sec/batch\n",
      "Epoch:1/20... Training Step:149... Training loss:3.1916... 0.3860 sec/batch\n",
      "Epoch:1/20... Training Step:150... Training loss:3.2015... 0.4398 sec/batch\n",
      "Epoch:1/20... Training Step:151... Training loss:3.2221... 0.4139 sec/batch\n",
      "Epoch:1/20... Training Step:152... Training loss:3.2219... 0.4019 sec/batch\n",
      "Epoch:1/20... Training Step:153... Training loss:3.2106... 0.4049 sec/batch\n",
      "Epoch:1/20... Training Step:154... Training loss:3.2052... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:155... Training loss:3.1916... 0.3850 sec/batch\n",
      "Epoch:1/20... Training Step:156... Training loss:3.1972... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:157... Training loss:3.1872... 0.3850 sec/batch\n",
      "Epoch:1/20... Training Step:158... Training loss:3.1939... 0.3959 sec/batch\n",
      "Epoch:1/20... Training Step:159... Training loss:3.1796... 0.4119 sec/batch\n",
      "Epoch:1/20... Training Step:160... Training loss:3.1822... 0.3929 sec/batch\n",
      "Epoch:1/20... Training Step:161... Training loss:3.2076... 0.3900 sec/batch\n",
      "Epoch:1/20... Training Step:162... Training loss:3.1737... 0.3830 sec/batch\n",
      "Epoch:1/20... Training Step:163... Training loss:3.1699... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:164... Training loss:3.1887... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:165... Training loss:3.1835... 0.3810 sec/batch\n",
      "Epoch:1/20... Training Step:166... Training loss:3.1808... 0.3859 sec/batch\n",
      "Epoch:1/20... Training Step:167... Training loss:3.1897... 0.4029 sec/batch\n",
      "Epoch:1/20... Training Step:168... Training loss:3.1873... 0.3940 sec/batch\n",
      "Epoch:1/20... Training Step:169... Training loss:3.1841... 0.3930 sec/batch\n",
      "Epoch:1/20... Training Step:170... Training loss:3.1714... 0.3969 sec/batch\n",
      "Epoch:1/20... Training Step:171... Training loss:3.1901... 0.3790 sec/batch\n",
      "Epoch:1/20... Training Step:172... Training loss:3.2109... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:173... Training loss:3.2191... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:174... Training loss:3.2237... 0.3949 sec/batch\n",
      "Epoch:1/20... Training Step:175... Training loss:3.1935... 0.3940 sec/batch\n",
      "Epoch:1/20... Training Step:176... Training loss:3.1968... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:177... Training loss:3.1907... 0.3820 sec/batch\n",
      "Epoch:1/20... Training Step:178... Training loss:3.1622... 0.3899 sec/batch\n",
      "Epoch:1/20... Training Step:179... Training loss:3.1698... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:180... Training loss:3.1775... 0.3840 sec/batch\n",
      "Epoch:1/20... Training Step:181... Training loss:3.1900... 0.3959 sec/batch\n",
      "Epoch:1/20... Training Step:182... Training loss:3.1981... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:183... Training loss:3.1756... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:184... Training loss:3.1945... 0.4009 sec/batch\n",
      "Epoch:1/20... Training Step:185... Training loss:3.2068... 0.3800 sec/batch\n",
      "Epoch:1/20... Training Step:186... Training loss:3.1780... 0.3910 sec/batch\n",
      "Epoch:1/20... Training Step:187... Training loss:3.1765... 0.3760 sec/batch\n",
      "Epoch:1/20... Training Step:188... Training loss:3.1689... 0.3939 sec/batch\n",
      "Epoch:1/20... Training Step:189... Training loss:3.1882... 0.3880 sec/batch\n",
      "Epoch:1/20... Training Step:190... Training loss:3.1839... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:191... Training loss:3.1860... 0.3870 sec/batch\n",
      "Epoch:1/20... Training Step:192... Training loss:3.1530... 0.3929 sec/batch\n",
      "Epoch:1/20... Training Step:193... Training loss:3.1765... 0.3840 sec/batch\n",
      "Epoch:1/20... Training Step:194... Training loss:3.1797... 0.3890 sec/batch\n",
      "Epoch:1/20... Training Step:195... Training loss:3.1562... 0.3850 sec/batch\n",
      "Epoch:1/20... Training Step:196... Training loss:3.1712... 0.3920 sec/batch\n",
      "Epoch:1/20... Training Step:197... Training loss:3.1687... 0.3909 sec/batch\n",
      "Epoch:1/20... Training Step:198... Training loss:3.1621... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:199... Training loss:3.3542... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:200... Training loss:3.1561... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:201... Training loss:3.1632... 0.4079 sec/batch\n",
      "Epoch:2/20... Training Step:202... Training loss:3.1687... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:203... Training loss:3.1787... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:204... Training loss:3.1867... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:205... Training loss:3.1811... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:206... Training loss:3.1872... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:207... Training loss:3.1712... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:208... Training loss:3.1662... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:209... Training loss:3.1589... 0.4019 sec/batch\n",
      "Epoch:2/20... Training Step:210... Training loss:3.1765... 0.3950 sec/batch\n",
      "Epoch:2/20... Training Step:211... Training loss:3.1696... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:212... Training loss:3.1799... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:213... Training loss:3.1813... 0.3950 sec/batch\n",
      "Epoch:2/20... Training Step:214... Training loss:3.1714... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:215... Training loss:3.1627... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:216... Training loss:3.2045... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:217... Training loss:3.1861... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:218... Training loss:3.1449... 0.4039 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/20... Training Step:219... Training loss:3.1822... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:220... Training loss:3.1671... 0.3870 sec/batch\n",
      "Epoch:2/20... Training Step:221... Training loss:3.1624... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:222... Training loss:3.1678... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:223... Training loss:3.1558... 0.3930 sec/batch\n",
      "Epoch:2/20... Training Step:224... Training loss:3.1711... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:225... Training loss:3.1806... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:226... Training loss:3.1598... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:227... Training loss:3.1662... 0.3870 sec/batch\n",
      "Epoch:2/20... Training Step:228... Training loss:3.1714... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:229... Training loss:3.1936... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:230... Training loss:3.1676... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:231... Training loss:3.1541... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:232... Training loss:3.1829... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:233... Training loss:3.1647... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:234... Training loss:3.1822... 0.3919 sec/batch\n",
      "Epoch:2/20... Training Step:235... Training loss:3.1483... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:236... Training loss:3.1551... 0.4049 sec/batch\n",
      "Epoch:2/20... Training Step:237... Training loss:3.1528... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:238... Training loss:3.1599... 0.3820 sec/batch\n",
      "Epoch:2/20... Training Step:239... Training loss:3.1433... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:240... Training loss:3.1482... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:241... Training loss:3.1472... 0.3870 sec/batch\n",
      "Epoch:2/20... Training Step:242... Training loss:3.1498... 0.3820 sec/batch\n",
      "Epoch:2/20... Training Step:243... Training loss:3.1455... 0.3989 sec/batch\n",
      "Epoch:2/20... Training Step:244... Training loss:3.1586... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:245... Training loss:3.1653... 0.3869 sec/batch\n",
      "Epoch:2/20... Training Step:246... Training loss:3.1671... 0.3919 sec/batch\n",
      "Epoch:2/20... Training Step:247... Training loss:3.1589... 0.3890 sec/batch\n",
      "Epoch:2/20... Training Step:248... Training loss:3.1617... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:249... Training loss:3.1612... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:250... Training loss:3.1530... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:251... Training loss:3.1589... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:252... Training loss:3.1529... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:253... Training loss:3.1589... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:254... Training loss:3.1355... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:255... Training loss:3.1506... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:256... Training loss:3.1550... 0.4189 sec/batch\n",
      "Epoch:2/20... Training Step:257... Training loss:3.1466... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:258... Training loss:3.1460... 0.4129 sec/batch\n",
      "Epoch:2/20... Training Step:259... Training loss:3.1538... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:260... Training loss:3.1669... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:261... Training loss:3.1743... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:262... Training loss:3.1389... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:263... Training loss:3.1353... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:264... Training loss:3.1698... 0.3890 sec/batch\n",
      "Epoch:2/20... Training Step:265... Training loss:3.1560... 0.3810 sec/batch\n",
      "Epoch:2/20... Training Step:266... Training loss:3.1099... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:267... Training loss:3.1364... 0.3999 sec/batch\n",
      "Epoch:2/20... Training Step:268... Training loss:3.1528... 0.3919 sec/batch\n",
      "Epoch:2/20... Training Step:269... Training loss:3.1482... 0.4029 sec/batch\n",
      "Epoch:2/20... Training Step:270... Training loss:3.1597... 0.3940 sec/batch\n",
      "Epoch:2/20... Training Step:271... Training loss:3.1397... 0.3970 sec/batch\n",
      "Epoch:2/20... Training Step:272... Training loss:3.1546... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:273... Training loss:3.1518... 0.3810 sec/batch\n",
      "Epoch:2/20... Training Step:274... Training loss:3.1588... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:275... Training loss:3.1577... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:276... Training loss:3.1462... 0.3859 sec/batch\n",
      "Epoch:2/20... Training Step:277... Training loss:3.1466... 0.3940 sec/batch\n",
      "Epoch:2/20... Training Step:278... Training loss:3.1307... 0.3989 sec/batch\n",
      "Epoch:2/20... Training Step:279... Training loss:3.1364... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:280... Training loss:3.1495... 0.3890 sec/batch\n",
      "Epoch:2/20... Training Step:281... Training loss:3.1518... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:282... Training loss:3.1415... 0.3889 sec/batch\n",
      "Epoch:2/20... Training Step:283... Training loss:3.1279... 0.3800 sec/batch\n",
      "Epoch:2/20... Training Step:284... Training loss:3.1355... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:285... Training loss:3.1301... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:286... Training loss:3.1334... 0.3890 sec/batch\n",
      "Epoch:2/20... Training Step:287... Training loss:3.1510... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:288... Training loss:3.1524... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:289... Training loss:3.1419... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:290... Training loss:3.1356... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:291... Training loss:3.1405... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:292... Training loss:3.1434... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:293... Training loss:3.1371... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:294... Training loss:3.1350... 0.3820 sec/batch\n",
      "Epoch:2/20... Training Step:295... Training loss:3.1519... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:296... Training loss:3.1412... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:297... Training loss:3.1417... 0.3890 sec/batch\n",
      "Epoch:2/20... Training Step:298... Training loss:3.1415... 0.3870 sec/batch\n",
      "Epoch:2/20... Training Step:299... Training loss:3.1539... 0.3820 sec/batch\n",
      "Epoch:2/20... Training Step:300... Training loss:3.1461... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:301... Training loss:3.1419... 0.3940 sec/batch\n",
      "Epoch:2/20... Training Step:302... Training loss:3.1391... 0.3989 sec/batch\n",
      "Epoch:2/20... Training Step:303... Training loss:3.1403... 0.4029 sec/batch\n",
      "Epoch:2/20... Training Step:304... Training loss:3.1387... 0.3830 sec/batch\n",
      "Epoch:2/20... Training Step:305... Training loss:3.1195... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:306... Training loss:3.1295... 0.3879 sec/batch\n",
      "Epoch:2/20... Training Step:307... Training loss:3.1387... 0.3830 sec/batch\n",
      "Epoch:2/20... Training Step:308... Training loss:3.1042... 0.3930 sec/batch\n",
      "Epoch:2/20... Training Step:309... Training loss:3.1296... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:310... Training loss:3.1371... 0.4029 sec/batch\n",
      "Epoch:2/20... Training Step:311... Training loss:3.1243... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:312... Training loss:3.1140... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:313... Training loss:3.1140... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:314... Training loss:3.1148... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:315... Training loss:3.1116... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:316... Training loss:3.1403... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:317... Training loss:3.1317... 0.4019 sec/batch\n",
      "Epoch:2/20... Training Step:318... Training loss:3.1159... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:319... Training loss:3.1503... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:320... Training loss:3.1303... 0.3970 sec/batch\n",
      "Epoch:2/20... Training Step:321... Training loss:3.1333... 0.3879 sec/batch\n",
      "Epoch:2/20... Training Step:322... Training loss:3.1366... 0.3870 sec/batch\n",
      "Epoch:2/20... Training Step:323... Training loss:3.1089... 0.4009 sec/batch\n",
      "Epoch:2/20... Training Step:324... Training loss:3.0957... 0.3920 sec/batch\n",
      "Epoch:2/20... Training Step:325... Training loss:3.1227... 0.4149 sec/batch\n",
      "Epoch:2/20... Training Step:326... Training loss:3.1326... 0.3810 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/20... Training Step:327... Training loss:3.1101... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:328... Training loss:3.1179... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:329... Training loss:3.1306... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:330... Training loss:3.1053... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:331... Training loss:3.1239... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:332... Training loss:3.1126... 0.3930 sec/batch\n",
      "Epoch:2/20... Training Step:333... Training loss:3.0812... 0.4049 sec/batch\n",
      "Epoch:2/20... Training Step:334... Training loss:3.0820... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:335... Training loss:3.0909... 0.3830 sec/batch\n",
      "Epoch:2/20... Training Step:336... Training loss:3.0839... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:337... Training loss:3.1201... 0.3930 sec/batch\n",
      "Epoch:2/20... Training Step:338... Training loss:3.1052... 0.3830 sec/batch\n",
      "Epoch:2/20... Training Step:339... Training loss:3.0978... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:340... Training loss:3.0839... 0.4119 sec/batch\n",
      "Epoch:2/20... Training Step:341... Training loss:3.0969... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:342... Training loss:3.0944... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:343... Training loss:3.0915... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:344... Training loss:3.1001... 0.3810 sec/batch\n",
      "Epoch:2/20... Training Step:345... Training loss:3.1049... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:346... Training loss:3.1193... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:347... Training loss:3.0838... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:348... Training loss:3.0882... 0.4019 sec/batch\n",
      "Epoch:2/20... Training Step:349... Training loss:3.1170... 0.3800 sec/batch\n",
      "Epoch:2/20... Training Step:350... Training loss:3.1285... 0.3970 sec/batch\n",
      "Epoch:2/20... Training Step:351... Training loss:3.1021... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:352... Training loss:3.0924... 0.3979 sec/batch\n",
      "Epoch:2/20... Training Step:353... Training loss:3.0762... 0.3869 sec/batch\n",
      "Epoch:2/20... Training Step:354... Training loss:3.0864... 0.3869 sec/batch\n",
      "Epoch:2/20... Training Step:355... Training loss:3.0746... 0.3999 sec/batch\n",
      "Epoch:2/20... Training Step:356... Training loss:3.0839... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:357... Training loss:3.0564... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:358... Training loss:3.0768... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:359... Training loss:3.0875... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:360... Training loss:3.0514... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:361... Training loss:3.0561... 0.4089 sec/batch\n",
      "Epoch:2/20... Training Step:362... Training loss:3.0730... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:363... Training loss:3.0724... 0.4069 sec/batch\n",
      "Epoch:2/20... Training Step:364... Training loss:3.0662... 0.4059 sec/batch\n",
      "Epoch:2/20... Training Step:365... Training loss:3.0614... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:366... Training loss:3.0718... 0.3919 sec/batch\n",
      "Epoch:2/20... Training Step:367... Training loss:3.0695... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:368... Training loss:3.0454... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:369... Training loss:3.0729... 0.3830 sec/batch\n",
      "Epoch:2/20... Training Step:370... Training loss:3.0863... 0.3849 sec/batch\n",
      "Epoch:2/20... Training Step:371... Training loss:3.0933... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:372... Training loss:3.0858... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:373... Training loss:3.0609... 0.4009 sec/batch\n",
      "Epoch:2/20... Training Step:374... Training loss:3.0715... 0.3910 sec/batch\n",
      "Epoch:2/20... Training Step:375... Training loss:3.0577... 0.3969 sec/batch\n",
      "Epoch:2/20... Training Step:376... Training loss:3.0332... 0.3860 sec/batch\n",
      "Epoch:2/20... Training Step:377... Training loss:3.0434... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:378... Training loss:3.0397... 0.3850 sec/batch\n",
      "Epoch:2/20... Training Step:379... Training loss:3.0528... 0.3959 sec/batch\n",
      "Epoch:2/20... Training Step:380... Training loss:3.0647... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:381... Training loss:3.0392... 0.3840 sec/batch\n",
      "Epoch:2/20... Training Step:382... Training loss:3.0528... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:383... Training loss:3.0713... 0.3919 sec/batch\n",
      "Epoch:2/20... Training Step:384... Training loss:3.0407... 0.3939 sec/batch\n",
      "Epoch:2/20... Training Step:385... Training loss:3.0353... 0.3950 sec/batch\n",
      "Epoch:2/20... Training Step:386... Training loss:3.0232... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:387... Training loss:3.0450... 0.3929 sec/batch\n",
      "Epoch:2/20... Training Step:388... Training loss:3.0371... 0.3880 sec/batch\n",
      "Epoch:2/20... Training Step:389... Training loss:3.0375... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:390... Training loss:3.0128... 0.3999 sec/batch\n",
      "Epoch:2/20... Training Step:391... Training loss:3.0307... 0.3900 sec/batch\n",
      "Epoch:2/20... Training Step:392... Training loss:3.0217... 0.4059 sec/batch\n",
      "Epoch:2/20... Training Step:393... Training loss:3.0092... 0.4059 sec/batch\n",
      "Epoch:2/20... Training Step:394... Training loss:3.0196... 0.4049 sec/batch\n",
      "Epoch:2/20... Training Step:395... Training loss:3.0136... 0.3949 sec/batch\n",
      "Epoch:2/20... Training Step:396... Training loss:3.0070... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:397... Training loss:3.2406... 0.3910 sec/batch\n",
      "Epoch:3/20... Training Step:398... Training loss:3.0067... 0.4139 sec/batch\n",
      "Epoch:3/20... Training Step:399... Training loss:3.0069... 0.4109 sec/batch\n",
      "Epoch:3/20... Training Step:400... Training loss:3.0099... 0.4119 sec/batch\n",
      "Epoch:3/20... Training Step:401... Training loss:3.0115... 0.4169 sec/batch\n",
      "Epoch:3/20... Training Step:402... Training loss:3.0269... 0.4099 sec/batch\n",
      "Epoch:3/20... Training Step:403... Training loss:3.0138... 0.4089 sec/batch\n",
      "Epoch:3/20... Training Step:404... Training loss:3.0184... 0.4298 sec/batch\n",
      "Epoch:3/20... Training Step:405... Training loss:3.0062... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:406... Training loss:3.0034... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:407... Training loss:2.9980... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:408... Training loss:3.0012... 0.4189 sec/batch\n",
      "Epoch:3/20... Training Step:409... Training loss:2.9950... 0.3850 sec/batch\n",
      "Epoch:3/20... Training Step:410... Training loss:3.0239... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:411... Training loss:3.0108... 0.4159 sec/batch\n",
      "Epoch:3/20... Training Step:412... Training loss:3.0019... 0.4079 sec/batch\n",
      "Epoch:3/20... Training Step:413... Training loss:2.9895... 0.4029 sec/batch\n",
      "Epoch:3/20... Training Step:414... Training loss:3.0432... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:415... Training loss:3.0008... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:416... Training loss:2.9682... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:417... Training loss:2.9997... 0.3929 sec/batch\n",
      "Epoch:3/20... Training Step:418... Training loss:3.0088... 0.3879 sec/batch\n",
      "Epoch:3/20... Training Step:419... Training loss:2.9811... 0.3870 sec/batch\n",
      "Epoch:3/20... Training Step:420... Training loss:2.9881... 0.3870 sec/batch\n",
      "Epoch:3/20... Training Step:421... Training loss:2.9808... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:422... Training loss:2.9835... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:423... Training loss:2.9913... 0.3980 sec/batch\n",
      "Epoch:3/20... Training Step:424... Training loss:2.9667... 0.3909 sec/batch\n",
      "Epoch:3/20... Training Step:425... Training loss:2.9769... 0.4029 sec/batch\n",
      "Epoch:3/20... Training Step:426... Training loss:2.9761... 0.3929 sec/batch\n",
      "Epoch:3/20... Training Step:427... Training loss:2.9984... 0.6024 sec/batch\n",
      "Epoch:3/20... Training Step:428... Training loss:2.9661... 0.4289 sec/batch\n",
      "Epoch:3/20... Training Step:429... Training loss:2.9609... 0.4189 sec/batch\n",
      "Epoch:3/20... Training Step:430... Training loss:2.9774... 0.4408 sec/batch\n",
      "Epoch:3/20... Training Step:431... Training loss:2.9467... 0.4338 sec/batch\n",
      "Epoch:3/20... Training Step:432... Training loss:2.9693... 0.4109 sec/batch\n",
      "Epoch:3/20... Training Step:433... Training loss:2.9480... 0.4039 sec/batch\n",
      "Epoch:3/20... Training Step:434... Training loss:2.9495... 0.4079 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/20... Training Step:435... Training loss:2.9416... 0.4259 sec/batch\n",
      "Epoch:3/20... Training Step:436... Training loss:2.9501... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:437... Training loss:2.9368... 0.4019 sec/batch\n",
      "Epoch:3/20... Training Step:438... Training loss:2.9366... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:439... Training loss:2.9392... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:440... Training loss:2.9313... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:441... Training loss:2.9164... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:442... Training loss:2.9366... 0.3870 sec/batch\n",
      "Epoch:3/20... Training Step:443... Training loss:2.9499... 0.4169 sec/batch\n",
      "Epoch:3/20... Training Step:444... Training loss:2.9531... 0.3970 sec/batch\n",
      "Epoch:3/20... Training Step:445... Training loss:2.9372... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:446... Training loss:2.9413... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:447... Training loss:2.9248... 0.4039 sec/batch\n",
      "Epoch:3/20... Training Step:448... Training loss:2.9250... 0.4329 sec/batch\n",
      "Epoch:3/20... Training Step:449... Training loss:2.9297... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:450... Training loss:2.9102... 0.4159 sec/batch\n",
      "Epoch:3/20... Training Step:451... Training loss:2.9220... 0.4139 sec/batch\n",
      "Epoch:3/20... Training Step:452... Training loss:2.9137... 0.4279 sec/batch\n",
      "Epoch:3/20... Training Step:453... Training loss:2.9251... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:454... Training loss:2.9202... 0.4169 sec/batch\n",
      "Epoch:3/20... Training Step:455... Training loss:2.9062... 0.3860 sec/batch\n",
      "Epoch:3/20... Training Step:456... Training loss:2.9234... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:457... Training loss:2.9237... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:458... Training loss:2.9244... 0.4059 sec/batch\n",
      "Epoch:3/20... Training Step:459... Training loss:2.9443... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:460... Training loss:2.9024... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:461... Training loss:2.8981... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:462... Training loss:2.9250... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:463... Training loss:2.9088... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:464... Training loss:2.8826... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:465... Training loss:2.8877... 0.3910 sec/batch\n",
      "Epoch:3/20... Training Step:466... Training loss:2.9054... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:467... Training loss:2.9009... 0.3810 sec/batch\n",
      "Epoch:3/20... Training Step:468... Training loss:2.9123... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:469... Training loss:2.8938... 0.3860 sec/batch\n",
      "Epoch:3/20... Training Step:470... Training loss:2.8990... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:471... Training loss:2.9078... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:472... Training loss:2.9214... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:473... Training loss:2.9067... 0.4000 sec/batch\n",
      "Epoch:3/20... Training Step:474... Training loss:2.8982... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:475... Training loss:2.8897... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:476... Training loss:2.8618... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:477... Training loss:2.8810... 0.3929 sec/batch\n",
      "Epoch:3/20... Training Step:478... Training loss:2.8968... 0.4039 sec/batch\n",
      "Epoch:3/20... Training Step:479... Training loss:2.8795... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:480... Training loss:2.8732... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:481... Training loss:2.8449... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:482... Training loss:2.8717... 0.3810 sec/batch\n",
      "Epoch:3/20... Training Step:483... Training loss:2.8686... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:484... Training loss:2.8679... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:485... Training loss:2.8733... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:486... Training loss:2.8800... 0.3800 sec/batch\n",
      "Epoch:3/20... Training Step:487... Training loss:2.8704... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:488... Training loss:2.8629... 0.3870 sec/batch\n",
      "Epoch:3/20... Training Step:489... Training loss:2.8665... 0.3910 sec/batch\n",
      "Epoch:3/20... Training Step:490... Training loss:2.8577... 0.3820 sec/batch\n",
      "Epoch:3/20... Training Step:491... Training loss:2.8369... 0.3919 sec/batch\n",
      "Epoch:3/20... Training Step:492... Training loss:2.8477... 0.3960 sec/batch\n",
      "Epoch:3/20... Training Step:493... Training loss:2.8603... 0.3810 sec/batch\n",
      "Epoch:3/20... Training Step:494... Training loss:2.8582... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:495... Training loss:2.8612... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:496... Training loss:2.8503... 0.3980 sec/batch\n",
      "Epoch:3/20... Training Step:497... Training loss:2.8600... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:498... Training loss:2.8556... 0.3990 sec/batch\n",
      "Epoch:3/20... Training Step:499... Training loss:2.8425... 0.3930 sec/batch\n",
      "Epoch:3/20... Training Step:500... Training loss:2.8342... 0.3840 sec/batch\n",
      "Epoch:3/20... Training Step:501... Training loss:2.8466... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:502... Training loss:2.8491... 0.3970 sec/batch\n",
      "Epoch:3/20... Training Step:503... Training loss:2.8258... 0.3929 sec/batch\n",
      "Epoch:3/20... Training Step:504... Training loss:2.8593... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:505... Training loss:2.8507... 0.3780 sec/batch\n",
      "Epoch:3/20... Training Step:506... Training loss:2.8132... 0.3830 sec/batch\n",
      "Epoch:3/20... Training Step:507... Training loss:2.8330... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:508... Training loss:2.8509... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:509... Training loss:2.8305... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:510... Training loss:2.8165... 0.3999 sec/batch\n",
      "Epoch:3/20... Training Step:511... Training loss:2.8131... 0.3860 sec/batch\n",
      "Epoch:3/20... Training Step:512... Training loss:2.8102... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:513... Training loss:2.8223... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:514... Training loss:2.8273... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:515... Training loss:2.8563... 0.3860 sec/batch\n",
      "Epoch:3/20... Training Step:516... Training loss:2.8233... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:517... Training loss:2.8631... 0.4019 sec/batch\n",
      "Epoch:3/20... Training Step:518... Training loss:2.8296... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:519... Training loss:2.8253... 0.4049 sec/batch\n",
      "Epoch:3/20... Training Step:520... Training loss:2.8394... 0.3899 sec/batch\n",
      "Epoch:3/20... Training Step:521... Training loss:2.8134... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:522... Training loss:2.7894... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:523... Training loss:2.8113... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:524... Training loss:2.8328... 0.3919 sec/batch\n",
      "Epoch:3/20... Training Step:525... Training loss:2.8118... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:526... Training loss:2.8136... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:527... Training loss:2.8212... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:528... Training loss:2.7885... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:529... Training loss:2.8233... 0.3919 sec/batch\n",
      "Epoch:3/20... Training Step:530... Training loss:2.8107... 0.3970 sec/batch\n",
      "Epoch:3/20... Training Step:531... Training loss:2.7722... 0.3959 sec/batch\n",
      "Epoch:3/20... Training Step:532... Training loss:2.7871... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:533... Training loss:2.7902... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:534... Training loss:2.7805... 0.3940 sec/batch\n",
      "Epoch:3/20... Training Step:535... Training loss:2.8098... 0.3919 sec/batch\n",
      "Epoch:3/20... Training Step:536... Training loss:2.7905... 0.3910 sec/batch\n",
      "Epoch:3/20... Training Step:537... Training loss:2.8129... 0.3870 sec/batch\n",
      "Epoch:3/20... Training Step:538... Training loss:2.7721... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:539... Training loss:2.7933... 0.4049 sec/batch\n",
      "Epoch:3/20... Training Step:540... Training loss:2.7862... 0.4029 sec/batch\n",
      "Epoch:3/20... Training Step:541... Training loss:2.7974... 0.4249 sec/batch\n",
      "Epoch:3/20... Training Step:542... Training loss:2.8040... 0.4109 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/20... Training Step:543... Training loss:2.7927... 0.3890 sec/batch\n",
      "Epoch:3/20... Training Step:544... Training loss:2.8195... 0.4019 sec/batch\n",
      "Epoch:3/20... Training Step:545... Training loss:2.7657... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:546... Training loss:2.7726... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:547... Training loss:2.8305... 0.3910 sec/batch\n",
      "Epoch:3/20... Training Step:548... Training loss:2.8243... 0.3999 sec/batch\n",
      "Epoch:3/20... Training Step:549... Training loss:2.7852... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:550... Training loss:2.7936... 0.4029 sec/batch\n",
      "Epoch:3/20... Training Step:551... Training loss:2.7717... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:552... Training loss:2.7746... 0.4029 sec/batch\n",
      "Epoch:3/20... Training Step:553... Training loss:2.7577... 0.4169 sec/batch\n",
      "Epoch:3/20... Training Step:554... Training loss:2.7639... 0.4069 sec/batch\n",
      "Epoch:3/20... Training Step:555... Training loss:2.7387... 0.4069 sec/batch\n",
      "Epoch:3/20... Training Step:556... Training loss:2.7879... 0.4269 sec/batch\n",
      "Epoch:3/20... Training Step:557... Training loss:2.7700... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:558... Training loss:2.7323... 0.4059 sec/batch\n",
      "Epoch:3/20... Training Step:559... Training loss:2.7300... 0.4229 sec/batch\n",
      "Epoch:3/20... Training Step:560... Training loss:2.7571... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:561... Training loss:2.7524... 0.4368 sec/batch\n",
      "Epoch:3/20... Training Step:562... Training loss:2.7435... 0.4099 sec/batch\n",
      "Epoch:3/20... Training Step:563... Training loss:2.7507... 0.4299 sec/batch\n",
      "Epoch:3/20... Training Step:564... Training loss:2.7543... 0.4059 sec/batch\n",
      "Epoch:3/20... Training Step:565... Training loss:2.7601... 0.3850 sec/batch\n",
      "Epoch:3/20... Training Step:566... Training loss:2.7235... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:567... Training loss:2.7522... 0.3850 sec/batch\n",
      "Epoch:3/20... Training Step:568... Training loss:2.7613... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:569... Training loss:2.7726... 0.3960 sec/batch\n",
      "Epoch:3/20... Training Step:570... Training loss:2.7568... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:571... Training loss:2.7483... 0.3880 sec/batch\n",
      "Epoch:3/20... Training Step:572... Training loss:2.7508... 0.4009 sec/batch\n",
      "Epoch:3/20... Training Step:573... Training loss:2.7378... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:574... Training loss:2.7158... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:575... Training loss:2.7189... 0.3950 sec/batch\n",
      "Epoch:3/20... Training Step:576... Training loss:2.7061... 0.3999 sec/batch\n",
      "Epoch:3/20... Training Step:577... Training loss:2.7221... 0.3949 sec/batch\n",
      "Epoch:3/20... Training Step:578... Training loss:2.7266... 0.3979 sec/batch\n",
      "Epoch:3/20... Training Step:579... Training loss:2.7174... 0.3900 sec/batch\n",
      "Epoch:3/20... Training Step:580... Training loss:2.7369... 0.4019 sec/batch\n",
      "Epoch:3/20... Training Step:581... Training loss:2.7527... 0.3840 sec/batch\n",
      "Epoch:3/20... Training Step:582... Training loss:2.7115... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:583... Training loss:2.7151... 0.3840 sec/batch\n",
      "Epoch:3/20... Training Step:584... Training loss:2.6933... 0.3920 sec/batch\n",
      "Epoch:3/20... Training Step:585... Training loss:2.7132... 0.3969 sec/batch\n",
      "Epoch:3/20... Training Step:586... Training loss:2.7106... 0.3929 sec/batch\n",
      "Epoch:3/20... Training Step:587... Training loss:2.7175... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:588... Training loss:2.6732... 0.3989 sec/batch\n",
      "Epoch:3/20... Training Step:589... Training loss:2.7144... 0.3850 sec/batch\n",
      "Epoch:3/20... Training Step:590... Training loss:2.6995... 0.3940 sec/batch\n",
      "Epoch:3/20... Training Step:591... Training loss:2.6858... 0.3939 sec/batch\n",
      "Epoch:3/20... Training Step:592... Training loss:2.6989... 0.3970 sec/batch\n",
      "Epoch:3/20... Training Step:593... Training loss:2.6941... 0.3930 sec/batch\n",
      "Epoch:3/20... Training Step:594... Training loss:2.6842... 0.4069 sec/batch\n",
      "Epoch:4/20... Training Step:595... Training loss:2.9537... 0.3910 sec/batch\n",
      "Epoch:4/20... Training Step:596... Training loss:2.6772... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:597... Training loss:2.6793... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:598... Training loss:2.6857... 0.3969 sec/batch\n",
      "Epoch:4/20... Training Step:599... Training loss:2.6823... 0.3969 sec/batch\n",
      "Epoch:4/20... Training Step:600... Training loss:2.6909... 0.4019 sec/batch\n",
      "Epoch:4/20... Training Step:601... Training loss:2.6987... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:602... Training loss:2.6872... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:603... Training loss:2.7041... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:604... Training loss:2.6792... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:605... Training loss:2.6668... 0.3800 sec/batch\n",
      "Epoch:4/20... Training Step:606... Training loss:2.6985... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:607... Training loss:2.6826... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:608... Training loss:2.7175... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:609... Training loss:2.6872... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:610... Training loss:2.6871... 0.3940 sec/batch\n",
      "Epoch:4/20... Training Step:611... Training loss:2.6833... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:612... Training loss:2.7188... 0.4119 sec/batch\n",
      "Epoch:4/20... Training Step:613... Training loss:2.6886... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:614... Training loss:2.6585... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:615... Training loss:2.6738... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:616... Training loss:2.7103... 0.4069 sec/batch\n",
      "Epoch:4/20... Training Step:617... Training loss:2.6768... 0.3980 sec/batch\n",
      "Epoch:4/20... Training Step:618... Training loss:2.6719... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:619... Training loss:2.6597... 0.3870 sec/batch\n",
      "Epoch:4/20... Training Step:620... Training loss:2.6792... 0.3940 sec/batch\n",
      "Epoch:4/20... Training Step:621... Training loss:2.6755... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:622... Training loss:2.6628... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:623... Training loss:2.6717... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:624... Training loss:2.6664... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:625... Training loss:2.6936... 0.3930 sec/batch\n",
      "Epoch:4/20... Training Step:626... Training loss:2.6519... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:627... Training loss:2.6372... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:628... Training loss:2.6763... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:629... Training loss:2.6413... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:630... Training loss:2.6595... 0.4468 sec/batch\n",
      "Epoch:4/20... Training Step:631... Training loss:2.6324... 0.4538 sec/batch\n",
      "Epoch:4/20... Training Step:632... Training loss:2.6287... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:633... Training loss:2.6366... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:634... Training loss:2.6412... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:635... Training loss:2.6308... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:636... Training loss:2.6321... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:637... Training loss:2.6327... 0.4019 sec/batch\n",
      "Epoch:4/20... Training Step:638... Training loss:2.6195... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:639... Training loss:2.6264... 0.3830 sec/batch\n",
      "Epoch:4/20... Training Step:640... Training loss:2.6154... 0.3950 sec/batch\n",
      "Epoch:4/20... Training Step:641... Training loss:2.6436... 0.3879 sec/batch\n",
      "Epoch:4/20... Training Step:642... Training loss:2.6424... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:643... Training loss:2.6350... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:644... Training loss:2.6467... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:645... Training loss:2.6227... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:646... Training loss:2.6329... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:647... Training loss:2.6178... 0.3979 sec/batch\n",
      "Epoch:4/20... Training Step:648... Training loss:2.6146... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:649... Training loss:2.6116... 0.4000 sec/batch\n",
      "Epoch:4/20... Training Step:650... Training loss:2.6267... 0.3970 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/20... Training Step:651... Training loss:2.6120... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:652... Training loss:2.6154... 0.4558 sec/batch\n",
      "Epoch:4/20... Training Step:653... Training loss:2.6072... 0.4259 sec/batch\n",
      "Epoch:4/20... Training Step:654... Training loss:2.6251... 0.4019 sec/batch\n",
      "Epoch:4/20... Training Step:655... Training loss:2.6116... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:656... Training loss:2.6250... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:657... Training loss:2.6445... 0.3969 sec/batch\n",
      "Epoch:4/20... Training Step:658... Training loss:2.6101... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:659... Training loss:2.5967... 0.3830 sec/batch\n",
      "Epoch:4/20... Training Step:660... Training loss:2.6325... 0.4079 sec/batch\n",
      "Epoch:4/20... Training Step:661... Training loss:2.6237... 0.3979 sec/batch\n",
      "Epoch:4/20... Training Step:662... Training loss:2.5800... 0.3870 sec/batch\n",
      "Epoch:4/20... Training Step:663... Training loss:2.5912... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:664... Training loss:2.6096... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:665... Training loss:2.6050... 0.3930 sec/batch\n",
      "Epoch:4/20... Training Step:666... Training loss:2.6178... 0.3930 sec/batch\n",
      "Epoch:4/20... Training Step:667... Training loss:2.6034... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:668... Training loss:2.6148... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:669... Training loss:2.6177... 0.3969 sec/batch\n",
      "Epoch:4/20... Training Step:670... Training loss:2.6389... 0.3870 sec/batch\n",
      "Epoch:4/20... Training Step:671... Training loss:2.6093... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:672... Training loss:2.6043... 0.3940 sec/batch\n",
      "Epoch:4/20... Training Step:673... Training loss:2.5990... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:674... Training loss:2.5852... 0.3970 sec/batch\n",
      "Epoch:4/20... Training Step:675... Training loss:2.5908... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:676... Training loss:2.6103... 0.4069 sec/batch\n",
      "Epoch:4/20... Training Step:677... Training loss:2.5969... 0.3979 sec/batch\n",
      "Epoch:4/20... Training Step:678... Training loss:2.5773... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:679... Training loss:2.5619... 0.4069 sec/batch\n",
      "Epoch:4/20... Training Step:680... Training loss:2.5907... 0.3910 sec/batch\n",
      "Epoch:4/20... Training Step:681... Training loss:2.5856... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:682... Training loss:2.5893... 0.4079 sec/batch\n",
      "Epoch:4/20... Training Step:683... Training loss:2.5821... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:684... Training loss:2.6006... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:685... Training loss:2.5797... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:686... Training loss:2.5927... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:687... Training loss:2.5909... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:688... Training loss:2.5791... 0.4099 sec/batch\n",
      "Epoch:4/20... Training Step:689... Training loss:2.5742... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:690... Training loss:2.5684... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:691... Training loss:2.5882... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:692... Training loss:2.5778... 0.4029 sec/batch\n",
      "Epoch:4/20... Training Step:693... Training loss:2.5808... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:694... Training loss:2.5670... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:695... Training loss:2.6016... 0.4079 sec/batch\n",
      "Epoch:4/20... Training Step:696... Training loss:2.5761... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:697... Training loss:2.5657... 0.3979 sec/batch\n",
      "Epoch:4/20... Training Step:698... Training loss:2.5691... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:699... Training loss:2.5707... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:700... Training loss:2.5750... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:701... Training loss:2.5629... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:702... Training loss:2.5942... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:703... Training loss:2.5897... 0.3830 sec/batch\n",
      "Epoch:4/20... Training Step:704... Training loss:2.5406... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:705... Training loss:2.5691... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:706... Training loss:2.5840... 0.4029 sec/batch\n",
      "Epoch:4/20... Training Step:707... Training loss:2.5718... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:708... Training loss:2.5528... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:709... Training loss:2.5547... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:710... Training loss:2.5452... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:711... Training loss:2.5641... 0.3830 sec/batch\n",
      "Epoch:4/20... Training Step:712... Training loss:2.5714... 0.3859 sec/batch\n",
      "Epoch:4/20... Training Step:713... Training loss:2.5952... 0.4099 sec/batch\n",
      "Epoch:4/20... Training Step:714... Training loss:2.5769... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:715... Training loss:2.6037... 0.3909 sec/batch\n",
      "Epoch:4/20... Training Step:716... Training loss:2.5741... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:717... Training loss:2.5610... 0.4039 sec/batch\n",
      "Epoch:4/20... Training Step:718... Training loss:2.5717... 0.3919 sec/batch\n",
      "Epoch:4/20... Training Step:719... Training loss:2.5663... 0.4019 sec/batch\n",
      "Epoch:4/20... Training Step:720... Training loss:2.5357... 0.4019 sec/batch\n",
      "Epoch:4/20... Training Step:721... Training loss:2.5741... 0.4807 sec/batch\n",
      "Epoch:4/20... Training Step:722... Training loss:2.5836... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:723... Training loss:2.5607... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:724... Training loss:2.5581... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:725... Training loss:2.5601... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:726... Training loss:2.5361... 0.4687 sec/batch\n",
      "Epoch:4/20... Training Step:727... Training loss:2.5721... 0.4318 sec/batch\n",
      "Epoch:4/20... Training Step:728... Training loss:2.5685... 0.3990 sec/batch\n",
      "Epoch:4/20... Training Step:729... Training loss:2.5349... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:730... Training loss:2.5496... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:731... Training loss:2.5422... 0.4179 sec/batch\n",
      "Epoch:4/20... Training Step:732... Training loss:2.5478... 0.4379 sec/batch\n",
      "Epoch:4/20... Training Step:733... Training loss:2.5751... 0.4099 sec/batch\n",
      "Epoch:4/20... Training Step:734... Training loss:2.5430... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:735... Training loss:2.5832... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:736... Training loss:2.5395... 0.3969 sec/batch\n",
      "Epoch:4/20... Training Step:737... Training loss:2.5553... 0.4259 sec/batch\n",
      "Epoch:4/20... Training Step:738... Training loss:2.5517... 0.4049 sec/batch\n",
      "Epoch:4/20... Training Step:739... Training loss:2.5517... 0.3980 sec/batch\n",
      "Epoch:4/20... Training Step:740... Training loss:2.5711... 0.4239 sec/batch\n",
      "Epoch:4/20... Training Step:741... Training loss:2.5622... 0.4099 sec/batch\n",
      "Epoch:4/20... Training Step:742... Training loss:2.5724... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:743... Training loss:2.5393... 0.3919 sec/batch\n",
      "Epoch:4/20... Training Step:744... Training loss:2.5390... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:745... Training loss:2.6002... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:746... Training loss:2.5982... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:747... Training loss:2.5647... 0.3930 sec/batch\n",
      "Epoch:4/20... Training Step:748... Training loss:2.5709... 0.3999 sec/batch\n",
      "Epoch:4/20... Training Step:749... Training loss:2.5403... 0.3950 sec/batch\n",
      "Epoch:4/20... Training Step:750... Training loss:2.5506... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:751... Training loss:2.5295... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:752... Training loss:2.5433... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:753... Training loss:2.5216... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:754... Training loss:2.5651... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:755... Training loss:2.5475... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:756... Training loss:2.5139... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:757... Training loss:2.5200... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:758... Training loss:2.5424... 0.3939 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/20... Training Step:759... Training loss:2.5502... 0.4009 sec/batch\n",
      "Epoch:4/20... Training Step:760... Training loss:2.5293... 0.3890 sec/batch\n",
      "Epoch:4/20... Training Step:761... Training loss:2.5391... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:762... Training loss:2.5329... 0.3989 sec/batch\n",
      "Epoch:4/20... Training Step:763... Training loss:2.5483... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:764... Training loss:2.5093... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:765... Training loss:2.5335... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:766... Training loss:2.5372... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:767... Training loss:2.5581... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:768... Training loss:2.5620... 0.3929 sec/batch\n",
      "Epoch:4/20... Training Step:769... Training loss:2.5507... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:770... Training loss:2.5394... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:771... Training loss:2.5410... 0.3900 sec/batch\n",
      "Epoch:4/20... Training Step:772... Training loss:2.5138... 0.3939 sec/batch\n",
      "Epoch:4/20... Training Step:773... Training loss:2.5073... 0.4089 sec/batch\n",
      "Epoch:4/20... Training Step:774... Training loss:2.5080... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:775... Training loss:2.5268... 0.3850 sec/batch\n",
      "Epoch:4/20... Training Step:776... Training loss:2.5266... 0.3959 sec/batch\n",
      "Epoch:4/20... Training Step:777... Training loss:2.5119... 0.3810 sec/batch\n",
      "Epoch:4/20... Training Step:778... Training loss:2.5410... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:779... Training loss:2.5544... 0.3880 sec/batch\n",
      "Epoch:4/20... Training Step:780... Training loss:2.5242... 0.3840 sec/batch\n",
      "Epoch:4/20... Training Step:781... Training loss:2.5072... 0.4039 sec/batch\n",
      "Epoch:4/20... Training Step:782... Training loss:2.4936... 0.3910 sec/batch\n",
      "Epoch:4/20... Training Step:783... Training loss:2.5003... 0.3920 sec/batch\n",
      "Epoch:4/20... Training Step:784... Training loss:2.5162... 0.4079 sec/batch\n",
      "Epoch:4/20... Training Step:785... Training loss:2.5243... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:786... Training loss:2.4898... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:787... Training loss:2.5206... 0.3949 sec/batch\n",
      "Epoch:4/20... Training Step:788... Training loss:2.4993... 0.3979 sec/batch\n",
      "Epoch:4/20... Training Step:789... Training loss:2.5055... 0.3980 sec/batch\n",
      "Epoch:4/20... Training Step:790... Training loss:2.5012... 0.3870 sec/batch\n",
      "Epoch:4/20... Training Step:791... Training loss:2.5146... 0.3860 sec/batch\n",
      "Epoch:4/20... Training Step:792... Training loss:2.4985... 0.3840 sec/batch\n",
      "Epoch:5/20... Training Step:793... Training loss:2.7575... 0.3850 sec/batch\n",
      "Epoch:5/20... Training Step:794... Training loss:2.4990... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:795... Training loss:2.5084... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:796... Training loss:2.5194... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:797... Training loss:2.5061... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:798... Training loss:2.5142... 0.3930 sec/batch\n",
      "Epoch:5/20... Training Step:799... Training loss:2.5235... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:800... Training loss:2.5062... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:801... Training loss:2.5437... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:802... Training loss:2.5163... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:803... Training loss:2.5080... 0.3869 sec/batch\n",
      "Epoch:5/20... Training Step:804... Training loss:2.5161... 0.3960 sec/batch\n",
      "Epoch:5/20... Training Step:805... Training loss:2.5127... 0.3900 sec/batch\n",
      "Epoch:5/20... Training Step:806... Training loss:2.5466... 0.4000 sec/batch\n",
      "Epoch:5/20... Training Step:807... Training loss:2.5210... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:808... Training loss:2.5154... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:809... Training loss:2.5166... 0.3989 sec/batch\n",
      "Epoch:5/20... Training Step:810... Training loss:2.5512... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:811... Training loss:2.5299... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:812... Training loss:2.5006... 0.3999 sec/batch\n",
      "Epoch:5/20... Training Step:813... Training loss:2.5114... 0.3850 sec/batch\n",
      "Epoch:5/20... Training Step:814... Training loss:2.5420... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:815... Training loss:2.5062... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:816... Training loss:2.5075... 0.3960 sec/batch\n",
      "Epoch:5/20... Training Step:817... Training loss:2.5043... 0.3960 sec/batch\n",
      "Epoch:5/20... Training Step:818... Training loss:2.5020... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:819... Training loss:2.5100... 0.3860 sec/batch\n",
      "Epoch:5/20... Training Step:820... Training loss:2.5015... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:821... Training loss:2.5143... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:822... Training loss:2.5145... 0.3919 sec/batch\n",
      "Epoch:5/20... Training Step:823... Training loss:2.5356... 0.3970 sec/batch\n",
      "Epoch:5/20... Training Step:824... Training loss:2.5033... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:825... Training loss:2.4893... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:826... Training loss:2.5158... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:827... Training loss:2.4944... 0.3930 sec/batch\n",
      "Epoch:5/20... Training Step:828... Training loss:2.5124... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:829... Training loss:2.4957... 0.3919 sec/batch\n",
      "Epoch:5/20... Training Step:830... Training loss:2.4722... 0.3840 sec/batch\n",
      "Epoch:5/20... Training Step:831... Training loss:2.4948... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:832... Training loss:2.4879... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:833... Training loss:2.4822... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:834... Training loss:2.4860... 0.3860 sec/batch\n",
      "Epoch:5/20... Training Step:835... Training loss:2.4800... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:836... Training loss:2.4824... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:837... Training loss:2.4800... 0.4069 sec/batch\n",
      "Epoch:5/20... Training Step:838... Training loss:2.4576... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:839... Training loss:2.5040... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:840... Training loss:2.4864... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:841... Training loss:2.4855... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:842... Training loss:2.5158... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:843... Training loss:2.4750... 0.3790 sec/batch\n",
      "Epoch:5/20... Training Step:844... Training loss:2.4890... 0.3970 sec/batch\n",
      "Epoch:5/20... Training Step:845... Training loss:2.4832... 0.3930 sec/batch\n",
      "Epoch:5/20... Training Step:846... Training loss:2.4787... 0.3870 sec/batch\n",
      "Epoch:5/20... Training Step:847... Training loss:2.4712... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:848... Training loss:2.4865... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:849... Training loss:2.4827... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:850... Training loss:2.4758... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:851... Training loss:2.4801... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:852... Training loss:2.4905... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:853... Training loss:2.4785... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:854... Training loss:2.4792... 0.4039 sec/batch\n",
      "Epoch:5/20... Training Step:855... Training loss:2.5009... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:856... Training loss:2.4786... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:857... Training loss:2.4688... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:858... Training loss:2.5026... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:859... Training loss:2.4780... 0.4089 sec/batch\n",
      "Epoch:5/20... Training Step:860... Training loss:2.4475... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:861... Training loss:2.4504... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:862... Training loss:2.4800... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:863... Training loss:2.4927... 0.3900 sec/batch\n",
      "Epoch:5/20... Training Step:864... Training loss:2.4821... 0.3810 sec/batch\n",
      "Epoch:5/20... Training Step:865... Training loss:2.4822... 0.3940 sec/batch\n",
      "Epoch:5/20... Training Step:866... Training loss:2.4679... 0.3800 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/20... Training Step:867... Training loss:2.4828... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:868... Training loss:2.5163... 0.3909 sec/batch\n",
      "Epoch:5/20... Training Step:869... Training loss:2.4785... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:870... Training loss:2.4750... 0.3980 sec/batch\n",
      "Epoch:5/20... Training Step:871... Training loss:2.4770... 0.3919 sec/batch\n",
      "Epoch:5/20... Training Step:872... Training loss:2.4677... 0.3950 sec/batch\n",
      "Epoch:5/20... Training Step:873... Training loss:2.4615... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:874... Training loss:2.4896... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:875... Training loss:2.4781... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:876... Training loss:2.4482... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:877... Training loss:2.4369... 0.3840 sec/batch\n",
      "Epoch:5/20... Training Step:878... Training loss:2.4639... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:879... Training loss:2.4678... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:880... Training loss:2.4724... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:881... Training loss:2.4574... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:882... Training loss:2.4812... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:883... Training loss:2.4605... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:884... Training loss:2.4767... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:885... Training loss:2.4711... 0.4049 sec/batch\n",
      "Epoch:5/20... Training Step:886... Training loss:2.4608... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:887... Training loss:2.4380... 0.3919 sec/batch\n",
      "Epoch:5/20... Training Step:888... Training loss:2.4496... 0.3840 sec/batch\n",
      "Epoch:5/20... Training Step:889... Training loss:2.4652... 0.3869 sec/batch\n",
      "Epoch:5/20... Training Step:890... Training loss:2.4610... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:891... Training loss:2.4582... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:892... Training loss:2.4521... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:893... Training loss:2.4825... 0.3999 sec/batch\n",
      "Epoch:5/20... Training Step:894... Training loss:2.4612... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:895... Training loss:2.4512... 0.3889 sec/batch\n",
      "Epoch:5/20... Training Step:896... Training loss:2.4486... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:897... Training loss:2.4599... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:898... Training loss:2.4586... 0.3970 sec/batch\n",
      "Epoch:5/20... Training Step:899... Training loss:2.4433... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:900... Training loss:2.4943... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:901... Training loss:2.4795... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:902... Training loss:2.4355... 0.3860 sec/batch\n",
      "Epoch:5/20... Training Step:903... Training loss:2.4619... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:904... Training loss:2.4817... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:905... Training loss:2.4594... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:906... Training loss:2.4556... 0.3980 sec/batch\n",
      "Epoch:5/20... Training Step:907... Training loss:2.4472... 0.4039 sec/batch\n",
      "Epoch:5/20... Training Step:908... Training loss:2.4246... 0.4009 sec/batch\n",
      "Epoch:5/20... Training Step:909... Training loss:2.4512... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:910... Training loss:2.4651... 0.3919 sec/batch\n",
      "Epoch:5/20... Training Step:911... Training loss:2.4834... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:912... Training loss:2.4730... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:913... Training loss:2.4867... 0.3989 sec/batch\n",
      "Epoch:5/20... Training Step:914... Training loss:2.4628... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:915... Training loss:2.4490... 0.4019 sec/batch\n",
      "Epoch:5/20... Training Step:916... Training loss:2.4722... 0.3900 sec/batch\n",
      "Epoch:5/20... Training Step:917... Training loss:2.4586... 0.4019 sec/batch\n",
      "Epoch:5/20... Training Step:918... Training loss:2.4310... 0.3860 sec/batch\n",
      "Epoch:5/20... Training Step:919... Training loss:2.4714... 0.3989 sec/batch\n",
      "Epoch:5/20... Training Step:920... Training loss:2.4687... 0.3840 sec/batch\n",
      "Epoch:5/20... Training Step:921... Training loss:2.4556... 0.3940 sec/batch\n",
      "Epoch:5/20... Training Step:922... Training loss:2.4602... 0.3820 sec/batch\n",
      "Epoch:5/20... Training Step:923... Training loss:2.4498... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:924... Training loss:2.4361... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:925... Training loss:2.4645... 0.3989 sec/batch\n",
      "Epoch:5/20... Training Step:926... Training loss:2.4767... 0.4000 sec/batch\n",
      "Epoch:5/20... Training Step:927... Training loss:2.4416... 0.4029 sec/batch\n",
      "Epoch:5/20... Training Step:928... Training loss:2.4442... 0.3870 sec/batch\n",
      "Epoch:5/20... Training Step:929... Training loss:2.4513... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:930... Training loss:2.4672... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:931... Training loss:2.4828... 0.4019 sec/batch\n",
      "Epoch:5/20... Training Step:932... Training loss:2.4357... 0.3920 sec/batch\n",
      "Epoch:5/20... Training Step:933... Training loss:2.4799... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:934... Training loss:2.4401... 0.3810 sec/batch\n",
      "Epoch:5/20... Training Step:935... Training loss:2.4629... 0.3889 sec/batch\n",
      "Epoch:5/20... Training Step:936... Training loss:2.4453... 0.3960 sec/batch\n",
      "Epoch:5/20... Training Step:937... Training loss:2.4477... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:938... Training loss:2.4780... 0.3980 sec/batch\n",
      "Epoch:5/20... Training Step:939... Training loss:2.4584... 0.4019 sec/batch\n",
      "Epoch:5/20... Training Step:940... Training loss:2.4882... 0.3820 sec/batch\n",
      "Epoch:5/20... Training Step:941... Training loss:2.4453... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:942... Training loss:2.4457... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:943... Training loss:2.4905... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:944... Training loss:2.5000... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:945... Training loss:2.4612... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:946... Training loss:2.4657... 0.4119 sec/batch\n",
      "Epoch:5/20... Training Step:947... Training loss:2.4400... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:948... Training loss:2.4524... 0.3889 sec/batch\n",
      "Epoch:5/20... Training Step:949... Training loss:2.4386... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:950... Training loss:2.4480... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:951... Training loss:2.4254... 0.3900 sec/batch\n",
      "Epoch:5/20... Training Step:952... Training loss:2.4866... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:953... Training loss:2.4557... 0.3970 sec/batch\n",
      "Epoch:5/20... Training Step:954... Training loss:2.4195... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:955... Training loss:2.4329... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:956... Training loss:2.4488... 0.3899 sec/batch\n",
      "Epoch:5/20... Training Step:957... Training loss:2.4552... 0.3899 sec/batch\n",
      "Epoch:5/20... Training Step:958... Training loss:2.4536... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:959... Training loss:2.4528... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:960... Training loss:2.4504... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:961... Training loss:2.4533... 0.4079 sec/batch\n",
      "Epoch:5/20... Training Step:962... Training loss:2.4173... 0.4019 sec/batch\n",
      "Epoch:5/20... Training Step:963... Training loss:2.4348... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:964... Training loss:2.4446... 0.3929 sec/batch\n",
      "Epoch:5/20... Training Step:965... Training loss:2.4696... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:966... Training loss:2.4616... 0.3880 sec/batch\n",
      "Epoch:5/20... Training Step:967... Training loss:2.4618... 0.3850 sec/batch\n",
      "Epoch:5/20... Training Step:968... Training loss:2.4519... 0.3910 sec/batch\n",
      "Epoch:5/20... Training Step:969... Training loss:2.4292... 0.3950 sec/batch\n",
      "Epoch:5/20... Training Step:970... Training loss:2.4230... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:971... Training loss:2.4220... 0.3969 sec/batch\n",
      "Epoch:5/20... Training Step:972... Training loss:2.4239... 0.3930 sec/batch\n",
      "Epoch:5/20... Training Step:973... Training loss:2.4361... 0.3950 sec/batch\n",
      "Epoch:5/20... Training Step:974... Training loss:2.4389... 0.4049 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/20... Training Step:975... Training loss:2.4280... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:976... Training loss:2.4540... 0.3939 sec/batch\n",
      "Epoch:5/20... Training Step:977... Training loss:2.4586... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:978... Training loss:2.4280... 0.3870 sec/batch\n",
      "Epoch:5/20... Training Step:979... Training loss:2.4166... 0.3990 sec/batch\n",
      "Epoch:5/20... Training Step:980... Training loss:2.4175... 0.3930 sec/batch\n",
      "Epoch:5/20... Training Step:981... Training loss:2.4215... 0.3890 sec/batch\n",
      "Epoch:5/20... Training Step:982... Training loss:2.4188... 0.3949 sec/batch\n",
      "Epoch:5/20... Training Step:983... Training loss:2.4474... 0.3889 sec/batch\n",
      "Epoch:5/20... Training Step:984... Training loss:2.3987... 0.3850 sec/batch\n",
      "Epoch:5/20... Training Step:985... Training loss:2.4366... 0.3830 sec/batch\n",
      "Epoch:5/20... Training Step:986... Training loss:2.4272... 0.3959 sec/batch\n",
      "Epoch:5/20... Training Step:987... Training loss:2.4110... 0.4099 sec/batch\n",
      "Epoch:5/20... Training Step:988... Training loss:2.4194... 0.3999 sec/batch\n",
      "Epoch:5/20... Training Step:989... Training loss:2.4140... 0.3979 sec/batch\n",
      "Epoch:5/20... Training Step:990... Training loss:2.4083... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:991... Training loss:2.6483... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:992... Training loss:2.3990... 0.3950 sec/batch\n",
      "Epoch:6/20... Training Step:993... Training loss:2.4080... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:994... Training loss:2.4288... 0.4049 sec/batch\n",
      "Epoch:6/20... Training Step:995... Training loss:2.4276... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:996... Training loss:2.4156... 0.3840 sec/batch\n",
      "Epoch:6/20... Training Step:997... Training loss:2.4320... 0.3820 sec/batch\n",
      "Epoch:6/20... Training Step:998... Training loss:2.4306... 0.3960 sec/batch\n",
      "Epoch:6/20... Training Step:999... Training loss:2.4533... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1000... Training loss:2.4192... 0.3810 sec/batch\n",
      "Epoch:6/20... Training Step:1001... Training loss:2.4132... 0.3840 sec/batch\n",
      "Epoch:6/20... Training Step:1002... Training loss:2.4298... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1003... Training loss:2.4268... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1004... Training loss:2.4685... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1005... Training loss:2.4321... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1006... Training loss:2.4284... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1007... Training loss:2.4306... 0.3960 sec/batch\n",
      "Epoch:6/20... Training Step:1008... Training loss:2.4652... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1009... Training loss:2.4427... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1010... Training loss:2.4111... 0.3800 sec/batch\n",
      "Epoch:6/20... Training Step:1011... Training loss:2.4283... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1012... Training loss:2.4652... 0.3860 sec/batch\n",
      "Epoch:6/20... Training Step:1013... Training loss:2.4275... 0.3959 sec/batch\n",
      "Epoch:6/20... Training Step:1014... Training loss:2.4178... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1015... Training loss:2.4125... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1016... Training loss:2.4314... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1017... Training loss:2.4167... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1018... Training loss:2.4258... 0.3850 sec/batch\n",
      "Epoch:6/20... Training Step:1019... Training loss:2.4411... 0.3820 sec/batch\n",
      "Epoch:6/20... Training Step:1020... Training loss:2.4299... 0.4039 sec/batch\n",
      "Epoch:6/20... Training Step:1021... Training loss:2.4447... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1022... Training loss:2.4212... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1023... Training loss:2.4054... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1024... Training loss:2.4405... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1025... Training loss:2.4068... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1026... Training loss:2.4343... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1027... Training loss:2.4106... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1028... Training loss:2.3886... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1029... Training loss:2.4043... 0.3790 sec/batch\n",
      "Epoch:6/20... Training Step:1030... Training loss:2.4043... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1031... Training loss:2.3954... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1032... Training loss:2.3911... 0.3800 sec/batch\n",
      "Epoch:6/20... Training Step:1033... Training loss:2.4016... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1034... Training loss:2.3955... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1035... Training loss:2.4040... 0.3980 sec/batch\n",
      "Epoch:6/20... Training Step:1036... Training loss:2.3737... 0.4079 sec/batch\n",
      "Epoch:6/20... Training Step:1037... Training loss:2.4218... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1038... Training loss:2.4105... 0.3889 sec/batch\n",
      "Epoch:6/20... Training Step:1039... Training loss:2.4081... 0.3909 sec/batch\n",
      "Epoch:6/20... Training Step:1040... Training loss:2.4281... 0.3850 sec/batch\n",
      "Epoch:6/20... Training Step:1041... Training loss:2.3950... 0.3830 sec/batch\n",
      "Epoch:6/20... Training Step:1042... Training loss:2.4244... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1043... Training loss:2.4036... 0.3959 sec/batch\n",
      "Epoch:6/20... Training Step:1044... Training loss:2.4035... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1045... Training loss:2.4038... 0.3830 sec/batch\n",
      "Epoch:6/20... Training Step:1046... Training loss:2.4160... 0.3810 sec/batch\n",
      "Epoch:6/20... Training Step:1047... Training loss:2.4074... 0.3840 sec/batch\n",
      "Epoch:6/20... Training Step:1048... Training loss:2.3919... 0.3999 sec/batch\n",
      "Epoch:6/20... Training Step:1049... Training loss:2.3888... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1050... Training loss:2.4256... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1051... Training loss:2.3928... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1052... Training loss:2.4207... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1053... Training loss:2.4313... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1054... Training loss:2.4042... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1055... Training loss:2.4046... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1056... Training loss:2.4228... 0.3869 sec/batch\n",
      "Epoch:6/20... Training Step:1057... Training loss:2.4116... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1058... Training loss:2.3682... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1059... Training loss:2.3832... 0.4039 sec/batch\n",
      "Epoch:6/20... Training Step:1060... Training loss:2.4022... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1061... Training loss:2.4131... 0.3860 sec/batch\n",
      "Epoch:6/20... Training Step:1062... Training loss:2.4079... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1063... Training loss:2.4051... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1064... Training loss:2.3933... 0.3920 sec/batch\n",
      "Epoch:6/20... Training Step:1065... Training loss:2.3966... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1066... Training loss:2.4464... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1067... Training loss:2.3963... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1068... Training loss:2.4092... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1069... Training loss:2.3851... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1070... Training loss:2.3836... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1071... Training loss:2.3778... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1072... Training loss:2.4196... 0.3920 sec/batch\n",
      "Epoch:6/20... Training Step:1073... Training loss:2.3883... 0.3830 sec/batch\n",
      "Epoch:6/20... Training Step:1074... Training loss:2.3739... 0.4059 sec/batch\n",
      "Epoch:6/20... Training Step:1075... Training loss:2.3505... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1076... Training loss:2.3836... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1077... Training loss:2.3880... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1078... Training loss:2.3931... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1079... Training loss:2.3813... 0.3979 sec/batch\n",
      "Epoch:6/20... Training Step:1080... Training loss:2.4076... 0.3830 sec/batch\n",
      "Epoch:6/20... Training Step:1081... Training loss:2.3856... 0.3850 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6/20... Training Step:1082... Training loss:2.4029... 0.3850 sec/batch\n",
      "Epoch:6/20... Training Step:1083... Training loss:2.3852... 0.4039 sec/batch\n",
      "Epoch:6/20... Training Step:1084... Training loss:2.3765... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1085... Training loss:2.3776... 0.3810 sec/batch\n",
      "Epoch:6/20... Training Step:1086... Training loss:2.3853... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1087... Training loss:2.3972... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1088... Training loss:2.3889... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1089... Training loss:2.3876... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1090... Training loss:2.3821... 0.3790 sec/batch\n",
      "Epoch:6/20... Training Step:1091... Training loss:2.4013... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1092... Training loss:2.3937... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1093... Training loss:2.3669... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1094... Training loss:2.3854... 0.3999 sec/batch\n",
      "Epoch:6/20... Training Step:1095... Training loss:2.3883... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1096... Training loss:2.3833... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1097... Training loss:2.3763... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1098... Training loss:2.4187... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1099... Training loss:2.4014... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1100... Training loss:2.3720... 0.3889 sec/batch\n",
      "Epoch:6/20... Training Step:1101... Training loss:2.3907... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1102... Training loss:2.4010... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1103... Training loss:2.3875... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1104... Training loss:2.3697... 0.3870 sec/batch\n",
      "Epoch:6/20... Training Step:1105... Training loss:2.3744... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1106... Training loss:2.3457... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1107... Training loss:2.3844... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1108... Training loss:2.3825... 0.3909 sec/batch\n",
      "Epoch:6/20... Training Step:1109... Training loss:2.4142... 0.3960 sec/batch\n",
      "Epoch:6/20... Training Step:1110... Training loss:2.3958... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1111... Training loss:2.4123... 0.3810 sec/batch\n",
      "Epoch:6/20... Training Step:1112... Training loss:2.3788... 0.3919 sec/batch\n",
      "Epoch:6/20... Training Step:1113... Training loss:2.3799... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1114... Training loss:2.3966... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1115... Training loss:2.3866... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1116... Training loss:2.3632... 0.3919 sec/batch\n",
      "Epoch:6/20... Training Step:1117... Training loss:2.4025... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1118... Training loss:2.4015... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1119... Training loss:2.3858... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1120... Training loss:2.3982... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1121... Training loss:2.3746... 0.3950 sec/batch\n",
      "Epoch:6/20... Training Step:1122... Training loss:2.3623... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1123... Training loss:2.3993... 0.3820 sec/batch\n",
      "Epoch:6/20... Training Step:1124... Training loss:2.3925... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1125... Training loss:2.3698... 0.3840 sec/batch\n",
      "Epoch:6/20... Training Step:1126... Training loss:2.3869... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1127... Training loss:2.3794... 0.3840 sec/batch\n",
      "Epoch:6/20... Training Step:1128... Training loss:2.3841... 0.3909 sec/batch\n",
      "Epoch:6/20... Training Step:1129... Training loss:2.4134... 0.4029 sec/batch\n",
      "Epoch:6/20... Training Step:1130... Training loss:2.3665... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1131... Training loss:2.4034... 0.3949 sec/batch\n",
      "Epoch:6/20... Training Step:1132... Training loss:2.3704... 0.3850 sec/batch\n",
      "Epoch:6/20... Training Step:1133... Training loss:2.3822... 0.4069 sec/batch\n",
      "Epoch:6/20... Training Step:1134... Training loss:2.3744... 0.4059 sec/batch\n",
      "Epoch:6/20... Training Step:1135... Training loss:2.3797... 0.4119 sec/batch\n",
      "Epoch:6/20... Training Step:1136... Training loss:2.4110... 0.3980 sec/batch\n",
      "Epoch:6/20... Training Step:1137... Training loss:2.3830... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1138... Training loss:2.4156... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1139... Training loss:2.3817... 0.3959 sec/batch\n",
      "Epoch:6/20... Training Step:1140... Training loss:2.3744... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1141... Training loss:2.4155... 0.4009 sec/batch\n",
      "Epoch:6/20... Training Step:1142... Training loss:2.4256... 0.4049 sec/batch\n",
      "Epoch:6/20... Training Step:1143... Training loss:2.3995... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1144... Training loss:2.4016... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1145... Training loss:2.3881... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1146... Training loss:2.3868... 0.3939 sec/batch\n",
      "Epoch:6/20... Training Step:1147... Training loss:2.3730... 0.3859 sec/batch\n",
      "Epoch:6/20... Training Step:1148... Training loss:2.3832... 0.4019 sec/batch\n",
      "Epoch:6/20... Training Step:1149... Training loss:2.3528... 0.4029 sec/batch\n",
      "Epoch:6/20... Training Step:1150... Training loss:2.4191... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1151... Training loss:2.3849... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1152... Training loss:2.3588... 0.3900 sec/batch\n",
      "Epoch:6/20... Training Step:1153... Training loss:2.3773... 0.3990 sec/batch\n",
      "Epoch:6/20... Training Step:1154... Training loss:2.3754... 0.3999 sec/batch\n",
      "Epoch:6/20... Training Step:1155... Training loss:2.3816... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1156... Training loss:2.3812... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1157... Training loss:2.3887... 0.4099 sec/batch\n",
      "Epoch:6/20... Training Step:1158... Training loss:2.3843... 0.4029 sec/batch\n",
      "Epoch:6/20... Training Step:1159... Training loss:2.3872... 0.4159 sec/batch\n",
      "Epoch:6/20... Training Step:1160... Training loss:2.3618... 0.4129 sec/batch\n",
      "Epoch:6/20... Training Step:1161... Training loss:2.3728... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1162... Training loss:2.3832... 0.4049 sec/batch\n",
      "Epoch:6/20... Training Step:1163... Training loss:2.4013... 0.4009 sec/batch\n",
      "Epoch:6/20... Training Step:1164... Training loss:2.3893... 0.3880 sec/batch\n",
      "Epoch:6/20... Training Step:1165... Training loss:2.3887... 0.4049 sec/batch\n",
      "Epoch:6/20... Training Step:1166... Training loss:2.3878... 0.4049 sec/batch\n",
      "Epoch:6/20... Training Step:1167... Training loss:2.3696... 0.3989 sec/batch\n",
      "Epoch:6/20... Training Step:1168... Training loss:2.3591... 0.4089 sec/batch\n",
      "Epoch:6/20... Training Step:1169... Training loss:2.3601... 0.3999 sec/batch\n",
      "Epoch:6/20... Training Step:1170... Training loss:2.3540... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1171... Training loss:2.3597... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1172... Training loss:2.3756... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1173... Training loss:2.3622... 0.3910 sec/batch\n",
      "Epoch:6/20... Training Step:1174... Training loss:2.3915... 0.3929 sec/batch\n",
      "Epoch:6/20... Training Step:1175... Training loss:2.3900... 0.4029 sec/batch\n",
      "Epoch:6/20... Training Step:1176... Training loss:2.3715... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1177... Training loss:2.3533... 0.4039 sec/batch\n",
      "Epoch:6/20... Training Step:1178... Training loss:2.3467... 0.3940 sec/batch\n",
      "Epoch:6/20... Training Step:1179... Training loss:2.3571... 0.3969 sec/batch\n",
      "Epoch:6/20... Training Step:1180... Training loss:2.3561... 0.4029 sec/batch\n",
      "Epoch:6/20... Training Step:1181... Training loss:2.3676... 0.4089 sec/batch\n",
      "Epoch:6/20... Training Step:1182... Training loss:2.3306... 0.4069 sec/batch\n",
      "Epoch:6/20... Training Step:1183... Training loss:2.3638... 0.3930 sec/batch\n",
      "Epoch:6/20... Training Step:1184... Training loss:2.3575... 0.4059 sec/batch\n",
      "Epoch:6/20... Training Step:1185... Training loss:2.3411... 0.3990 sec/batch\n",
      "Epoch:6/20... Training Step:1186... Training loss:2.3521... 0.3890 sec/batch\n",
      "Epoch:6/20... Training Step:1187... Training loss:2.3467... 0.4039 sec/batch\n",
      "Epoch:6/20... Training Step:1188... Training loss:2.3454... 0.3949 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/20... Training Step:1189... Training loss:2.5727... 0.3949 sec/batch\n",
      "Epoch:7/20... Training Step:1190... Training loss:2.3333... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1191... Training loss:2.3536... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1192... Training loss:2.3690... 0.3959 sec/batch\n",
      "Epoch:7/20... Training Step:1193... Training loss:2.3674... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1194... Training loss:2.3558... 0.3999 sec/batch\n",
      "Epoch:7/20... Training Step:1195... Training loss:2.3690... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1196... Training loss:2.3743... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1197... Training loss:2.3880... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1198... Training loss:2.3586... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1199... Training loss:2.3464... 0.3979 sec/batch\n",
      "Epoch:7/20... Training Step:1200... Training loss:2.3637... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1201... Training loss:2.3633... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1202... Training loss:2.4058... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1203... Training loss:2.3609... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1204... Training loss:2.3609... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1205... Training loss:2.3707... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1206... Training loss:2.4110... 0.4039 sec/batch\n",
      "Epoch:7/20... Training Step:1207... Training loss:2.3741... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1208... Training loss:2.3584... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1209... Training loss:2.3643... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1210... Training loss:2.4040... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1211... Training loss:2.3767... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1212... Training loss:2.3584... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1213... Training loss:2.3587... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1214... Training loss:2.3600... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1215... Training loss:2.3603... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1216... Training loss:2.3642... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1217... Training loss:2.3814... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1218... Training loss:2.3683... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1219... Training loss:2.3727... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1220... Training loss:2.3502... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1221... Training loss:2.3367... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1222... Training loss:2.3742... 0.3999 sec/batch\n",
      "Epoch:7/20... Training Step:1223... Training loss:2.3500... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1224... Training loss:2.3579... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1225... Training loss:2.3538... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1226... Training loss:2.3224... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1227... Training loss:2.3436... 0.3980 sec/batch\n",
      "Epoch:7/20... Training Step:1228... Training loss:2.3354... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1229... Training loss:2.3397... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1230... Training loss:2.3387... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1231... Training loss:2.3304... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1232... Training loss:2.3273... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1233... Training loss:2.3342... 0.3989 sec/batch\n",
      "Epoch:7/20... Training Step:1234... Training loss:2.3021... 0.3990 sec/batch\n",
      "Epoch:7/20... Training Step:1235... Training loss:2.3549... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1236... Training loss:2.3309... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1237... Training loss:2.3440... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1238... Training loss:2.3689... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1239... Training loss:2.3148... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1240... Training loss:2.3561... 0.4039 sec/batch\n",
      "Epoch:7/20... Training Step:1241... Training loss:2.3378... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1242... Training loss:2.3359... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1243... Training loss:2.3351... 0.3999 sec/batch\n",
      "Epoch:7/20... Training Step:1244... Training loss:2.3546... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1245... Training loss:2.3417... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1246... Training loss:2.3412... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1247... Training loss:2.3363... 0.3950 sec/batch\n",
      "Epoch:7/20... Training Step:1248... Training loss:2.3669... 0.3960 sec/batch\n",
      "Epoch:7/20... Training Step:1249... Training loss:2.3339... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1250... Training loss:2.3609... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1251... Training loss:2.3657... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1252... Training loss:2.3512... 0.4199 sec/batch\n",
      "Epoch:7/20... Training Step:1253... Training loss:2.3363... 0.4039 sec/batch\n",
      "Epoch:7/20... Training Step:1254... Training loss:2.3671... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1255... Training loss:2.3548... 0.4328 sec/batch\n",
      "Epoch:7/20... Training Step:1256... Training loss:2.3053... 0.4279 sec/batch\n",
      "Epoch:7/20... Training Step:1257... Training loss:2.3161... 0.4259 sec/batch\n",
      "Epoch:7/20... Training Step:1258... Training loss:2.3524... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1259... Training loss:2.3570... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1260... Training loss:2.3601... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1261... Training loss:2.3532... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1262... Training loss:2.3319... 0.4328 sec/batch\n",
      "Epoch:7/20... Training Step:1263... Training loss:2.3394... 0.4388 sec/batch\n",
      "Epoch:7/20... Training Step:1264... Training loss:2.3819... 0.4288 sec/batch\n",
      "Epoch:7/20... Training Step:1265... Training loss:2.3430... 0.4448 sec/batch\n",
      "Epoch:7/20... Training Step:1266... Training loss:2.3512... 0.4288 sec/batch\n",
      "Epoch:7/20... Training Step:1267... Training loss:2.3160... 0.4169 sec/batch\n",
      "Epoch:7/20... Training Step:1268... Training loss:2.3291... 0.4209 sec/batch\n",
      "Epoch:7/20... Training Step:1269... Training loss:2.3141... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1270... Training loss:2.3625... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1271... Training loss:2.3355... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1272... Training loss:2.3125... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1273... Training loss:2.2897... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1274... Training loss:2.3202... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1275... Training loss:2.3356... 0.3979 sec/batch\n",
      "Epoch:7/20... Training Step:1276... Training loss:2.3320... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1277... Training loss:2.3166... 0.3979 sec/batch\n",
      "Epoch:7/20... Training Step:1278... Training loss:2.3463... 0.3960 sec/batch\n",
      "Epoch:7/20... Training Step:1279... Training loss:2.3260... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1280... Training loss:2.3349... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1281... Training loss:2.3191... 0.4259 sec/batch\n",
      "Epoch:7/20... Training Step:1282... Training loss:2.3151... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1283... Training loss:2.3220... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1284... Training loss:2.3234... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1285... Training loss:2.3282... 0.4039 sec/batch\n",
      "Epoch:7/20... Training Step:1286... Training loss:2.3303... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1287... Training loss:2.3262... 0.4338 sec/batch\n",
      "Epoch:7/20... Training Step:1288... Training loss:2.3127... 0.4398 sec/batch\n",
      "Epoch:7/20... Training Step:1289... Training loss:2.3508... 0.4308 sec/batch\n",
      "Epoch:7/20... Training Step:1290... Training loss:2.3405... 0.4229 sec/batch\n",
      "Epoch:7/20... Training Step:1291... Training loss:2.3080... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1292... Training loss:2.3187... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1293... Training loss:2.3156... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1294... Training loss:2.3263... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1295... Training loss:2.3155... 0.4039 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7/20... Training Step:1296... Training loss:2.3603... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1297... Training loss:2.3419... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1298... Training loss:2.3186... 0.3959 sec/batch\n",
      "Epoch:7/20... Training Step:1299... Training loss:2.3332... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1300... Training loss:2.3502... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1301... Training loss:2.3262... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1302... Training loss:2.3142... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1303... Training loss:2.3204... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1304... Training loss:2.2990... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1305... Training loss:2.3347... 0.3910 sec/batch\n",
      "Epoch:7/20... Training Step:1306... Training loss:2.3313... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1307... Training loss:2.3511... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1308... Training loss:2.3294... 0.3990 sec/batch\n",
      "Epoch:7/20... Training Step:1309... Training loss:2.3498... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1310... Training loss:2.3188... 0.4209 sec/batch\n",
      "Epoch:7/20... Training Step:1311... Training loss:2.3121... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1312... Training loss:2.3358... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1313... Training loss:2.3277... 0.3949 sec/batch\n",
      "Epoch:7/20... Training Step:1314... Training loss:2.2979... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1315... Training loss:2.3338... 0.3999 sec/batch\n",
      "Epoch:7/20... Training Step:1316... Training loss:2.3363... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1317... Training loss:2.3232... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1318... Training loss:2.3352... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1319... Training loss:2.3189... 0.4039 sec/batch\n",
      "Epoch:7/20... Training Step:1320... Training loss:2.2926... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1321... Training loss:2.3345... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1322... Training loss:2.3389... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1323... Training loss:2.3203... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1324... Training loss:2.3263... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1325... Training loss:2.3170... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1326... Training loss:2.3258... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1327... Training loss:2.3557... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1328... Training loss:2.3062... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1329... Training loss:2.3333... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1330... Training loss:2.3121... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1331... Training loss:2.3209... 0.3979 sec/batch\n",
      "Epoch:7/20... Training Step:1332... Training loss:2.3135... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1333... Training loss:2.3192... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1334... Training loss:2.3500... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1335... Training loss:2.3238... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1336... Training loss:2.3475... 0.4169 sec/batch\n",
      "Epoch:7/20... Training Step:1337... Training loss:2.3183... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1338... Training loss:2.3126... 0.4239 sec/batch\n",
      "Epoch:7/20... Training Step:1339... Training loss:2.3603... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1340... Training loss:2.3811... 0.4179 sec/batch\n",
      "Epoch:7/20... Training Step:1341... Training loss:2.3337... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1342... Training loss:2.3374... 0.4059 sec/batch\n",
      "Epoch:7/20... Training Step:1343... Training loss:2.3111... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1344... Training loss:2.3164... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1345... Training loss:2.3050... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1346... Training loss:2.3145... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1347... Training loss:2.2873... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1348... Training loss:2.3559... 0.4089 sec/batch\n",
      "Epoch:7/20... Training Step:1349... Training loss:2.3289... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1350... Training loss:2.3048... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1351... Training loss:2.3134... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1352... Training loss:2.3268... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1353... Training loss:2.3354... 0.4169 sec/batch\n",
      "Epoch:7/20... Training Step:1354... Training loss:2.3255... 0.4239 sec/batch\n",
      "Epoch:7/20... Training Step:1355... Training loss:2.3182... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1356... Training loss:2.3261... 0.4169 sec/batch\n",
      "Epoch:7/20... Training Step:1357... Training loss:2.3297... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1358... Training loss:2.3066... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1359... Training loss:2.3066... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1360... Training loss:2.3151... 0.4189 sec/batch\n",
      "Epoch:7/20... Training Step:1361... Training loss:2.3361... 0.4328 sec/batch\n",
      "Epoch:7/20... Training Step:1362... Training loss:2.3264... 0.4209 sec/batch\n",
      "Epoch:7/20... Training Step:1363... Training loss:2.3292... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1364... Training loss:2.3283... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1365... Training loss:2.3048... 0.4239 sec/batch\n",
      "Epoch:7/20... Training Step:1366... Training loss:2.2981... 0.4029 sec/batch\n",
      "Epoch:7/20... Training Step:1367... Training loss:2.2958... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1368... Training loss:2.2901... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1369... Training loss:2.3082... 0.4099 sec/batch\n",
      "Epoch:7/20... Training Step:1370... Training loss:2.3165... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1371... Training loss:2.3015... 0.4019 sec/batch\n",
      "Epoch:7/20... Training Step:1372... Training loss:2.3350... 0.4069 sec/batch\n",
      "Epoch:7/20... Training Step:1373... Training loss:2.3355... 0.4109 sec/batch\n",
      "Epoch:7/20... Training Step:1374... Training loss:2.3098... 0.4129 sec/batch\n",
      "Epoch:7/20... Training Step:1375... Training loss:2.3297... 0.4139 sec/batch\n",
      "Epoch:7/20... Training Step:1376... Training loss:2.2974... 0.4119 sec/batch\n",
      "Epoch:7/20... Training Step:1377... Training loss:2.3106... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1378... Training loss:2.3037... 0.4049 sec/batch\n",
      "Epoch:7/20... Training Step:1379... Training loss:2.3305... 0.4009 sec/batch\n",
      "Epoch:7/20... Training Step:1380... Training loss:2.2921... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1381... Training loss:2.3152... 0.4159 sec/batch\n",
      "Epoch:7/20... Training Step:1382... Training loss:2.3052... 0.4149 sec/batch\n",
      "Epoch:7/20... Training Step:1383... Training loss:2.2977... 0.4299 sec/batch\n",
      "Epoch:7/20... Training Step:1384... Training loss:2.3168... 0.3990 sec/batch\n",
      "Epoch:7/20... Training Step:1385... Training loss:2.3052... 0.4079 sec/batch\n",
      "Epoch:7/20... Training Step:1386... Training loss:2.2880... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1387... Training loss:2.5236... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1388... Training loss:2.2788... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1389... Training loss:2.2858... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1390... Training loss:2.3042... 0.4278 sec/batch\n",
      "Epoch:8/20... Training Step:1391... Training loss:2.3116... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1392... Training loss:2.2904... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1393... Training loss:2.3163... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1394... Training loss:2.3177... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1395... Training loss:2.3319... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1396... Training loss:2.2994... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1397... Training loss:2.2933... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1398... Training loss:2.2958... 0.4049 sec/batch\n",
      "Epoch:8/20... Training Step:1399... Training loss:2.3093... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1400... Training loss:2.3464... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1401... Training loss:2.3068... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1402... Training loss:2.3144... 0.4149 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/20... Training Step:1403... Training loss:2.3079... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1404... Training loss:2.3447... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1405... Training loss:2.3209... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1406... Training loss:2.2933... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1407... Training loss:2.3070... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1408... Training loss:2.3498... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1409... Training loss:2.3167... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1410... Training loss:2.3130... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1411... Training loss:2.3048... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1412... Training loss:2.2958... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1413... Training loss:2.3045... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1414... Training loss:2.3081... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1415... Training loss:2.3245... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1416... Training loss:2.3088... 0.4029 sec/batch\n",
      "Epoch:8/20... Training Step:1417... Training loss:2.3200... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1418... Training loss:2.2951... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1419... Training loss:2.2880... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1420... Training loss:2.3229... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1421... Training loss:2.2938... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1422... Training loss:2.3091... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1423... Training loss:2.3071... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1424... Training loss:2.2692... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1425... Training loss:2.2802... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1426... Training loss:2.2809... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1427... Training loss:2.2807... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1428... Training loss:2.2841... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1429... Training loss:2.2796... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1430... Training loss:2.2828... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1431... Training loss:2.2966... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1432... Training loss:2.2466... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1433... Training loss:2.3130... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1434... Training loss:2.2928... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1435... Training loss:2.2882... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1436... Training loss:2.3124... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1437... Training loss:2.2725... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1438... Training loss:2.3150... 0.4049 sec/batch\n",
      "Epoch:8/20... Training Step:1439... Training loss:2.2811... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1440... Training loss:2.2892... 0.4019 sec/batch\n",
      "Epoch:8/20... Training Step:1441... Training loss:2.2840... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1442... Training loss:2.2995... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1443... Training loss:2.2993... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1444... Training loss:2.2794... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1445... Training loss:2.2878... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1446... Training loss:2.3207... 0.3989 sec/batch\n",
      "Epoch:8/20... Training Step:1447... Training loss:2.2736... 0.4219 sec/batch\n",
      "Epoch:8/20... Training Step:1448... Training loss:2.3149... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1449... Training loss:2.3235... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1450... Training loss:2.3020... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1451... Training loss:2.2842... 0.4029 sec/batch\n",
      "Epoch:8/20... Training Step:1452... Training loss:2.3175... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1453... Training loss:2.2904... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1454... Training loss:2.2592... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1455... Training loss:2.2740... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1456... Training loss:2.2978... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1457... Training loss:2.3174... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1458... Training loss:2.2977... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1459... Training loss:2.2947... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1460... Training loss:2.2790... 0.4019 sec/batch\n",
      "Epoch:8/20... Training Step:1461... Training loss:2.2823... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1462... Training loss:2.3229... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1463... Training loss:2.2851... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1464... Training loss:2.3047... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1465... Training loss:2.2709... 0.4039 sec/batch\n",
      "Epoch:8/20... Training Step:1466... Training loss:2.2758... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1467... Training loss:2.2568... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1468... Training loss:2.3056... 0.4219 sec/batch\n",
      "Epoch:8/20... Training Step:1469... Training loss:2.2730... 0.4019 sec/batch\n",
      "Epoch:8/20... Training Step:1470... Training loss:2.2651... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1471... Training loss:2.2459... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1472... Training loss:2.2781... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1473... Training loss:2.2807... 0.4029 sec/batch\n",
      "Epoch:8/20... Training Step:1474... Training loss:2.2746... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1475... Training loss:2.2618... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1476... Training loss:2.2981... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1477... Training loss:2.2658... 0.4049 sec/batch\n",
      "Epoch:8/20... Training Step:1478... Training loss:2.2831... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1479... Training loss:2.2636... 0.4219 sec/batch\n",
      "Epoch:8/20... Training Step:1480... Training loss:2.2562... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1481... Training loss:2.2643... 0.4039 sec/batch\n",
      "Epoch:8/20... Training Step:1482... Training loss:2.2668... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1483... Training loss:2.2792... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1484... Training loss:2.2674... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1485... Training loss:2.2586... 0.4239 sec/batch\n",
      "Epoch:8/20... Training Step:1486... Training loss:2.2602... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1487... Training loss:2.2978... 0.4368 sec/batch\n",
      "Epoch:8/20... Training Step:1488... Training loss:2.2755... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1489... Training loss:2.2555... 0.4358 sec/batch\n",
      "Epoch:8/20... Training Step:1490... Training loss:2.2683... 0.4328 sec/batch\n",
      "Epoch:8/20... Training Step:1491... Training loss:2.2668... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1492... Training loss:2.2780... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1493... Training loss:2.2677... 0.4229 sec/batch\n",
      "Epoch:8/20... Training Step:1494... Training loss:2.2934... 0.4229 sec/batch\n",
      "Epoch:8/20... Training Step:1495... Training loss:2.2854... 0.4358 sec/batch\n",
      "Epoch:8/20... Training Step:1496... Training loss:2.2617... 0.4338 sec/batch\n",
      "Epoch:8/20... Training Step:1497... Training loss:2.2804... 0.5595 sec/batch\n",
      "Epoch:8/20... Training Step:1498... Training loss:2.2830... 0.4279 sec/batch\n",
      "Epoch:8/20... Training Step:1499... Training loss:2.2646... 0.4528 sec/batch\n",
      "Epoch:8/20... Training Step:1500... Training loss:2.2672... 0.4438 sec/batch\n",
      "Epoch:8/20... Training Step:1501... Training loss:2.2618... 0.4448 sec/batch\n",
      "Epoch:8/20... Training Step:1502... Training loss:2.2389... 0.4348 sec/batch\n",
      "Epoch:8/20... Training Step:1503... Training loss:2.2765... 0.4229 sec/batch\n",
      "Epoch:8/20... Training Step:1504... Training loss:2.2742... 0.4239 sec/batch\n",
      "Epoch:8/20... Training Step:1505... Training loss:2.2926... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1506... Training loss:2.2837... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1507... Training loss:2.2924... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1508... Training loss:2.2696... 0.4708 sec/batch\n",
      "Epoch:8/20... Training Step:1509... Training loss:2.2630... 0.4538 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8/20... Training Step:1510... Training loss:2.2989... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1511... Training loss:2.2662... 0.4099 sec/batch\n",
      "Epoch:8/20... Training Step:1512... Training loss:2.2378... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1513... Training loss:2.2839... 0.4059 sec/batch\n",
      "Epoch:8/20... Training Step:1514... Training loss:2.2843... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1515... Training loss:2.2793... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1516... Training loss:2.2784... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1517... Training loss:2.2634... 0.4219 sec/batch\n",
      "Epoch:8/20... Training Step:1518... Training loss:2.2502... 0.4219 sec/batch\n",
      "Epoch:8/20... Training Step:1519... Training loss:2.2803... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1520... Training loss:2.2867... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1521... Training loss:2.2735... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1522... Training loss:2.2794... 0.4109 sec/batch\n",
      "Epoch:8/20... Training Step:1523... Training loss:2.2657... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1524... Training loss:2.2733... 0.4278 sec/batch\n",
      "Epoch:8/20... Training Step:1525... Training loss:2.3096... 0.4279 sec/batch\n",
      "Epoch:8/20... Training Step:1526... Training loss:2.2543... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1527... Training loss:2.2844... 0.4228 sec/batch\n",
      "Epoch:8/20... Training Step:1528... Training loss:2.2482... 0.4249 sec/batch\n",
      "Epoch:8/20... Training Step:1529... Training loss:2.2710... 0.4388 sec/batch\n",
      "Epoch:8/20... Training Step:1530... Training loss:2.2743... 0.4328 sec/batch\n",
      "Epoch:8/20... Training Step:1531... Training loss:2.2691... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1532... Training loss:2.3019... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1533... Training loss:2.2753... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1534... Training loss:2.3080... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1535... Training loss:2.2785... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1536... Training loss:2.2702... 0.4308 sec/batch\n",
      "Epoch:8/20... Training Step:1537... Training loss:2.2937... 0.4249 sec/batch\n",
      "Epoch:8/20... Training Step:1538... Training loss:2.3185... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1539... Training loss:2.2862... 0.4269 sec/batch\n",
      "Epoch:8/20... Training Step:1540... Training loss:2.2803... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1541... Training loss:2.2579... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1542... Training loss:2.2683... 0.4079 sec/batch\n",
      "Epoch:8/20... Training Step:1543... Training loss:2.2650... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1544... Training loss:2.2774... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1545... Training loss:2.2326... 0.4199 sec/batch\n",
      "Epoch:8/20... Training Step:1546... Training loss:2.3168... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1547... Training loss:2.2881... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1548... Training loss:2.2515... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1549... Training loss:2.2658... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1550... Training loss:2.2851... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1551... Training loss:2.2805... 0.4239 sec/batch\n",
      "Epoch:8/20... Training Step:1552... Training loss:2.2676... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1553... Training loss:2.2703... 0.4368 sec/batch\n",
      "Epoch:8/20... Training Step:1554... Training loss:2.2857... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1555... Training loss:2.2732... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1556... Training loss:2.2484... 0.4279 sec/batch\n",
      "Epoch:8/20... Training Step:1557... Training loss:2.2608... 0.4149 sec/batch\n",
      "Epoch:8/20... Training Step:1558... Training loss:2.2611... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1559... Training loss:2.2874... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1560... Training loss:2.2823... 0.4089 sec/batch\n",
      "Epoch:8/20... Training Step:1561... Training loss:2.2755... 0.4288 sec/batch\n",
      "Epoch:8/20... Training Step:1562... Training loss:2.2750... 0.4129 sec/batch\n",
      "Epoch:8/20... Training Step:1563... Training loss:2.2690... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1564... Training loss:2.2651... 0.4259 sec/batch\n",
      "Epoch:8/20... Training Step:1565... Training loss:2.2451... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1566... Training loss:2.2425... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1567... Training loss:2.2429... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1568... Training loss:2.2738... 0.4239 sec/batch\n",
      "Epoch:8/20... Training Step:1569... Training loss:2.2539... 0.4229 sec/batch\n",
      "Epoch:8/20... Training Step:1570... Training loss:2.2905... 0.4179 sec/batch\n",
      "Epoch:8/20... Training Step:1571... Training loss:2.2844... 0.4269 sec/batch\n",
      "Epoch:8/20... Training Step:1572... Training loss:2.2642... 0.4069 sec/batch\n",
      "Epoch:8/20... Training Step:1573... Training loss:2.2537... 0.4189 sec/batch\n",
      "Epoch:8/20... Training Step:1574... Training loss:2.2465... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1575... Training loss:2.2476... 0.4209 sec/batch\n",
      "Epoch:8/20... Training Step:1576... Training loss:2.2531... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1577... Training loss:2.2705... 0.4159 sec/batch\n",
      "Epoch:8/20... Training Step:1578... Training loss:2.2333... 0.4119 sec/batch\n",
      "Epoch:8/20... Training Step:1579... Training loss:2.2602... 0.4258 sec/batch\n",
      "Epoch:8/20... Training Step:1580... Training loss:2.2546... 0.4169 sec/batch\n",
      "Epoch:8/20... Training Step:1581... Training loss:2.2449... 0.4139 sec/batch\n",
      "Epoch:8/20... Training Step:1582... Training loss:2.2437... 0.4229 sec/batch\n",
      "Epoch:8/20... Training Step:1583... Training loss:2.2472... 0.4249 sec/batch\n",
      "Epoch:8/20... Training Step:1584... Training loss:2.2328... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1585... Training loss:2.4639... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1586... Training loss:2.2484... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1587... Training loss:2.2377... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1588... Training loss:2.2552... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1589... Training loss:2.2540... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1590... Training loss:2.2480... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1591... Training loss:2.2622... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1592... Training loss:2.2676... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1593... Training loss:2.2785... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1594... Training loss:2.2597... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1595... Training loss:2.2413... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1596... Training loss:2.2477... 0.4348 sec/batch\n",
      "Epoch:9/20... Training Step:1597... Training loss:2.2607... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1598... Training loss:2.2966... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1599... Training loss:2.2512... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1600... Training loss:2.2580... 0.4259 sec/batch\n",
      "Epoch:9/20... Training Step:1601... Training loss:2.2625... 0.4129 sec/batch\n",
      "Epoch:9/20... Training Step:1602... Training loss:2.2975... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1603... Training loss:2.2771... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1604... Training loss:2.2576... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1605... Training loss:2.2528... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1606... Training loss:2.2917... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1607... Training loss:2.2711... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1608... Training loss:2.2616... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1609... Training loss:2.2490... 0.4339 sec/batch\n",
      "Epoch:9/20... Training Step:1610... Training loss:2.2517... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1611... Training loss:2.2500... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1612... Training loss:2.2693... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1613... Training loss:2.2774... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1614... Training loss:2.2692... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1615... Training loss:2.2744... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1616... Training loss:2.2458... 0.4279 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/20... Training Step:1617... Training loss:2.2458... 0.4269 sec/batch\n",
      "Epoch:9/20... Training Step:1618... Training loss:2.2803... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1619... Training loss:2.2470... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1620... Training loss:2.2519... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1621... Training loss:2.2537... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1622... Training loss:2.2086... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1623... Training loss:2.2199... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1624... Training loss:2.2335... 0.4279 sec/batch\n",
      "Epoch:9/20... Training Step:1625... Training loss:2.2401... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1626... Training loss:2.2404... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1627... Training loss:2.2290... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1628... Training loss:2.2250... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1629... Training loss:2.2440... 0.4408 sec/batch\n",
      "Epoch:9/20... Training Step:1630... Training loss:2.1879... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1631... Training loss:2.2485... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1632... Training loss:2.2313... 0.4109 sec/batch\n",
      "Epoch:9/20... Training Step:1633... Training loss:2.2411... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1634... Training loss:2.2780... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1635... Training loss:2.2144... 0.4269 sec/batch\n",
      "Epoch:9/20... Training Step:1636... Training loss:2.2699... 0.4289 sec/batch\n",
      "Epoch:9/20... Training Step:1637... Training loss:2.2316... 0.4279 sec/batch\n",
      "Epoch:9/20... Training Step:1638... Training loss:2.2310... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1639... Training loss:2.2366... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1640... Training loss:2.2489... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1641... Training loss:2.2416... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1642... Training loss:2.2311... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1643... Training loss:2.2327... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1644... Training loss:2.2661... 0.4079 sec/batch\n",
      "Epoch:9/20... Training Step:1645... Training loss:2.2327... 0.4338 sec/batch\n",
      "Epoch:9/20... Training Step:1646... Training loss:2.2547... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1647... Training loss:2.2664... 0.4308 sec/batch\n",
      "Epoch:9/20... Training Step:1648... Training loss:2.2528... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1649... Training loss:2.2312... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1650... Training loss:2.2574... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1651... Training loss:2.2480... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1652... Training loss:2.2086... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1653... Training loss:2.2200... 0.4049 sec/batch\n",
      "Epoch:9/20... Training Step:1654... Training loss:2.2444... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1655... Training loss:2.2603... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1656... Training loss:2.2468... 0.4079 sec/batch\n",
      "Epoch:9/20... Training Step:1657... Training loss:2.2515... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1658... Training loss:2.2249... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1659... Training loss:2.2387... 0.4518 sec/batch\n",
      "Epoch:9/20... Training Step:1660... Training loss:2.2760... 0.4398 sec/batch\n",
      "Epoch:9/20... Training Step:1661... Training loss:2.2395... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1662... Training loss:2.2489... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1663... Training loss:2.2130... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1664... Training loss:2.2301... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1665... Training loss:2.2254... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1666... Training loss:2.2564... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1667... Training loss:2.2180... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1668... Training loss:2.2160... 0.4289 sec/batch\n",
      "Epoch:9/20... Training Step:1669... Training loss:2.2037... 0.4109 sec/batch\n",
      "Epoch:9/20... Training Step:1670... Training loss:2.2376... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1671... Training loss:2.2461... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1672... Training loss:2.2325... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1673... Training loss:2.2218... 0.4298 sec/batch\n",
      "Epoch:9/20... Training Step:1674... Training loss:2.2475... 0.4079 sec/batch\n",
      "Epoch:9/20... Training Step:1675... Training loss:2.2230... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1676... Training loss:2.2441... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1677... Training loss:2.2150... 0.4099 sec/batch\n",
      "Epoch:9/20... Training Step:1678... Training loss:2.2106... 0.4269 sec/batch\n",
      "Epoch:9/20... Training Step:1679... Training loss:2.2196... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1680... Training loss:2.2221... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1681... Training loss:2.2323... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1682... Training loss:2.2226... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1683... Training loss:2.2197... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1684... Training loss:2.2046... 0.4328 sec/batch\n",
      "Epoch:9/20... Training Step:1685... Training loss:2.2494... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1686... Training loss:2.2432... 0.4308 sec/batch\n",
      "Epoch:9/20... Training Step:1687... Training loss:2.2083... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1688... Training loss:2.2304... 0.4268 sec/batch\n",
      "Epoch:9/20... Training Step:1689... Training loss:2.2317... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1690... Training loss:2.2282... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1691... Training loss:2.2313... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1692... Training loss:2.2563... 0.4119 sec/batch\n",
      "Epoch:9/20... Training Step:1693... Training loss:2.2511... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1694... Training loss:2.2223... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1695... Training loss:2.2357... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1696... Training loss:2.2469... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1697... Training loss:2.2220... 0.4099 sec/batch\n",
      "Epoch:9/20... Training Step:1698... Training loss:2.2229... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1699... Training loss:2.2283... 0.4259 sec/batch\n",
      "Epoch:9/20... Training Step:1700... Training loss:2.1939... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1701... Training loss:2.2366... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1702... Training loss:2.2323... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1703... Training loss:2.2394... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1704... Training loss:2.2404... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1705... Training loss:2.2549... 0.4129 sec/batch\n",
      "Epoch:9/20... Training Step:1706... Training loss:2.2197... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1707... Training loss:2.2092... 0.4159 sec/batch\n",
      "Epoch:9/20... Training Step:1708... Training loss:2.2515... 0.4488 sec/batch\n",
      "Epoch:9/20... Training Step:1709... Training loss:2.2296... 0.4458 sec/batch\n",
      "Epoch:9/20... Training Step:1710... Training loss:2.2023... 0.4398 sec/batch\n",
      "Epoch:9/20... Training Step:1711... Training loss:2.2388... 0.4099 sec/batch\n",
      "Epoch:9/20... Training Step:1712... Training loss:2.2419... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1713... Training loss:2.2302... 0.4059 sec/batch\n",
      "Epoch:9/20... Training Step:1714... Training loss:2.2353... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1715... Training loss:2.2127... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1716... Training loss:2.2058... 0.4328 sec/batch\n",
      "Epoch:9/20... Training Step:1717... Training loss:2.2387... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1718... Training loss:2.2340... 0.4089 sec/batch\n",
      "Epoch:9/20... Training Step:1719... Training loss:2.2217... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1720... Training loss:2.2347... 0.4199 sec/batch\n",
      "Epoch:9/20... Training Step:1721... Training loss:2.2277... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1722... Training loss:2.2290... 0.4129 sec/batch\n",
      "Epoch:9/20... Training Step:1723... Training loss:2.2686... 0.4219 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9/20... Training Step:1724... Training loss:2.2046... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1725... Training loss:2.2434... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1726... Training loss:2.2246... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1727... Training loss:2.2212... 0.4428 sec/batch\n",
      "Epoch:9/20... Training Step:1728... Training loss:2.2187... 0.4428 sec/batch\n",
      "Epoch:9/20... Training Step:1729... Training loss:2.2171... 0.4348 sec/batch\n",
      "Epoch:9/20... Training Step:1730... Training loss:2.2540... 0.4508 sec/batch\n",
      "Epoch:9/20... Training Step:1731... Training loss:2.2358... 0.4598 sec/batch\n",
      "Epoch:9/20... Training Step:1732... Training loss:2.2486... 0.4349 sec/batch\n",
      "Epoch:9/20... Training Step:1733... Training loss:2.2298... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1734... Training loss:2.2214... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1735... Training loss:2.2367... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1736... Training loss:2.2654... 0.4149 sec/batch\n",
      "Epoch:9/20... Training Step:1737... Training loss:2.2428... 0.4368 sec/batch\n",
      "Epoch:9/20... Training Step:1738... Training loss:2.2363... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1739... Training loss:2.2195... 0.4169 sec/batch\n",
      "Epoch:9/20... Training Step:1740... Training loss:2.2141... 0.4338 sec/batch\n",
      "Epoch:9/20... Training Step:1741... Training loss:2.2229... 0.4309 sec/batch\n",
      "Epoch:9/20... Training Step:1742... Training loss:2.2286... 0.4398 sec/batch\n",
      "Epoch:9/20... Training Step:1743... Training loss:2.1933... 0.4269 sec/batch\n",
      "Epoch:9/20... Training Step:1744... Training loss:2.2661... 0.4408 sec/batch\n",
      "Epoch:9/20... Training Step:1745... Training loss:2.2473... 0.4708 sec/batch\n",
      "Epoch:9/20... Training Step:1746... Training loss:2.2046... 0.4398 sec/batch\n",
      "Epoch:9/20... Training Step:1747... Training loss:2.2283... 0.4268 sec/batch\n",
      "Epoch:9/20... Training Step:1748... Training loss:2.2259... 0.4259 sec/batch\n",
      "Epoch:9/20... Training Step:1749... Training loss:2.2314... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1750... Training loss:2.2222... 0.4229 sec/batch\n",
      "Epoch:9/20... Training Step:1751... Training loss:2.2271... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1752... Training loss:2.2335... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1753... Training loss:2.2199... 0.4318 sec/batch\n",
      "Epoch:9/20... Training Step:1754... Training loss:2.2095... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1755... Training loss:2.2106... 0.4219 sec/batch\n",
      "Epoch:9/20... Training Step:1756... Training loss:2.2072... 0.4428 sec/batch\n",
      "Epoch:9/20... Training Step:1757... Training loss:2.2447... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1758... Training loss:2.2419... 0.4458 sec/batch\n",
      "Epoch:9/20... Training Step:1759... Training loss:2.2374... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1760... Training loss:2.2316... 0.4358 sec/batch\n",
      "Epoch:9/20... Training Step:1761... Training loss:2.2145... 0.4239 sec/batch\n",
      "Epoch:9/20... Training Step:1762... Training loss:2.2184... 0.4228 sec/batch\n",
      "Epoch:9/20... Training Step:1763... Training loss:2.2032... 0.4279 sec/batch\n",
      "Epoch:9/20... Training Step:1764... Training loss:2.1960... 0.4259 sec/batch\n",
      "Epoch:9/20... Training Step:1765... Training loss:2.2098... 0.4688 sec/batch\n",
      "Epoch:9/20... Training Step:1766... Training loss:2.2208... 0.4269 sec/batch\n",
      "Epoch:9/20... Training Step:1767... Training loss:2.2198... 0.4358 sec/batch\n",
      "Epoch:9/20... Training Step:1768... Training loss:2.2347... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1769... Training loss:2.2306... 0.4348 sec/batch\n",
      "Epoch:9/20... Training Step:1770... Training loss:2.2192... 0.4328 sec/batch\n",
      "Epoch:9/20... Training Step:1771... Training loss:2.2095... 0.4189 sec/batch\n",
      "Epoch:9/20... Training Step:1772... Training loss:2.2009... 0.4408 sec/batch\n",
      "Epoch:9/20... Training Step:1773... Training loss:2.1970... 0.4678 sec/batch\n",
      "Epoch:9/20... Training Step:1774... Training loss:2.2053... 0.4179 sec/batch\n",
      "Epoch:9/20... Training Step:1775... Training loss:2.2346... 0.4249 sec/batch\n",
      "Epoch:9/20... Training Step:1776... Training loss:2.1922... 0.4368 sec/batch\n",
      "Epoch:9/20... Training Step:1777... Training loss:2.2216... 0.4139 sec/batch\n",
      "Epoch:9/20... Training Step:1778... Training loss:2.2038... 0.4378 sec/batch\n",
      "Epoch:9/20... Training Step:1779... Training loss:2.2013... 0.4358 sec/batch\n",
      "Epoch:9/20... Training Step:1780... Training loss:2.2107... 0.4209 sec/batch\n",
      "Epoch:9/20... Training Step:1781... Training loss:2.2099... 0.4398 sec/batch\n",
      "Epoch:9/20... Training Step:1782... Training loss:2.1900... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1783... Training loss:2.4103... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1784... Training loss:2.1806... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1785... Training loss:2.1893... 0.4508 sec/batch\n",
      "Epoch:10/20... Training Step:1786... Training loss:2.2017... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1787... Training loss:2.2065... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1788... Training loss:2.1924... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1789... Training loss:2.2056... 0.4099 sec/batch\n",
      "Epoch:10/20... Training Step:1790... Training loss:2.2158... 0.4179 sec/batch\n",
      "Epoch:10/20... Training Step:1791... Training loss:2.2406... 0.4139 sec/batch\n",
      "Epoch:10/20... Training Step:1792... Training loss:2.2066... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1793... Training loss:2.2045... 0.4338 sec/batch\n",
      "Epoch:10/20... Training Step:1794... Training loss:2.1967... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1795... Training loss:2.2162... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1796... Training loss:2.2554... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1797... Training loss:2.2262... 0.4119 sec/batch\n",
      "Epoch:10/20... Training Step:1798... Training loss:2.2072... 0.4149 sec/batch\n",
      "Epoch:10/20... Training Step:1799... Training loss:2.2263... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1800... Training loss:2.2598... 0.4328 sec/batch\n",
      "Epoch:10/20... Training Step:1801... Training loss:2.2311... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1802... Training loss:2.2133... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1803... Training loss:2.2134... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1804... Training loss:2.2522... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1805... Training loss:2.2176... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1806... Training loss:2.2151... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1807... Training loss:2.2189... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1808... Training loss:2.2002... 0.4169 sec/batch\n",
      "Epoch:10/20... Training Step:1809... Training loss:2.2017... 0.4139 sec/batch\n",
      "Epoch:10/20... Training Step:1810... Training loss:2.2128... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1811... Training loss:2.2311... 0.4149 sec/batch\n",
      "Epoch:10/20... Training Step:1812... Training loss:2.2164... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1813... Training loss:2.2249... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1814... Training loss:2.1938... 0.4139 sec/batch\n",
      "Epoch:10/20... Training Step:1815... Training loss:2.2008... 0.4179 sec/batch\n",
      "Epoch:10/20... Training Step:1816... Training loss:2.2346... 0.4119 sec/batch\n",
      "Epoch:10/20... Training Step:1817... Training loss:2.1983... 0.4139 sec/batch\n",
      "Epoch:10/20... Training Step:1818... Training loss:2.2169... 0.4099 sec/batch\n",
      "Epoch:10/20... Training Step:1819... Training loss:2.2153... 0.4109 sec/batch\n",
      "Epoch:10/20... Training Step:1820... Training loss:2.1671... 0.4568 sec/batch\n",
      "Epoch:10/20... Training Step:1821... Training loss:2.1820... 0.4258 sec/batch\n",
      "Epoch:10/20... Training Step:1822... Training loss:2.1773... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1823... Training loss:2.1990... 0.4338 sec/batch\n",
      "Epoch:10/20... Training Step:1824... Training loss:2.1919... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1825... Training loss:2.1837... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1826... Training loss:2.1761... 0.4139 sec/batch\n",
      "Epoch:10/20... Training Step:1827... Training loss:2.2060... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1828... Training loss:2.1464... 0.4109 sec/batch\n",
      "Epoch:10/20... Training Step:1829... Training loss:2.2064... 0.4199 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/20... Training Step:1830... Training loss:2.1867... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1831... Training loss:2.1961... 0.4299 sec/batch\n",
      "Epoch:10/20... Training Step:1832... Training loss:2.2265... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1833... Training loss:2.1702... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1834... Training loss:2.2366... 0.4658 sec/batch\n",
      "Epoch:10/20... Training Step:1835... Training loss:2.1834... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1836... Training loss:2.1983... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1837... Training loss:2.1940... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1838... Training loss:2.2088... 0.4169 sec/batch\n",
      "Epoch:10/20... Training Step:1839... Training loss:2.2031... 0.4179 sec/batch\n",
      "Epoch:10/20... Training Step:1840... Training loss:2.1870... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1841... Training loss:2.1804... 0.4428 sec/batch\n",
      "Epoch:10/20... Training Step:1842... Training loss:2.2140... 0.4259 sec/batch\n",
      "Epoch:10/20... Training Step:1843... Training loss:2.1914... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1844... Training loss:2.2259... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1845... Training loss:2.2324... 0.4318 sec/batch\n",
      "Epoch:10/20... Training Step:1846... Training loss:2.2032... 0.4528 sec/batch\n",
      "Epoch:10/20... Training Step:1847... Training loss:2.1909... 0.4877 sec/batch\n",
      "Epoch:10/20... Training Step:1848... Training loss:2.2138... 0.4418 sec/batch\n",
      "Epoch:10/20... Training Step:1849... Training loss:2.2108... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1850... Training loss:2.1703... 0.4329 sec/batch\n",
      "Epoch:10/20... Training Step:1851... Training loss:2.1729... 0.4299 sec/batch\n",
      "Epoch:10/20... Training Step:1852... Training loss:2.1987... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1853... Training loss:2.2229... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1854... Training loss:2.2031... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1855... Training loss:2.2178... 0.4368 sec/batch\n",
      "Epoch:10/20... Training Step:1856... Training loss:2.1864... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1857... Training loss:2.1857... 0.4149 sec/batch\n",
      "Epoch:10/20... Training Step:1858... Training loss:2.2377... 0.4259 sec/batch\n",
      "Epoch:10/20... Training Step:1859... Training loss:2.1965... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1860... Training loss:2.2117... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1861... Training loss:2.1665... 0.4129 sec/batch\n",
      "Epoch:10/20... Training Step:1862... Training loss:2.1788... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1863... Training loss:2.1686... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1864... Training loss:2.2079... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1865... Training loss:2.1669... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1866... Training loss:2.1693... 0.4159 sec/batch\n",
      "Epoch:10/20... Training Step:1867... Training loss:2.1468... 0.4238 sec/batch\n",
      "Epoch:10/20... Training Step:1868... Training loss:2.1795... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1869... Training loss:2.1905... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1870... Training loss:2.1857... 0.4328 sec/batch\n",
      "Epoch:10/20... Training Step:1871... Training loss:2.1671... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1872... Training loss:2.1942... 0.4408 sec/batch\n",
      "Epoch:10/20... Training Step:1873... Training loss:2.1783... 0.4618 sec/batch\n",
      "Epoch:10/20... Training Step:1874... Training loss:2.1869... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1875... Training loss:2.1690... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1876... Training loss:2.1639... 0.4757 sec/batch\n",
      "Epoch:10/20... Training Step:1877... Training loss:2.1729... 0.4309 sec/batch\n",
      "Epoch:10/20... Training Step:1878... Training loss:2.1771... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1879... Training loss:2.1838... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1880... Training loss:2.1830... 0.4259 sec/batch\n",
      "Epoch:10/20... Training Step:1881... Training loss:2.1739... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1882... Training loss:2.1545... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1883... Training loss:2.1974... 0.4308 sec/batch\n",
      "Epoch:10/20... Training Step:1884... Training loss:2.1960... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1885... Training loss:2.1639... 0.4318 sec/batch\n",
      "Epoch:10/20... Training Step:1886... Training loss:2.1813... 0.4648 sec/batch\n",
      "Epoch:10/20... Training Step:1887... Training loss:2.1779... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1888... Training loss:2.1799... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1889... Training loss:2.1709... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1890... Training loss:2.1985... 0.4179 sec/batch\n",
      "Epoch:10/20... Training Step:1891... Training loss:2.1974... 0.4318 sec/batch\n",
      "Epoch:10/20... Training Step:1892... Training loss:2.1826... 0.4438 sec/batch\n",
      "Epoch:10/20... Training Step:1893... Training loss:2.1919... 0.4977 sec/batch\n",
      "Epoch:10/20... Training Step:1894... Training loss:2.1983... 0.4259 sec/batch\n",
      "Epoch:10/20... Training Step:1895... Training loss:2.1871... 0.4308 sec/batch\n",
      "Epoch:10/20... Training Step:1896... Training loss:2.1669... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1897... Training loss:2.1723... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1898... Training loss:2.1476... 0.4328 sec/batch\n",
      "Epoch:10/20... Training Step:1899... Training loss:2.1950... 0.4368 sec/batch\n",
      "Epoch:10/20... Training Step:1900... Training loss:2.1800... 0.4159 sec/batch\n",
      "Epoch:10/20... Training Step:1901... Training loss:2.2048... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1902... Training loss:2.1903... 0.4149 sec/batch\n",
      "Epoch:10/20... Training Step:1903... Training loss:2.2108... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1904... Training loss:2.1778... 0.4319 sec/batch\n",
      "Epoch:10/20... Training Step:1905... Training loss:2.1754... 0.4338 sec/batch\n",
      "Epoch:10/20... Training Step:1906... Training loss:2.2030... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1907... Training loss:2.1755... 0.4299 sec/batch\n",
      "Epoch:10/20... Training Step:1908... Training loss:2.1601... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1909... Training loss:2.1942... 0.4378 sec/batch\n",
      "Epoch:10/20... Training Step:1910... Training loss:2.1915... 0.4119 sec/batch\n",
      "Epoch:10/20... Training Step:1911... Training loss:2.1801... 0.4289 sec/batch\n",
      "Epoch:10/20... Training Step:1912... Training loss:2.1844... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1913... Training loss:2.1754... 0.4228 sec/batch\n",
      "Epoch:10/20... Training Step:1914... Training loss:2.1509... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1915... Training loss:2.1942... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1916... Training loss:2.2016... 0.4408 sec/batch\n",
      "Epoch:10/20... Training Step:1917... Training loss:2.1797... 0.4149 sec/batch\n",
      "Epoch:10/20... Training Step:1918... Training loss:2.1938... 0.4318 sec/batch\n",
      "Epoch:10/20... Training Step:1919... Training loss:2.1881... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1920... Training loss:2.1882... 0.4199 sec/batch\n",
      "Epoch:10/20... Training Step:1921... Training loss:2.2175... 0.4428 sec/batch\n",
      "Epoch:10/20... Training Step:1922... Training loss:2.1759... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1923... Training loss:2.2016... 0.4328 sec/batch\n",
      "Epoch:10/20... Training Step:1924... Training loss:2.1734... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1925... Training loss:2.1802... 0.4338 sec/batch\n",
      "Epoch:10/20... Training Step:1926... Training loss:2.1782... 0.4318 sec/batch\n",
      "Epoch:10/20... Training Step:1927... Training loss:2.1846... 0.4478 sec/batch\n",
      "Epoch:10/20... Training Step:1928... Training loss:2.2021... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1929... Training loss:2.2030... 0.4259 sec/batch\n",
      "Epoch:10/20... Training Step:1930... Training loss:2.2054... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1931... Training loss:2.1918... 0.4248 sec/batch\n",
      "Epoch:10/20... Training Step:1932... Training loss:2.1789... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1933... Training loss:2.1916... 0.4448 sec/batch\n",
      "Epoch:10/20... Training Step:1934... Training loss:2.2204... 0.4488 sec/batch\n",
      "Epoch:10/20... Training Step:1935... Training loss:2.1912... 0.4259 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/20... Training Step:1936... Training loss:2.1925... 0.4378 sec/batch\n",
      "Epoch:10/20... Training Step:1937... Training loss:2.1685... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1938... Training loss:2.1744... 0.4269 sec/batch\n",
      "Epoch:10/20... Training Step:1939... Training loss:2.1745... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1940... Training loss:2.1755... 0.4388 sec/batch\n",
      "Epoch:10/20... Training Step:1941... Training loss:2.1456... 0.4319 sec/batch\n",
      "Epoch:10/20... Training Step:1942... Training loss:2.2221... 0.4179 sec/batch\n",
      "Epoch:10/20... Training Step:1943... Training loss:2.1963... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1944... Training loss:2.1710... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1945... Training loss:2.1886... 0.4289 sec/batch\n",
      "Epoch:10/20... Training Step:1946... Training loss:2.1857... 0.4229 sec/batch\n",
      "Epoch:10/20... Training Step:1947... Training loss:2.1889... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1948... Training loss:2.1871... 0.4358 sec/batch\n",
      "Epoch:10/20... Training Step:1949... Training loss:2.1930... 0.5146 sec/batch\n",
      "Epoch:10/20... Training Step:1950... Training loss:2.1995... 0.4678 sec/batch\n",
      "Epoch:10/20... Training Step:1951... Training loss:2.1850... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1952... Training loss:2.1679... 0.4308 sec/batch\n",
      "Epoch:10/20... Training Step:1953... Training loss:2.1739... 0.4159 sec/batch\n",
      "Epoch:10/20... Training Step:1954... Training loss:2.1813... 0.4228 sec/batch\n",
      "Epoch:10/20... Training Step:1955... Training loss:2.1978... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1956... Training loss:2.2020... 0.4298 sec/batch\n",
      "Epoch:10/20... Training Step:1957... Training loss:2.2041... 0.4189 sec/batch\n",
      "Epoch:10/20... Training Step:1958... Training loss:2.1832... 0.4209 sec/batch\n",
      "Epoch:10/20... Training Step:1959... Training loss:2.1757... 0.4418 sec/batch\n",
      "Epoch:10/20... Training Step:1960... Training loss:2.1846... 0.4219 sec/batch\n",
      "Epoch:10/20... Training Step:1961... Training loss:2.1584... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1962... Training loss:2.1453... 0.4279 sec/batch\n",
      "Epoch:10/20... Training Step:1963... Training loss:2.1671... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1964... Training loss:2.1861... 0.4368 sec/batch\n",
      "Epoch:10/20... Training Step:1965... Training loss:2.1806... 0.4747 sec/batch\n",
      "Epoch:10/20... Training Step:1966... Training loss:2.2018... 0.4388 sec/batch\n",
      "Epoch:10/20... Training Step:1967... Training loss:2.1901... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1968... Training loss:2.1705... 0.4159 sec/batch\n",
      "Epoch:10/20... Training Step:1969... Training loss:2.1667... 0.4398 sec/batch\n",
      "Epoch:10/20... Training Step:1970... Training loss:2.1558... 0.4518 sec/batch\n",
      "Epoch:10/20... Training Step:1971... Training loss:2.1561... 0.4358 sec/batch\n",
      "Epoch:10/20... Training Step:1972... Training loss:2.1670... 0.4239 sec/batch\n",
      "Epoch:10/20... Training Step:1973... Training loss:2.1878... 0.4538 sec/batch\n",
      "Epoch:10/20... Training Step:1974... Training loss:2.1529... 0.4368 sec/batch\n",
      "Epoch:10/20... Training Step:1975... Training loss:2.1789... 0.4328 sec/batch\n",
      "Epoch:10/20... Training Step:1976... Training loss:2.1681... 0.4308 sec/batch\n",
      "Epoch:10/20... Training Step:1977... Training loss:2.1554... 0.4348 sec/batch\n",
      "Epoch:10/20... Training Step:1978... Training loss:2.1697... 0.4249 sec/batch\n",
      "Epoch:10/20... Training Step:1979... Training loss:2.1695... 0.4338 sec/batch\n",
      "Epoch:10/20... Training Step:1980... Training loss:2.1531... 0.4927 sec/batch\n",
      "Epoch:11/20... Training Step:1981... Training loss:2.3569... 0.4368 sec/batch\n",
      "Epoch:11/20... Training Step:1982... Training loss:2.1471... 0.4478 sec/batch\n",
      "Epoch:11/20... Training Step:1983... Training loss:2.1429... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:1984... Training loss:2.1544... 0.4259 sec/batch\n",
      "Epoch:11/20... Training Step:1985... Training loss:2.1656... 0.4228 sec/batch\n",
      "Epoch:11/20... Training Step:1986... Training loss:2.1449... 0.4528 sec/batch\n",
      "Epoch:11/20... Training Step:1987... Training loss:2.1798... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:1988... Training loss:2.1728... 0.4239 sec/batch\n",
      "Epoch:11/20... Training Step:1989... Training loss:2.1954... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:1990... Training loss:2.1632... 0.4369 sec/batch\n",
      "Epoch:11/20... Training Step:1991... Training loss:2.1510... 0.4308 sec/batch\n",
      "Epoch:11/20... Training Step:1992... Training loss:2.1503... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:1993... Training loss:2.1649... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:1994... Training loss:2.2088... 0.5485 sec/batch\n",
      "Epoch:11/20... Training Step:1995... Training loss:2.1686... 0.4359 sec/batch\n",
      "Epoch:11/20... Training Step:1996... Training loss:2.1601... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:1997... Training loss:2.1636... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:1998... Training loss:2.2096... 0.4259 sec/batch\n",
      "Epoch:11/20... Training Step:1999... Training loss:2.1711... 0.4269 sec/batch\n",
      "Epoch:11/20... Training Step:2000... Training loss:2.1600... 0.4309 sec/batch\n",
      "Epoch:11/20... Training Step:2001... Training loss:2.1733... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2002... Training loss:2.2041... 0.4259 sec/batch\n",
      "Epoch:11/20... Training Step:2003... Training loss:2.1711... 0.4348 sec/batch\n",
      "Epoch:11/20... Training Step:2004... Training loss:2.1631... 0.4518 sec/batch\n",
      "Epoch:11/20... Training Step:2005... Training loss:2.1609... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2006... Training loss:2.1522... 0.4349 sec/batch\n",
      "Epoch:11/20... Training Step:2007... Training loss:2.1465... 0.4289 sec/batch\n",
      "Epoch:11/20... Training Step:2008... Training loss:2.1723... 0.4289 sec/batch\n",
      "Epoch:11/20... Training Step:2009... Training loss:2.1824... 0.4498 sec/batch\n",
      "Epoch:11/20... Training Step:2010... Training loss:2.1727... 0.4378 sec/batch\n",
      "Epoch:11/20... Training Step:2011... Training loss:2.1734... 0.4229 sec/batch\n",
      "Epoch:11/20... Training Step:2012... Training loss:2.1529... 0.4349 sec/batch\n",
      "Epoch:11/20... Training Step:2013... Training loss:2.1571... 0.4239 sec/batch\n",
      "Epoch:11/20... Training Step:2014... Training loss:2.1944... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2015... Training loss:2.1544... 0.4229 sec/batch\n",
      "Epoch:11/20... Training Step:2016... Training loss:2.1600... 0.4259 sec/batch\n",
      "Epoch:11/20... Training Step:2017... Training loss:2.1647... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2018... Training loss:2.1190... 0.4348 sec/batch\n",
      "Epoch:11/20... Training Step:2019... Training loss:2.1314... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2020... Training loss:2.1343... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2021... Training loss:2.1558... 0.4299 sec/batch\n",
      "Epoch:11/20... Training Step:2022... Training loss:2.1565... 0.4259 sec/batch\n",
      "Epoch:11/20... Training Step:2023... Training loss:2.1395... 0.4398 sec/batch\n",
      "Epoch:11/20... Training Step:2024... Training loss:2.1412... 0.4239 sec/batch\n",
      "Epoch:11/20... Training Step:2025... Training loss:2.1598... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2026... Training loss:2.1108... 0.4249 sec/batch\n",
      "Epoch:11/20... Training Step:2027... Training loss:2.1725... 0.4299 sec/batch\n",
      "Epoch:11/20... Training Step:2028... Training loss:2.1421... 0.4368 sec/batch\n",
      "Epoch:11/20... Training Step:2029... Training loss:2.1545... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:2030... Training loss:2.1862... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2031... Training loss:2.1279... 0.6054 sec/batch\n",
      "Epoch:11/20... Training Step:2032... Training loss:2.1978... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:2033... Training loss:2.1542... 0.4438 sec/batch\n",
      "Epoch:11/20... Training Step:2034... Training loss:2.1554... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2035... Training loss:2.1436... 0.4279 sec/batch\n",
      "Epoch:11/20... Training Step:2036... Training loss:2.1725... 0.4468 sec/batch\n",
      "Epoch:11/20... Training Step:2037... Training loss:2.1608... 0.4368 sec/batch\n",
      "Epoch:11/20... Training Step:2038... Training loss:2.1496... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2039... Training loss:2.1423... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2040... Training loss:2.1766... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2041... Training loss:2.1492... 0.4309 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/20... Training Step:2042... Training loss:2.1742... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:2043... Training loss:2.1874... 0.4438 sec/batch\n",
      "Epoch:11/20... Training Step:2044... Training loss:2.1683... 0.4308 sec/batch\n",
      "Epoch:11/20... Training Step:2045... Training loss:2.1461... 0.4239 sec/batch\n",
      "Epoch:11/20... Training Step:2046... Training loss:2.1740... 0.4558 sec/batch\n",
      "Epoch:11/20... Training Step:2047... Training loss:2.1665... 0.4199 sec/batch\n",
      "Epoch:11/20... Training Step:2048... Training loss:2.1330... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2049... Training loss:2.1338... 0.4398 sec/batch\n",
      "Epoch:11/20... Training Step:2050... Training loss:2.1615... 0.4438 sec/batch\n",
      "Epoch:11/20... Training Step:2051... Training loss:2.1890... 0.4478 sec/batch\n",
      "Epoch:11/20... Training Step:2052... Training loss:2.1670... 0.4249 sec/batch\n",
      "Epoch:11/20... Training Step:2053... Training loss:2.1746... 0.4368 sec/batch\n",
      "Epoch:11/20... Training Step:2054... Training loss:2.1367... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2055... Training loss:2.1487... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2056... Training loss:2.1902... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2057... Training loss:2.1505... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2058... Training loss:2.1645... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2059... Training loss:2.1223... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:2060... Training loss:2.1424... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2061... Training loss:2.1199... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2062... Training loss:2.1686... 0.4398 sec/batch\n",
      "Epoch:11/20... Training Step:2063... Training loss:2.1307... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:2064... Training loss:2.1303... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2065... Training loss:2.1123... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2066... Training loss:2.1312... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2067... Training loss:2.1496... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2068... Training loss:2.1343... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2069... Training loss:2.1237... 0.4478 sec/batch\n",
      "Epoch:11/20... Training Step:2070... Training loss:2.1627... 0.4279 sec/batch\n",
      "Epoch:11/20... Training Step:2071... Training loss:2.1343... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2072... Training loss:2.1488... 0.4279 sec/batch\n",
      "Epoch:11/20... Training Step:2073... Training loss:2.1204... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2074... Training loss:2.1191... 0.4508 sec/batch\n",
      "Epoch:11/20... Training Step:2075... Training loss:2.1307... 0.4359 sec/batch\n",
      "Epoch:11/20... Training Step:2076... Training loss:2.1420... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2077... Training loss:2.1386... 0.4588 sec/batch\n",
      "Epoch:11/20... Training Step:2078... Training loss:2.1331... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2079... Training loss:2.1274... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:2080... Training loss:2.1099... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2081... Training loss:2.1601... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2082... Training loss:2.1515... 0.5156 sec/batch\n",
      "Epoch:11/20... Training Step:2083... Training loss:2.1291... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2084... Training loss:2.1360... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:2085... Training loss:2.1301... 0.5136 sec/batch\n",
      "Epoch:11/20... Training Step:2086... Training loss:2.1425... 0.4957 sec/batch\n",
      "Epoch:11/20... Training Step:2087... Training loss:2.1424... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2088... Training loss:2.1503... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2089... Training loss:2.1594... 0.4578 sec/batch\n",
      "Epoch:11/20... Training Step:2090... Training loss:2.1380... 0.4378 sec/batch\n",
      "Epoch:11/20... Training Step:2091... Training loss:2.1470... 0.4378 sec/batch\n",
      "Epoch:11/20... Training Step:2092... Training loss:2.1546... 0.5216 sec/batch\n",
      "Epoch:11/20... Training Step:2093... Training loss:2.1436... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2094... Training loss:2.1353... 0.4398 sec/batch\n",
      "Epoch:11/20... Training Step:2095... Training loss:2.1311... 0.4428 sec/batch\n",
      "Epoch:11/20... Training Step:2096... Training loss:2.1057... 0.4338 sec/batch\n",
      "Epoch:11/20... Training Step:2097... Training loss:2.1481... 0.4428 sec/batch\n",
      "Epoch:11/20... Training Step:2098... Training loss:2.1317... 0.4558 sec/batch\n",
      "Epoch:11/20... Training Step:2099... Training loss:2.1569... 0.4578 sec/batch\n",
      "Epoch:11/20... Training Step:2100... Training loss:2.1512... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2101... Training loss:2.1676... 0.4787 sec/batch\n",
      "Epoch:11/20... Training Step:2102... Training loss:2.1287... 0.4727 sec/batch\n",
      "Epoch:11/20... Training Step:2103... Training loss:2.1277... 0.4299 sec/batch\n",
      "Epoch:11/20... Training Step:2104... Training loss:2.1628... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2105... Training loss:2.1333... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2106... Training loss:2.1033... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2107... Training loss:2.1575... 0.4588 sec/batch\n",
      "Epoch:11/20... Training Step:2108... Training loss:2.1495... 0.4528 sec/batch\n",
      "Epoch:11/20... Training Step:2109... Training loss:2.1460... 0.4568 sec/batch\n",
      "Epoch:11/20... Training Step:2110... Training loss:2.1491... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2111... Training loss:2.1322... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2112... Training loss:2.1224... 0.4279 sec/batch\n",
      "Epoch:11/20... Training Step:2113... Training loss:2.1549... 0.4468 sec/batch\n",
      "Epoch:11/20... Training Step:2114... Training loss:2.1462... 0.4298 sec/batch\n",
      "Epoch:11/20... Training Step:2115... Training loss:2.1411... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2116... Training loss:2.1541... 0.4408 sec/batch\n",
      "Epoch:11/20... Training Step:2117... Training loss:2.1443... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:2118... Training loss:2.1494... 0.4558 sec/batch\n",
      "Epoch:11/20... Training Step:2119... Training loss:2.1817... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2120... Training loss:2.1320... 0.4598 sec/batch\n",
      "Epoch:11/20... Training Step:2121... Training loss:2.1548... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2122... Training loss:2.1363... 0.4508 sec/batch\n",
      "Epoch:11/20... Training Step:2123... Training loss:2.1399... 0.4508 sec/batch\n",
      "Epoch:11/20... Training Step:2124... Training loss:2.1345... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2125... Training loss:2.1291... 0.4468 sec/batch\n",
      "Epoch:11/20... Training Step:2126... Training loss:2.1636... 0.4568 sec/batch\n",
      "Epoch:11/20... Training Step:2127... Training loss:2.1590... 0.4618 sec/batch\n",
      "Epoch:11/20... Training Step:2128... Training loss:2.1640... 0.4488 sec/batch\n",
      "Epoch:11/20... Training Step:2129... Training loss:2.1288... 0.5216 sec/batch\n",
      "Epoch:11/20... Training Step:2130... Training loss:2.1336... 0.4807 sec/batch\n",
      "Epoch:11/20... Training Step:2131... Training loss:2.1472... 0.4817 sec/batch\n",
      "Epoch:11/20... Training Step:2132... Training loss:2.1799... 0.4817 sec/batch\n",
      "Epoch:11/20... Training Step:2133... Training loss:2.1506... 0.4847 sec/batch\n",
      "Epoch:11/20... Training Step:2134... Training loss:2.1485... 0.4648 sec/batch\n",
      "Epoch:11/20... Training Step:2135... Training loss:2.1410... 0.4697 sec/batch\n",
      "Epoch:11/20... Training Step:2136... Training loss:2.1367... 0.4588 sec/batch\n",
      "Epoch:11/20... Training Step:2137... Training loss:2.1392... 0.4827 sec/batch\n",
      "Epoch:11/20... Training Step:2138... Training loss:2.1398... 0.4298 sec/batch\n",
      "Epoch:11/20... Training Step:2139... Training loss:2.1148... 0.4598 sec/batch\n",
      "Epoch:11/20... Training Step:2140... Training loss:2.1765... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2141... Training loss:2.1596... 0.4588 sec/batch\n",
      "Epoch:11/20... Training Step:2142... Training loss:2.1320... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2143... Training loss:2.1520... 0.4428 sec/batch\n",
      "Epoch:11/20... Training Step:2144... Training loss:2.1486... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2145... Training loss:2.1480... 0.4378 sec/batch\n",
      "Epoch:11/20... Training Step:2146... Training loss:2.1460... 0.4548 sec/batch\n",
      "Epoch:11/20... Training Step:2147... Training loss:2.1590... 0.4408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11/20... Training Step:2148... Training loss:2.1660... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2149... Training loss:2.1516... 0.4318 sec/batch\n",
      "Epoch:11/20... Training Step:2150... Training loss:2.1234... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2151... Training loss:2.1294... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2152... Training loss:2.1439... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2153... Training loss:2.1624... 0.4348 sec/batch\n",
      "Epoch:11/20... Training Step:2154... Training loss:2.1533... 0.4329 sec/batch\n",
      "Epoch:11/20... Training Step:2155... Training loss:2.1633... 0.4478 sec/batch\n",
      "Epoch:11/20... Training Step:2156... Training loss:2.1529... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2157... Training loss:2.1368... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2158... Training loss:2.1498... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2159... Training loss:2.1213... 0.4348 sec/batch\n",
      "Epoch:11/20... Training Step:2160... Training loss:2.1125... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2161... Training loss:2.1314... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2162... Training loss:2.1427... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2163... Training loss:2.1412... 0.4398 sec/batch\n",
      "Epoch:11/20... Training Step:2164... Training loss:2.1622... 0.4418 sec/batch\n",
      "Epoch:11/20... Training Step:2165... Training loss:2.1494... 0.4328 sec/batch\n",
      "Epoch:11/20... Training Step:2166... Training loss:2.1268... 0.4568 sec/batch\n",
      "Epoch:11/20... Training Step:2167... Training loss:2.1308... 0.4308 sec/batch\n",
      "Epoch:11/20... Training Step:2168... Training loss:2.1180... 0.4458 sec/batch\n",
      "Epoch:11/20... Training Step:2169... Training loss:2.1217... 0.4388 sec/batch\n",
      "Epoch:11/20... Training Step:2170... Training loss:2.1247... 0.4358 sec/batch\n",
      "Epoch:11/20... Training Step:2171... Training loss:2.1558... 0.4448 sec/batch\n",
      "Epoch:11/20... Training Step:2172... Training loss:2.1134... 0.4308 sec/batch\n",
      "Epoch:11/20... Training Step:2173... Training loss:2.1406... 0.4608 sec/batch\n",
      "Epoch:11/20... Training Step:2174... Training loss:2.1274... 0.4378 sec/batch\n",
      "Epoch:11/20... Training Step:2175... Training loss:2.1181... 0.4368 sec/batch\n",
      "Epoch:11/20... Training Step:2176... Training loss:2.1410... 0.4468 sec/batch\n",
      "Epoch:11/20... Training Step:2177... Training loss:2.1330... 0.4348 sec/batch\n",
      "Epoch:11/20... Training Step:2178... Training loss:2.1229... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2179... Training loss:2.3146... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2180... Training loss:2.1054... 0.4518 sec/batch\n",
      "Epoch:12/20... Training Step:2181... Training loss:2.1127... 0.4368 sec/batch\n",
      "Epoch:12/20... Training Step:2182... Training loss:2.1160... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2183... Training loss:2.1179... 0.4468 sec/batch\n",
      "Epoch:12/20... Training Step:2184... Training loss:2.1078... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2185... Training loss:2.1323... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2186... Training loss:2.1246... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2187... Training loss:2.1561... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2188... Training loss:2.1215... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2189... Training loss:2.1083... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2190... Training loss:2.1137... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2191... Training loss:2.1294... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2192... Training loss:2.1553... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2193... Training loss:2.1206... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2194... Training loss:2.1107... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2195... Training loss:2.1278... 0.4319 sec/batch\n",
      "Epoch:12/20... Training Step:2196... Training loss:2.1688... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2197... Training loss:2.1406... 0.4468 sec/batch\n",
      "Epoch:12/20... Training Step:2198... Training loss:2.1257... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2199... Training loss:2.1318... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2200... Training loss:2.1639... 0.4608 sec/batch\n",
      "Epoch:12/20... Training Step:2201... Training loss:2.1349... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2202... Training loss:2.1197... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2203... Training loss:2.1285... 0.4558 sec/batch\n",
      "Epoch:12/20... Training Step:2204... Training loss:2.1108... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2205... Training loss:2.1100... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2206... Training loss:2.1254... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2207... Training loss:2.1479... 0.4468 sec/batch\n",
      "Epoch:12/20... Training Step:2208... Training loss:2.1302... 0.4428 sec/batch\n",
      "Epoch:12/20... Training Step:2209... Training loss:2.1299... 0.4428 sec/batch\n",
      "Epoch:12/20... Training Step:2210... Training loss:2.1126... 0.4348 sec/batch\n",
      "Epoch:12/20... Training Step:2211... Training loss:2.1175... 0.4558 sec/batch\n",
      "Epoch:12/20... Training Step:2212... Training loss:2.1642... 0.4538 sec/batch\n",
      "Epoch:12/20... Training Step:2213... Training loss:2.1174... 0.4339 sec/batch\n",
      "Epoch:12/20... Training Step:2214... Training loss:2.1291... 0.4498 sec/batch\n",
      "Epoch:12/20... Training Step:2215... Training loss:2.1279... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2216... Training loss:2.0888... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2217... Training loss:2.0897... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2218... Training loss:2.0871... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2219... Training loss:2.1091... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2220... Training loss:2.1225... 0.4379 sec/batch\n",
      "Epoch:12/20... Training Step:2221... Training loss:2.1025... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2222... Training loss:2.0944... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2223... Training loss:2.1313... 0.4588 sec/batch\n",
      "Epoch:12/20... Training Step:2224... Training loss:2.0692... 0.4548 sec/batch\n",
      "Epoch:12/20... Training Step:2225... Training loss:2.1377... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2226... Training loss:2.1179... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2227... Training loss:2.1162... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2228... Training loss:2.1619... 0.4568 sec/batch\n",
      "Epoch:12/20... Training Step:2229... Training loss:2.0887... 0.4288 sec/batch\n",
      "Epoch:12/20... Training Step:2230... Training loss:2.1639... 0.4329 sec/batch\n",
      "Epoch:12/20... Training Step:2231... Training loss:2.1109... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2232... Training loss:2.1143... 0.4538 sec/batch\n",
      "Epoch:12/20... Training Step:2233... Training loss:2.1071... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2234... Training loss:2.1286... 0.4348 sec/batch\n",
      "Epoch:12/20... Training Step:2235... Training loss:2.1312... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2236... Training loss:2.1225... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2237... Training loss:2.1015... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2238... Training loss:2.1507... 0.4508 sec/batch\n",
      "Epoch:12/20... Training Step:2239... Training loss:2.1235... 0.4348 sec/batch\n",
      "Epoch:12/20... Training Step:2240... Training loss:2.1406... 0.4289 sec/batch\n",
      "Epoch:12/20... Training Step:2241... Training loss:2.1564... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2242... Training loss:2.1397... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2243... Training loss:2.1130... 0.4538 sec/batch\n",
      "Epoch:12/20... Training Step:2244... Training loss:2.1378... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2245... Training loss:2.1308... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2246... Training loss:2.0797... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2247... Training loss:2.0986... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2248... Training loss:2.1337... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2249... Training loss:2.1530... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2250... Training loss:2.1227... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2251... Training loss:2.1457... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2252... Training loss:2.1043... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2253... Training loss:2.1194... 0.4388 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/20... Training Step:2254... Training loss:2.1582... 0.4348 sec/batch\n",
      "Epoch:12/20... Training Step:2255... Training loss:2.1147... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2256... Training loss:2.1292... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2257... Training loss:2.0935... 0.4318 sec/batch\n",
      "Epoch:12/20... Training Step:2258... Training loss:2.1055... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2259... Training loss:2.0867... 0.4518 sec/batch\n",
      "Epoch:12/20... Training Step:2260... Training loss:2.1345... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2261... Training loss:2.0910... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2262... Training loss:2.1084... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2263... Training loss:2.0757... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2264... Training loss:2.1030... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2265... Training loss:2.1155... 0.4568 sec/batch\n",
      "Epoch:12/20... Training Step:2266... Training loss:2.1044... 0.4548 sec/batch\n",
      "Epoch:12/20... Training Step:2267... Training loss:2.0845... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2268... Training loss:2.1190... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2269... Training loss:2.0910... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2270... Training loss:2.1131... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2271... Training loss:2.0899... 0.4538 sec/batch\n",
      "Epoch:12/20... Training Step:2272... Training loss:2.0786... 0.4318 sec/batch\n",
      "Epoch:12/20... Training Step:2273... Training loss:2.0921... 0.4548 sec/batch\n",
      "Epoch:12/20... Training Step:2274... Training loss:2.1001... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2275... Training loss:2.0940... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2276... Training loss:2.1010... 0.4289 sec/batch\n",
      "Epoch:12/20... Training Step:2277... Training loss:2.0865... 0.4428 sec/batch\n",
      "Epoch:12/20... Training Step:2278... Training loss:2.0655... 0.4368 sec/batch\n",
      "Epoch:12/20... Training Step:2279... Training loss:2.1166... 0.4518 sec/batch\n",
      "Epoch:12/20... Training Step:2280... Training loss:2.1178... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2281... Training loss:2.0901... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2282... Training loss:2.1032... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2283... Training loss:2.0871... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2284... Training loss:2.0994... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2285... Training loss:2.1000... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2286... Training loss:2.1153... 0.4598 sec/batch\n",
      "Epoch:12/20... Training Step:2287... Training loss:2.1227... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2288... Training loss:2.0974... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2289... Training loss:2.1125... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2290... Training loss:2.0955... 0.4308 sec/batch\n",
      "Epoch:12/20... Training Step:2291... Training loss:2.1023... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2292... Training loss:2.1006... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2293... Training loss:2.1033... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2294... Training loss:2.0653... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2295... Training loss:2.1094... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2296... Training loss:2.0981... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2297... Training loss:2.1149... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2298... Training loss:2.1175... 0.4299 sec/batch\n",
      "Epoch:12/20... Training Step:2299... Training loss:2.1203... 0.4428 sec/batch\n",
      "Epoch:12/20... Training Step:2300... Training loss:2.0905... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2301... Training loss:2.0890... 0.4328 sec/batch\n",
      "Epoch:12/20... Training Step:2302... Training loss:2.1332... 0.4558 sec/batch\n",
      "Epoch:12/20... Training Step:2303... Training loss:2.0915... 0.4378 sec/batch\n",
      "Epoch:12/20... Training Step:2304... Training loss:2.0585... 0.4348 sec/batch\n",
      "Epoch:12/20... Training Step:2305... Training loss:2.1146... 0.4298 sec/batch\n",
      "Epoch:12/20... Training Step:2306... Training loss:2.1205... 0.4468 sec/batch\n",
      "Epoch:12/20... Training Step:2307... Training loss:2.1009... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2308... Training loss:2.1140... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2309... Training loss:2.0896... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2310... Training loss:2.0790... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2311... Training loss:2.1048... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2312... Training loss:2.1154... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2313... Training loss:2.1077... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2314... Training loss:2.1302... 0.4518 sec/batch\n",
      "Epoch:12/20... Training Step:2315... Training loss:2.1051... 0.4299 sec/batch\n",
      "Epoch:12/20... Training Step:2316... Training loss:2.1023... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2317... Training loss:2.1468... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2318... Training loss:2.0924... 0.4538 sec/batch\n",
      "Epoch:12/20... Training Step:2319... Training loss:2.1227... 0.4308 sec/batch\n",
      "Epoch:12/20... Training Step:2320... Training loss:2.1063... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2321... Training loss:2.1022... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2322... Training loss:2.1002... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2323... Training loss:2.0914... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2324... Training loss:2.1220... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2325... Training loss:2.1200... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2326... Training loss:2.1297... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2327... Training loss:2.1084... 0.4528 sec/batch\n",
      "Epoch:12/20... Training Step:2328... Training loss:2.0911... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2329... Training loss:2.1119... 0.4548 sec/batch\n",
      "Epoch:12/20... Training Step:2330... Training loss:2.1498... 0.4318 sec/batch\n",
      "Epoch:12/20... Training Step:2331... Training loss:2.1184... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2332... Training loss:2.1162... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2333... Training loss:2.0976... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2334... Training loss:2.1034... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2335... Training loss:2.1047... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2336... Training loss:2.1015... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2337... Training loss:2.0733... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2338... Training loss:2.1403... 0.4318 sec/batch\n",
      "Epoch:12/20... Training Step:2339... Training loss:2.1239... 0.4558 sec/batch\n",
      "Epoch:12/20... Training Step:2340... Training loss:2.1023... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2341... Training loss:2.1156... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2342... Training loss:2.1060... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2343... Training loss:2.1131... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2344... Training loss:2.0959... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2345... Training loss:2.1108... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2346... Training loss:2.1319... 0.4468 sec/batch\n",
      "Epoch:12/20... Training Step:2347... Training loss:2.1048... 0.4498 sec/batch\n",
      "Epoch:12/20... Training Step:2348... Training loss:2.0894... 0.4368 sec/batch\n",
      "Epoch:12/20... Training Step:2349... Training loss:2.0834... 0.4548 sec/batch\n",
      "Epoch:12/20... Training Step:2350... Training loss:2.1024... 0.4388 sec/batch\n",
      "Epoch:12/20... Training Step:2351... Training loss:2.1243... 0.4318 sec/batch\n",
      "Epoch:12/20... Training Step:2352... Training loss:2.1132... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2353... Training loss:2.1166... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2354... Training loss:2.1142... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2355... Training loss:2.1040... 0.4418 sec/batch\n",
      "Epoch:12/20... Training Step:2356... Training loss:2.1176... 0.4488 sec/batch\n",
      "Epoch:12/20... Training Step:2357... Training loss:2.0809... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2358... Training loss:2.0796... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2359... Training loss:2.0859... 0.4388 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12/20... Training Step:2360... Training loss:2.1061... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2361... Training loss:2.1010... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2362... Training loss:2.1290... 0.4398 sec/batch\n",
      "Epoch:12/20... Training Step:2363... Training loss:2.1063... 0.4408 sec/batch\n",
      "Epoch:12/20... Training Step:2364... Training loss:2.0956... 0.4358 sec/batch\n",
      "Epoch:12/20... Training Step:2365... Training loss:2.0916... 0.4478 sec/batch\n",
      "Epoch:12/20... Training Step:2366... Training loss:2.0721... 0.4338 sec/batch\n",
      "Epoch:12/20... Training Step:2367... Training loss:2.0832... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2368... Training loss:2.0883... 0.4309 sec/batch\n",
      "Epoch:12/20... Training Step:2369... Training loss:2.1106... 0.4438 sec/batch\n",
      "Epoch:12/20... Training Step:2370... Training loss:2.0731... 0.4428 sec/batch\n",
      "Epoch:12/20... Training Step:2371... Training loss:2.1009... 0.4458 sec/batch\n",
      "Epoch:12/20... Training Step:2372... Training loss:2.0890... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2373... Training loss:2.0681... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2374... Training loss:2.0963... 0.4568 sec/batch\n",
      "Epoch:12/20... Training Step:2375... Training loss:2.0948... 0.4448 sec/batch\n",
      "Epoch:12/20... Training Step:2376... Training loss:2.0743... 0.4538 sec/batch\n",
      "Epoch:13/20... Training Step:2377... Training loss:2.2712... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2378... Training loss:2.0704... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2379... Training loss:2.0719... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2380... Training loss:2.0746... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2381... Training loss:2.0842... 0.4648 sec/batch\n",
      "Epoch:13/20... Training Step:2382... Training loss:2.0619... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2383... Training loss:2.0982... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2384... Training loss:2.0932... 0.4289 sec/batch\n",
      "Epoch:13/20... Training Step:2385... Training loss:2.1208... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2386... Training loss:2.0879... 0.4378 sec/batch\n",
      "Epoch:13/20... Training Step:2387... Training loss:2.0772... 0.4368 sec/batch\n",
      "Epoch:13/20... Training Step:2388... Training loss:2.0728... 0.4358 sec/batch\n",
      "Epoch:13/20... Training Step:2389... Training loss:2.0968... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2390... Training loss:2.1321... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2391... Training loss:2.0940... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2392... Training loss:2.0719... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2393... Training loss:2.0841... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2394... Training loss:2.1386... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2395... Training loss:2.0987... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2396... Training loss:2.0880... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2397... Training loss:2.0897... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2398... Training loss:2.1242... 0.4538 sec/batch\n",
      "Epoch:13/20... Training Step:2399... Training loss:2.0882... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2400... Training loss:2.0872... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2401... Training loss:2.0945... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2402... Training loss:2.0754... 0.4518 sec/batch\n",
      "Epoch:13/20... Training Step:2403... Training loss:2.0693... 0.4687 sec/batch\n",
      "Epoch:13/20... Training Step:2404... Training loss:2.0959... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2405... Training loss:2.1257... 0.4379 sec/batch\n",
      "Epoch:13/20... Training Step:2406... Training loss:2.0981... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2407... Training loss:2.1043... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2408... Training loss:2.0733... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2409... Training loss:2.0824... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2410... Training loss:2.1290... 0.4488 sec/batch\n",
      "Epoch:13/20... Training Step:2411... Training loss:2.0839... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2412... Training loss:2.0937... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2413... Training loss:2.0898... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2414... Training loss:2.0485... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2415... Training loss:2.0559... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2416... Training loss:2.0609... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2417... Training loss:2.0691... 0.4318 sec/batch\n",
      "Epoch:13/20... Training Step:2418... Training loss:2.0850... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2419... Training loss:2.0629... 0.4538 sec/batch\n",
      "Epoch:13/20... Training Step:2420... Training loss:2.0558... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2421... Training loss:2.0948... 0.4518 sec/batch\n",
      "Epoch:13/20... Training Step:2422... Training loss:2.0263... 0.4338 sec/batch\n",
      "Epoch:13/20... Training Step:2423... Training loss:2.0890... 0.4338 sec/batch\n",
      "Epoch:13/20... Training Step:2424... Training loss:2.0718... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2425... Training loss:2.0817... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2426... Training loss:2.1198... 0.4558 sec/batch\n",
      "Epoch:13/20... Training Step:2427... Training loss:2.0510... 0.4299 sec/batch\n",
      "Epoch:13/20... Training Step:2428... Training loss:2.1238... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2429... Training loss:2.0868... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2430... Training loss:2.0715... 0.4299 sec/batch\n",
      "Epoch:13/20... Training Step:2431... Training loss:2.0758... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2432... Training loss:2.1005... 0.4318 sec/batch\n",
      "Epoch:13/20... Training Step:2433... Training loss:2.0898... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2434... Training loss:2.0877... 0.4289 sec/batch\n",
      "Epoch:13/20... Training Step:2435... Training loss:2.0735... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2436... Training loss:2.1193... 0.4358 sec/batch\n",
      "Epoch:13/20... Training Step:2437... Training loss:2.0773... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2438... Training loss:2.1198... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2439... Training loss:2.1105... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2440... Training loss:2.0949... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2441... Training loss:2.0812... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2442... Training loss:2.1024... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2443... Training loss:2.0990... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2444... Training loss:2.0586... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2445... Training loss:2.0562... 0.4299 sec/batch\n",
      "Epoch:13/20... Training Step:2446... Training loss:2.0866... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2447... Training loss:2.1227... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2448... Training loss:2.0910... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2449... Training loss:2.1042... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2450... Training loss:2.0645... 0.4259 sec/batch\n",
      "Epoch:13/20... Training Step:2451... Training loss:2.0797... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2452... Training loss:2.1142... 0.4518 sec/batch\n",
      "Epoch:13/20... Training Step:2453... Training loss:2.0807... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2454... Training loss:2.0876... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2455... Training loss:2.0502... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2456... Training loss:2.0690... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2457... Training loss:2.0475... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2458... Training loss:2.0887... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2459... Training loss:2.0480... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2460... Training loss:2.0617... 0.4568 sec/batch\n",
      "Epoch:13/20... Training Step:2461... Training loss:2.0393... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2462... Training loss:2.0671... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2463... Training loss:2.0655... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2464... Training loss:2.0623... 0.4318 sec/batch\n",
      "Epoch:13/20... Training Step:2465... Training loss:2.0512... 0.4448 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/20... Training Step:2466... Training loss:2.0732... 0.4378 sec/batch\n",
      "Epoch:13/20... Training Step:2467... Training loss:2.0606... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2468... Training loss:2.0774... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2469... Training loss:2.0393... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2470... Training loss:2.0531... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2471... Training loss:2.0559... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2472... Training loss:2.0537... 0.4348 sec/batch\n",
      "Epoch:13/20... Training Step:2473... Training loss:2.0676... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2474... Training loss:2.0598... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2475... Training loss:2.0476... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2476... Training loss:2.0306... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2477... Training loss:2.0794... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2478... Training loss:2.0742... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2479... Training loss:2.0548... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2480... Training loss:2.0611... 0.4488 sec/batch\n",
      "Epoch:13/20... Training Step:2481... Training loss:2.0492... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2482... Training loss:2.0645... 0.4648 sec/batch\n",
      "Epoch:13/20... Training Step:2483... Training loss:2.0685... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2484... Training loss:2.0785... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2485... Training loss:2.0894... 0.4658 sec/batch\n",
      "Epoch:13/20... Training Step:2486... Training loss:2.0728... 0.4568 sec/batch\n",
      "Epoch:13/20... Training Step:2487... Training loss:2.0699... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2488... Training loss:2.0694... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2489... Training loss:2.0637... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2490... Training loss:2.0639... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2491... Training loss:2.0599... 0.4318 sec/batch\n",
      "Epoch:13/20... Training Step:2492... Training loss:2.0332... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2493... Training loss:2.0836... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2494... Training loss:2.0663... 0.4598 sec/batch\n",
      "Epoch:13/20... Training Step:2495... Training loss:2.0778... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2496... Training loss:2.0717... 0.4378 sec/batch\n",
      "Epoch:13/20... Training Step:2497... Training loss:2.0840... 0.4548 sec/batch\n",
      "Epoch:13/20... Training Step:2498... Training loss:2.0528... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2499... Training loss:2.0428... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2500... Training loss:2.0901... 0.4548 sec/batch\n",
      "Epoch:13/20... Training Step:2501... Training loss:2.0640... 0.4528 sec/batch\n",
      "Epoch:13/20... Training Step:2502... Training loss:2.0305... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2503... Training loss:2.0760... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2504... Training loss:2.0724... 0.4368 sec/batch\n",
      "Epoch:13/20... Training Step:2505... Training loss:2.0708... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2506... Training loss:2.0786... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2507... Training loss:2.0553... 0.4578 sec/batch\n",
      "Epoch:13/20... Training Step:2508... Training loss:2.0445... 0.4368 sec/batch\n",
      "Epoch:13/20... Training Step:2509... Training loss:2.0823... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2510... Training loss:2.0703... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2511... Training loss:2.0737... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2512... Training loss:2.0791... 0.4578 sec/batch\n",
      "Epoch:13/20... Training Step:2513... Training loss:2.0755... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2514... Training loss:2.0736... 0.4628 sec/batch\n",
      "Epoch:13/20... Training Step:2515... Training loss:2.1127... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2516... Training loss:2.0519... 0.4528 sec/batch\n",
      "Epoch:13/20... Training Step:2517... Training loss:2.0785... 0.4518 sec/batch\n",
      "Epoch:13/20... Training Step:2518... Training loss:2.0562... 0.4488 sec/batch\n",
      "Epoch:13/20... Training Step:2519... Training loss:2.0484... 0.4588 sec/batch\n",
      "Epoch:13/20... Training Step:2520... Training loss:2.0610... 0.4538 sec/batch\n",
      "Epoch:13/20... Training Step:2521... Training loss:2.0590... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2522... Training loss:2.0869... 0.4568 sec/batch\n",
      "Epoch:13/20... Training Step:2523... Training loss:2.0939... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2524... Training loss:2.0945... 0.4568 sec/batch\n",
      "Epoch:13/20... Training Step:2525... Training loss:2.0613... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2526... Training loss:2.0598... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2527... Training loss:2.0649... 0.4528 sec/batch\n",
      "Epoch:13/20... Training Step:2528... Training loss:2.0996... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2529... Training loss:2.0793... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2530... Training loss:2.0872... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2531... Training loss:2.0701... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2532... Training loss:2.0590... 0.4518 sec/batch\n",
      "Epoch:13/20... Training Step:2533... Training loss:2.0757... 0.4468 sec/batch\n",
      "Epoch:13/20... Training Step:2534... Training loss:2.0772... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2535... Training loss:2.0382... 0.4478 sec/batch\n",
      "Epoch:13/20... Training Step:2536... Training loss:2.1045... 0.4608 sec/batch\n",
      "Epoch:13/20... Training Step:2537... Training loss:2.0961... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2538... Training loss:2.0611... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2539... Training loss:2.0781... 0.4658 sec/batch\n",
      "Epoch:13/20... Training Step:2540... Training loss:2.0821... 0.4548 sec/batch\n",
      "Epoch:13/20... Training Step:2541... Training loss:2.0687... 0.4707 sec/batch\n",
      "Epoch:13/20... Training Step:2542... Training loss:2.0696... 0.4488 sec/batch\n",
      "Epoch:13/20... Training Step:2543... Training loss:2.0915... 0.4548 sec/batch\n",
      "Epoch:13/20... Training Step:2544... Training loss:2.1085... 0.4568 sec/batch\n",
      "Epoch:13/20... Training Step:2545... Training loss:2.0791... 0.4498 sec/batch\n",
      "Epoch:13/20... Training Step:2546... Training loss:2.0607... 0.4388 sec/batch\n",
      "Epoch:13/20... Training Step:2547... Training loss:2.0513... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2548... Training loss:2.0715... 0.4358 sec/batch\n",
      "Epoch:13/20... Training Step:2549... Training loss:2.0941... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2550... Training loss:2.0795... 0.4528 sec/batch\n",
      "Epoch:13/20... Training Step:2551... Training loss:2.0840... 0.4408 sec/batch\n",
      "Epoch:13/20... Training Step:2552... Training loss:2.0890... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2553... Training loss:2.0646... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2554... Training loss:2.0806... 0.4448 sec/batch\n",
      "Epoch:13/20... Training Step:2555... Training loss:2.0550... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2556... Training loss:2.0436... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2557... Training loss:2.0504... 0.4488 sec/batch\n",
      "Epoch:13/20... Training Step:2558... Training loss:2.0782... 0.4418 sec/batch\n",
      "Epoch:13/20... Training Step:2559... Training loss:2.0719... 0.4558 sec/batch\n",
      "Epoch:13/20... Training Step:2560... Training loss:2.0854... 0.4328 sec/batch\n",
      "Epoch:13/20... Training Step:2561... Training loss:2.0806... 0.4398 sec/batch\n",
      "Epoch:13/20... Training Step:2562... Training loss:2.0667... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2563... Training loss:2.0664... 0.4548 sec/batch\n",
      "Epoch:13/20... Training Step:2564... Training loss:2.0480... 0.4358 sec/batch\n",
      "Epoch:13/20... Training Step:2565... Training loss:2.0498... 0.4508 sec/batch\n",
      "Epoch:13/20... Training Step:2566... Training loss:2.0561... 0.4338 sec/batch\n",
      "Epoch:13/20... Training Step:2567... Training loss:2.0816... 0.4438 sec/batch\n",
      "Epoch:13/20... Training Step:2568... Training loss:2.0444... 0.4538 sec/batch\n",
      "Epoch:13/20... Training Step:2569... Training loss:2.0576... 0.4338 sec/batch\n",
      "Epoch:13/20... Training Step:2570... Training loss:2.0620... 0.4378 sec/batch\n",
      "Epoch:13/20... Training Step:2571... Training loss:2.0458... 0.4408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13/20... Training Step:2572... Training loss:2.0737... 0.4458 sec/batch\n",
      "Epoch:13/20... Training Step:2573... Training loss:2.0602... 0.4428 sec/batch\n",
      "Epoch:13/20... Training Step:2574... Training loss:2.0516... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2575... Training loss:2.2352... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2576... Training loss:2.0421... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2577... Training loss:2.0409... 0.4418 sec/batch\n",
      "Epoch:14/20... Training Step:2578... Training loss:2.0512... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2579... Training loss:2.0544... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2580... Training loss:2.0337... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2581... Training loss:2.0616... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2582... Training loss:2.0526... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2583... Training loss:2.0851... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2584... Training loss:2.0548... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2585... Training loss:2.0436... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2586... Training loss:2.0384... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2587... Training loss:2.0530... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2588... Training loss:2.0891... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2589... Training loss:2.0538... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2590... Training loss:2.0380... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2591... Training loss:2.0507... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2592... Training loss:2.1043... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2593... Training loss:2.0603... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2594... Training loss:2.0542... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2595... Training loss:2.0509... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2596... Training loss:2.0878... 0.4368 sec/batch\n",
      "Epoch:14/20... Training Step:2597... Training loss:2.0570... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2598... Training loss:2.0431... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2599... Training loss:2.0559... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2600... Training loss:2.0335... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2601... Training loss:2.0401... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2602... Training loss:2.0616... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2603... Training loss:2.0830... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2604... Training loss:2.0569... 0.4548 sec/batch\n",
      "Epoch:14/20... Training Step:2605... Training loss:2.0568... 0.4418 sec/batch\n",
      "Epoch:14/20... Training Step:2606... Training loss:2.0423... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2607... Training loss:2.0553... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2608... Training loss:2.0880... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2609... Training loss:2.0425... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2610... Training loss:2.0504... 0.4548 sec/batch\n",
      "Epoch:14/20... Training Step:2611... Training loss:2.0506... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2612... Training loss:2.0175... 0.4598 sec/batch\n",
      "Epoch:14/20... Training Step:2613... Training loss:2.0175... 0.4628 sec/batch\n",
      "Epoch:14/20... Training Step:2614... Training loss:2.0238... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2615... Training loss:2.0387... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2616... Training loss:2.0479... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2617... Training loss:2.0192... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2618... Training loss:2.0333... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2619... Training loss:2.0636... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2620... Training loss:1.9922... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2621... Training loss:2.0607... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2622... Training loss:2.0366... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2623... Training loss:2.0510... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2624... Training loss:2.0891... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2625... Training loss:2.0261... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2626... Training loss:2.0971... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2627... Training loss:2.0390... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2628... Training loss:2.0459... 0.4368 sec/batch\n",
      "Epoch:14/20... Training Step:2629... Training loss:2.0535... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2630... Training loss:2.0717... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2631... Training loss:2.0594... 0.4418 sec/batch\n",
      "Epoch:14/20... Training Step:2632... Training loss:2.0394... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2633... Training loss:2.0302... 0.4548 sec/batch\n",
      "Epoch:14/20... Training Step:2634... Training loss:2.0862... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2635... Training loss:2.0504... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2636... Training loss:2.0834... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2637... Training loss:2.0824... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2638... Training loss:2.0709... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2639... Training loss:2.0450... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2640... Training loss:2.0735... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2641... Training loss:2.0643... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2642... Training loss:2.0102... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2643... Training loss:2.0304... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2644... Training loss:2.0459... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2645... Training loss:2.0957... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2646... Training loss:2.0522... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2647... Training loss:2.0619... 0.4608 sec/batch\n",
      "Epoch:14/20... Training Step:2648... Training loss:2.0265... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2649... Training loss:2.0484... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2650... Training loss:2.0909... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2651... Training loss:2.0503... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2652... Training loss:2.0536... 0.4468 sec/batch\n",
      "Epoch:14/20... Training Step:2653... Training loss:2.0225... 0.4548 sec/batch\n",
      "Epoch:14/20... Training Step:2654... Training loss:2.0304... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2655... Training loss:2.0096... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2656... Training loss:2.0560... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2657... Training loss:2.0082... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2658... Training loss:2.0383... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2659... Training loss:2.0068... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2660... Training loss:2.0274... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2661... Training loss:2.0422... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2662... Training loss:2.0181... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2663... Training loss:2.0056... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2664... Training loss:2.0442... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2665... Training loss:2.0181... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2666... Training loss:2.0364... 0.4588 sec/batch\n",
      "Epoch:14/20... Training Step:2667... Training loss:2.0149... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2668... Training loss:2.0139... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2669... Training loss:2.0196... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2670... Training loss:2.0337... 0.4548 sec/batch\n",
      "Epoch:14/20... Training Step:2671... Training loss:2.0389... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2672... Training loss:2.0305... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2673... Training loss:2.0161... 0.4857 sec/batch\n",
      "Epoch:14/20... Training Step:2674... Training loss:2.0008... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2675... Training loss:2.0404... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2676... Training loss:2.0451... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2677... Training loss:2.0251... 0.4488 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/20... Training Step:2678... Training loss:2.0283... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2679... Training loss:2.0215... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2680... Training loss:2.0389... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2681... Training loss:2.0420... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2682... Training loss:2.0455... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2683... Training loss:2.0513... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2684... Training loss:2.0334... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2685... Training loss:2.0472... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2686... Training loss:2.0329... 0.4608 sec/batch\n",
      "Epoch:14/20... Training Step:2687... Training loss:2.0440... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2688... Training loss:2.0294... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2689... Training loss:2.0265... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2690... Training loss:1.9966... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2691... Training loss:2.0389... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2692... Training loss:2.0308... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2693... Training loss:2.0543... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2694... Training loss:2.0336... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2695... Training loss:2.0585... 0.4717 sec/batch\n",
      "Epoch:14/20... Training Step:2696... Training loss:2.0148... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2697... Training loss:2.0195... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2698... Training loss:2.0561... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2699... Training loss:2.0254... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2700... Training loss:1.9971... 0.4657 sec/batch\n",
      "Epoch:14/20... Training Step:2701... Training loss:2.0535... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2702... Training loss:2.0461... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2703... Training loss:2.0408... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2704... Training loss:2.0452... 0.4368 sec/batch\n",
      "Epoch:14/20... Training Step:2705... Training loss:2.0179... 0.4588 sec/batch\n",
      "Epoch:14/20... Training Step:2706... Training loss:1.9997... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2707... Training loss:2.0416... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2708... Training loss:2.0451... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2709... Training loss:2.0319... 0.4339 sec/batch\n",
      "Epoch:14/20... Training Step:2710... Training loss:2.0515... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2711... Training loss:2.0438... 0.4508 sec/batch\n",
      "Epoch:14/20... Training Step:2712... Training loss:2.0377... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2713... Training loss:2.0783... 0.4658 sec/batch\n",
      "Epoch:14/20... Training Step:2714... Training loss:2.0242... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2715... Training loss:2.0592... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2716... Training loss:2.0226... 0.4378 sec/batch\n",
      "Epoch:14/20... Training Step:2717... Training loss:2.0361... 0.4568 sec/batch\n",
      "Epoch:14/20... Training Step:2718... Training loss:2.0249... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2719... Training loss:2.0251... 0.4428 sec/batch\n",
      "Epoch:14/20... Training Step:2720... Training loss:2.0562... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2721... Training loss:2.0538... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2722... Training loss:2.0605... 0.4468 sec/batch\n",
      "Epoch:14/20... Training Step:2723... Training loss:2.0450... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2724... Training loss:2.0276... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2725... Training loss:2.0324... 0.4358 sec/batch\n",
      "Epoch:14/20... Training Step:2726... Training loss:2.0636... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2727... Training loss:2.0465... 0.4538 sec/batch\n",
      "Epoch:14/20... Training Step:2728... Training loss:2.0418... 0.4338 sec/batch\n",
      "Epoch:14/20... Training Step:2729... Training loss:2.0335... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2730... Training loss:2.0330... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2731... Training loss:2.0398... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2732... Training loss:2.0307... 0.4418 sec/batch\n",
      "Epoch:14/20... Training Step:2733... Training loss:2.0076... 0.4568 sec/batch\n",
      "Epoch:14/20... Training Step:2734... Training loss:2.0666... 0.4348 sec/batch\n",
      "Epoch:14/20... Training Step:2735... Training loss:2.0588... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2736... Training loss:2.0320... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2737... Training loss:2.0472... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2738... Training loss:2.0453... 0.4348 sec/batch\n",
      "Epoch:14/20... Training Step:2739... Training loss:2.0443... 0.4578 sec/batch\n",
      "Epoch:14/20... Training Step:2740... Training loss:2.0253... 0.4458 sec/batch\n",
      "Epoch:14/20... Training Step:2741... Training loss:2.0474... 0.4478 sec/batch\n",
      "Epoch:14/20... Training Step:2742... Training loss:2.0633... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2743... Training loss:2.0410... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2744... Training loss:2.0267... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2745... Training loss:2.0203... 0.4398 sec/batch\n",
      "Epoch:14/20... Training Step:2746... Training loss:2.0250... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2747... Training loss:2.0519... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2748... Training loss:2.0522... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2749... Training loss:2.0519... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2750... Training loss:2.0404... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2751... Training loss:2.0239... 0.4558 sec/batch\n",
      "Epoch:14/20... Training Step:2752... Training loss:2.0481... 0.4438 sec/batch\n",
      "Epoch:14/20... Training Step:2753... Training loss:2.0108... 0.4578 sec/batch\n",
      "Epoch:14/20... Training Step:2754... Training loss:2.0076... 0.4498 sec/batch\n",
      "Epoch:14/20... Training Step:2755... Training loss:2.0157... 0.4408 sec/batch\n",
      "Epoch:14/20... Training Step:2756... Training loss:2.0444... 0.4388 sec/batch\n",
      "Epoch:14/20... Training Step:2757... Training loss:2.0334... 0.4348 sec/batch\n",
      "Epoch:14/20... Training Step:2758... Training loss:2.0573... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2759... Training loss:2.0395... 0.4568 sec/batch\n",
      "Epoch:14/20... Training Step:2760... Training loss:2.0257... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2761... Training loss:2.0311... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2762... Training loss:2.0135... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2763... Training loss:2.0280... 0.4468 sec/batch\n",
      "Epoch:14/20... Training Step:2764... Training loss:2.0249... 0.4518 sec/batch\n",
      "Epoch:14/20... Training Step:2765... Training loss:2.0417... 0.4348 sec/batch\n",
      "Epoch:14/20... Training Step:2766... Training loss:2.0013... 0.4418 sec/batch\n",
      "Epoch:14/20... Training Step:2767... Training loss:2.0347... 0.4528 sec/batch\n",
      "Epoch:14/20... Training Step:2768... Training loss:2.0226... 0.4468 sec/batch\n",
      "Epoch:14/20... Training Step:2769... Training loss:2.0184... 0.4368 sec/batch\n",
      "Epoch:14/20... Training Step:2770... Training loss:2.0348... 0.4488 sec/batch\n",
      "Epoch:14/20... Training Step:2771... Training loss:2.0261... 0.4448 sec/batch\n",
      "Epoch:14/20... Training Step:2772... Training loss:2.0140... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2773... Training loss:2.2010... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2774... Training loss:2.0066... 0.4518 sec/batch\n",
      "Epoch:15/20... Training Step:2775... Training loss:2.0106... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2776... Training loss:2.0171... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2777... Training loss:2.0226... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2778... Training loss:1.9987... 0.4518 sec/batch\n",
      "Epoch:15/20... Training Step:2779... Training loss:2.0327... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2780... Training loss:2.0180... 0.4578 sec/batch\n",
      "Epoch:15/20... Training Step:2781... Training loss:2.0526... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2782... Training loss:2.0171... 0.4638 sec/batch\n",
      "Epoch:15/20... Training Step:2783... Training loss:2.0149... 0.4408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/20... Training Step:2784... Training loss:2.0073... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2785... Training loss:2.0290... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2786... Training loss:2.0668... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2787... Training loss:2.0214... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2788... Training loss:2.0063... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2789... Training loss:2.0208... 0.4478 sec/batch\n",
      "Epoch:15/20... Training Step:2790... Training loss:2.0791... 0.4478 sec/batch\n",
      "Epoch:15/20... Training Step:2791... Training loss:2.0403... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2792... Training loss:2.0247... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2793... Training loss:2.0274... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2794... Training loss:2.0672... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2795... Training loss:2.0265... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2796... Training loss:2.0264... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2797... Training loss:2.0254... 0.4568 sec/batch\n",
      "Epoch:15/20... Training Step:2798... Training loss:2.0114... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2799... Training loss:2.0153... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2800... Training loss:2.0367... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2801... Training loss:2.0516... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2802... Training loss:2.0446... 0.4638 sec/batch\n",
      "Epoch:15/20... Training Step:2803... Training loss:2.0215... 0.4508 sec/batch\n",
      "Epoch:15/20... Training Step:2804... Training loss:2.0032... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2805... Training loss:2.0217... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2806... Training loss:2.0710... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2807... Training loss:2.0153... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2808... Training loss:2.0220... 0.4538 sec/batch\n",
      "Epoch:15/20... Training Step:2809... Training loss:2.0172... 0.4518 sec/batch\n",
      "Epoch:15/20... Training Step:2810... Training loss:1.9873... 0.4538 sec/batch\n",
      "Epoch:15/20... Training Step:2811... Training loss:1.9950... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2812... Training loss:1.9906... 0.4578 sec/batch\n",
      "Epoch:15/20... Training Step:2813... Training loss:2.0101... 0.4508 sec/batch\n",
      "Epoch:15/20... Training Step:2814... Training loss:2.0248... 0.4578 sec/batch\n",
      "Epoch:15/20... Training Step:2815... Training loss:1.9975... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2816... Training loss:1.9948... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2817... Training loss:2.0143... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2818... Training loss:1.9616... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2819... Training loss:2.0275... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2820... Training loss:2.0117... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2821... Training loss:2.0067... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2822... Training loss:2.0605... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2823... Training loss:1.9837... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2824... Training loss:2.0591... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2825... Training loss:2.0195... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2826... Training loss:2.0258... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2827... Training loss:2.0138... 0.4378 sec/batch\n",
      "Epoch:15/20... Training Step:2828... Training loss:2.0279... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2829... Training loss:2.0351... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2830... Training loss:2.0147... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2831... Training loss:1.9975... 0.4358 sec/batch\n",
      "Epoch:15/20... Training Step:2832... Training loss:2.0517... 0.4528 sec/batch\n",
      "Epoch:15/20... Training Step:2833... Training loss:2.0148... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2834... Training loss:2.0558... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2835... Training loss:2.0495... 0.4578 sec/batch\n",
      "Epoch:15/20... Training Step:2836... Training loss:2.0214... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2837... Training loss:2.0167... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2838... Training loss:2.0446... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2839... Training loss:2.0258... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2840... Training loss:1.9909... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2841... Training loss:2.0122... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2842... Training loss:2.0157... 0.4348 sec/batch\n",
      "Epoch:15/20... Training Step:2843... Training loss:2.0510... 0.4508 sec/batch\n",
      "Epoch:15/20... Training Step:2844... Training loss:2.0264... 0.4528 sec/batch\n",
      "Epoch:15/20... Training Step:2845... Training loss:2.0390... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2846... Training loss:2.0021... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2847... Training loss:2.0126... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2848... Training loss:2.0539... 0.4358 sec/batch\n",
      "Epoch:15/20... Training Step:2849... Training loss:2.0154... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2850... Training loss:2.0303... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2851... Training loss:1.9957... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2852... Training loss:2.0015... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2853... Training loss:1.9900... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2854... Training loss:2.0374... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2855... Training loss:1.9843... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2856... Training loss:2.0011... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2857... Training loss:1.9784... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2858... Training loss:1.9978... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2859... Training loss:2.0034... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2860... Training loss:1.9986... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2861... Training loss:1.9833... 0.4348 sec/batch\n",
      "Epoch:15/20... Training Step:2862... Training loss:2.0221... 0.4608 sec/batch\n",
      "Epoch:15/20... Training Step:2863... Training loss:1.9888... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2864... Training loss:2.0130... 0.4528 sec/batch\n",
      "Epoch:15/20... Training Step:2865... Training loss:1.9778... 0.4478 sec/batch\n",
      "Epoch:15/20... Training Step:2866... Training loss:1.9866... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2867... Training loss:1.9835... 0.4548 sec/batch\n",
      "Epoch:15/20... Training Step:2868... Training loss:2.0018... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2869... Training loss:2.0027... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2870... Training loss:1.9943... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2871... Training loss:1.9797... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2872... Training loss:1.9802... 0.4508 sec/batch\n",
      "Epoch:15/20... Training Step:2873... Training loss:2.0271... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2874... Training loss:2.0169... 0.4538 sec/batch\n",
      "Epoch:15/20... Training Step:2875... Training loss:1.9954... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2876... Training loss:2.0004... 0.4518 sec/batch\n",
      "Epoch:15/20... Training Step:2877... Training loss:1.9930... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2878... Training loss:1.9960... 0.4478 sec/batch\n",
      "Epoch:15/20... Training Step:2879... Training loss:1.9920... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2880... Training loss:2.0037... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2881... Training loss:2.0252... 0.4518 sec/batch\n",
      "Epoch:15/20... Training Step:2882... Training loss:2.0045... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2883... Training loss:2.0070... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2884... Training loss:1.9956... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2885... Training loss:2.0127... 0.4598 sec/batch\n",
      "Epoch:15/20... Training Step:2886... Training loss:1.9883... 0.4538 sec/batch\n",
      "Epoch:15/20... Training Step:2887... Training loss:1.9962... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2888... Training loss:1.9641... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2889... Training loss:2.0110... 0.4468 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:15/20... Training Step:2890... Training loss:2.0017... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2891... Training loss:2.0082... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2892... Training loss:1.9993... 0.4328 sec/batch\n",
      "Epoch:15/20... Training Step:2893... Training loss:2.0160... 0.4348 sec/batch\n",
      "Epoch:15/20... Training Step:2894... Training loss:1.9842... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2895... Training loss:1.9825... 0.4488 sec/batch\n",
      "Epoch:15/20... Training Step:2896... Training loss:2.0346... 0.4498 sec/batch\n",
      "Epoch:15/20... Training Step:2897... Training loss:2.0011... 0.4359 sec/batch\n",
      "Epoch:15/20... Training Step:2898... Training loss:1.9747... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2899... Training loss:2.0255... 0.4638 sec/batch\n",
      "Epoch:15/20... Training Step:2900... Training loss:2.0170... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2901... Training loss:2.0078... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2902... Training loss:2.0098... 0.4338 sec/batch\n",
      "Epoch:15/20... Training Step:2903... Training loss:1.9944... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2904... Training loss:1.9758... 0.4328 sec/batch\n",
      "Epoch:15/20... Training Step:2905... Training loss:2.0160... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2906... Training loss:2.0121... 0.4578 sec/batch\n",
      "Epoch:15/20... Training Step:2907... Training loss:2.0083... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2908... Training loss:2.0235... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2909... Training loss:2.0093... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2910... Training loss:2.0136... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2911... Training loss:2.0451... 0.4358 sec/batch\n",
      "Epoch:15/20... Training Step:2912... Training loss:1.9913... 0.4388 sec/batch\n",
      "Epoch:15/20... Training Step:2913... Training loss:2.0208... 0.4558 sec/batch\n",
      "Epoch:15/20... Training Step:2914... Training loss:1.9987... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2915... Training loss:1.9959... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2916... Training loss:1.9984... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2917... Training loss:1.9927... 0.4348 sec/batch\n",
      "Epoch:15/20... Training Step:2918... Training loss:2.0219... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2919... Training loss:2.0328... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2920... Training loss:2.0305... 0.4338 sec/batch\n",
      "Epoch:15/20... Training Step:2921... Training loss:2.0201... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2922... Training loss:1.9986... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2923... Training loss:1.9882... 0.4468 sec/batch\n",
      "Epoch:15/20... Training Step:2924... Training loss:2.0422... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2925... Training loss:2.0242... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2926... Training loss:2.0093... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2927... Training loss:2.0047... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2928... Training loss:1.9917... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2929... Training loss:2.0034... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2930... Training loss:2.0035... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2931... Training loss:1.9761... 0.4378 sec/batch\n",
      "Epoch:15/20... Training Step:2932... Training loss:2.0328... 0.4299 sec/batch\n",
      "Epoch:15/20... Training Step:2933... Training loss:2.0181... 0.4628 sec/batch\n",
      "Epoch:15/20... Training Step:2934... Training loss:2.0014... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2935... Training loss:2.0204... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2936... Training loss:2.0141... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2937... Training loss:2.0043... 0.4378 sec/batch\n",
      "Epoch:15/20... Training Step:2938... Training loss:2.0031... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2939... Training loss:2.0117... 0.4318 sec/batch\n",
      "Epoch:15/20... Training Step:2940... Training loss:2.0403... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2941... Training loss:2.0026... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2942... Training loss:1.9942... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2943... Training loss:1.9742... 0.4329 sec/batch\n",
      "Epoch:15/20... Training Step:2944... Training loss:1.9914... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2945... Training loss:2.0230... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2946... Training loss:2.0175... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2947... Training loss:2.0074... 0.4458 sec/batch\n",
      "Epoch:15/20... Training Step:2948... Training loss:2.0087... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2949... Training loss:1.9944... 0.4348 sec/batch\n",
      "Epoch:15/20... Training Step:2950... Training loss:2.0127... 0.4378 sec/batch\n",
      "Epoch:15/20... Training Step:2951... Training loss:1.9816... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2952... Training loss:1.9760... 0.4359 sec/batch\n",
      "Epoch:15/20... Training Step:2953... Training loss:1.9852... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2954... Training loss:2.0086... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2955... Training loss:2.0105... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2956... Training loss:2.0331... 0.4398 sec/batch\n",
      "Epoch:15/20... Training Step:2957... Training loss:2.0098... 0.4298 sec/batch\n",
      "Epoch:15/20... Training Step:2958... Training loss:1.9958... 0.4358 sec/batch\n",
      "Epoch:15/20... Training Step:2959... Training loss:1.9982... 0.4378 sec/batch\n",
      "Epoch:15/20... Training Step:2960... Training loss:1.9832... 0.4368 sec/batch\n",
      "Epoch:15/20... Training Step:2961... Training loss:1.9800... 0.4358 sec/batch\n",
      "Epoch:15/20... Training Step:2962... Training loss:1.9915... 0.4418 sec/batch\n",
      "Epoch:15/20... Training Step:2963... Training loss:2.0204... 0.4438 sec/batch\n",
      "Epoch:15/20... Training Step:2964... Training loss:1.9700... 0.4338 sec/batch\n",
      "Epoch:15/20... Training Step:2965... Training loss:1.9966... 0.4428 sec/batch\n",
      "Epoch:15/20... Training Step:2966... Training loss:1.9855... 0.4528 sec/batch\n",
      "Epoch:15/20... Training Step:2967... Training loss:1.9830... 0.4548 sec/batch\n",
      "Epoch:15/20... Training Step:2968... Training loss:1.9977... 0.4408 sec/batch\n",
      "Epoch:15/20... Training Step:2969... Training loss:2.0002... 0.4448 sec/batch\n",
      "Epoch:15/20... Training Step:2970... Training loss:1.9809... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:2971... Training loss:2.1701... 0.4308 sec/batch\n",
      "Epoch:16/20... Training Step:2972... Training loss:1.9734... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:2973... Training loss:1.9763... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:2974... Training loss:1.9880... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:2975... Training loss:1.9868... 0.4548 sec/batch\n",
      "Epoch:16/20... Training Step:2976... Training loss:1.9659... 0.4288 sec/batch\n",
      "Epoch:16/20... Training Step:2977... Training loss:1.9931... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:2978... Training loss:1.9987... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:2979... Training loss:2.0265... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:2980... Training loss:1.9857... 0.4508 sec/batch\n",
      "Epoch:16/20... Training Step:2981... Training loss:1.9671... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:2982... Training loss:1.9698... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:2983... Training loss:1.9959... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:2984... Training loss:2.0341... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:2985... Training loss:1.9840... 0.4528 sec/batch\n",
      "Epoch:16/20... Training Step:2986... Training loss:1.9708... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:2987... Training loss:1.9990... 0.4618 sec/batch\n",
      "Epoch:16/20... Training Step:2988... Training loss:2.0364... 0.4598 sec/batch\n",
      "Epoch:16/20... Training Step:2989... Training loss:2.0080... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:2990... Training loss:2.0058... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:2991... Training loss:1.9888... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:2992... Training loss:2.0336... 0.4458 sec/batch\n",
      "Epoch:16/20... Training Step:2993... Training loss:1.9976... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:2994... Training loss:1.9921... 0.4518 sec/batch\n",
      "Epoch:16/20... Training Step:2995... Training loss:1.9978... 0.4408 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/20... Training Step:2996... Training loss:1.9742... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:2997... Training loss:1.9787... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:2998... Training loss:2.0083... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:2999... Training loss:2.0246... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3000... Training loss:1.9983... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3001... Training loss:2.0050... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3002... Training loss:1.9690... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3003... Training loss:1.9987... 0.4568 sec/batch\n",
      "Epoch:16/20... Training Step:3004... Training loss:2.0312... 0.4309 sec/batch\n",
      "Epoch:16/20... Training Step:3005... Training loss:1.9821... 0.4368 sec/batch\n",
      "Epoch:16/20... Training Step:3006... Training loss:1.9934... 0.4558 sec/batch\n",
      "Epoch:16/20... Training Step:3007... Training loss:1.9966... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3008... Training loss:1.9498... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3009... Training loss:1.9580... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3010... Training loss:1.9668... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3011... Training loss:1.9792... 0.4329 sec/batch\n",
      "Epoch:16/20... Training Step:3012... Training loss:1.9967... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3013... Training loss:1.9683... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3014... Training loss:1.9696... 0.4349 sec/batch\n",
      "Epoch:16/20... Training Step:3015... Training loss:2.0019... 0.4328 sec/batch\n",
      "Epoch:16/20... Training Step:3016... Training loss:1.9291... 0.4488 sec/batch\n",
      "Epoch:16/20... Training Step:3017... Training loss:2.0023... 0.4508 sec/batch\n",
      "Epoch:16/20... Training Step:3018... Training loss:1.9721... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3019... Training loss:1.9845... 0.4518 sec/batch\n",
      "Epoch:16/20... Training Step:3020... Training loss:2.0226... 0.4498 sec/batch\n",
      "Epoch:16/20... Training Step:3021... Training loss:1.9501... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3022... Training loss:2.0381... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3023... Training loss:1.9917... 0.4309 sec/batch\n",
      "Epoch:16/20... Training Step:3024... Training loss:1.9827... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3025... Training loss:1.9836... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3026... Training loss:1.9999... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3027... Training loss:1.9922... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3028... Training loss:1.9861... 0.4379 sec/batch\n",
      "Epoch:16/20... Training Step:3029... Training loss:1.9749... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3030... Training loss:2.0334... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3031... Training loss:1.9837... 0.4308 sec/batch\n",
      "Epoch:16/20... Training Step:3032... Training loss:2.0297... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3033... Training loss:2.0219... 0.4518 sec/batch\n",
      "Epoch:16/20... Training Step:3034... Training loss:2.0074... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3035... Training loss:1.9755... 0.4458 sec/batch\n",
      "Epoch:16/20... Training Step:3036... Training loss:2.0164... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3037... Training loss:2.0137... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3038... Training loss:1.9587... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3039... Training loss:1.9729... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3040... Training loss:1.9853... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3041... Training loss:2.0201... 0.4488 sec/batch\n",
      "Epoch:16/20... Training Step:3042... Training loss:1.9972... 0.4528 sec/batch\n",
      "Epoch:16/20... Training Step:3043... Training loss:2.0136... 0.4318 sec/batch\n",
      "Epoch:16/20... Training Step:3044... Training loss:1.9745... 0.4308 sec/batch\n",
      "Epoch:16/20... Training Step:3045... Training loss:1.9847... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3046... Training loss:2.0302... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3047... Training loss:1.9914... 0.4478 sec/batch\n",
      "Epoch:16/20... Training Step:3048... Training loss:1.9977... 0.4329 sec/batch\n",
      "Epoch:16/20... Training Step:3049... Training loss:1.9651... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3050... Training loss:1.9743... 0.4318 sec/batch\n",
      "Epoch:16/20... Training Step:3051... Training loss:1.9614... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3052... Training loss:2.0012... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3053... Training loss:1.9562... 0.4339 sec/batch\n",
      "Epoch:16/20... Training Step:3054... Training loss:1.9849... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3055... Training loss:1.9530... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3056... Training loss:1.9770... 0.4697 sec/batch\n",
      "Epoch:16/20... Training Step:3057... Training loss:1.9744... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3058... Training loss:1.9726... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3059... Training loss:1.9483... 0.4279 sec/batch\n",
      "Epoch:16/20... Training Step:3060... Training loss:2.0093... 0.4458 sec/batch\n",
      "Epoch:16/20... Training Step:3061... Training loss:1.9640... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3062... Training loss:1.9738... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3063... Training loss:1.9564... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3064... Training loss:1.9558... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3065... Training loss:1.9637... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3066... Training loss:1.9779... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3067... Training loss:1.9737... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3068... Training loss:1.9587... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3069... Training loss:1.9649... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3070... Training loss:1.9394... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3071... Training loss:1.9839... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3072... Training loss:1.9807... 0.4687 sec/batch\n",
      "Epoch:16/20... Training Step:3073... Training loss:1.9650... 0.4368 sec/batch\n",
      "Epoch:16/20... Training Step:3074... Training loss:1.9714... 0.4568 sec/batch\n",
      "Epoch:16/20... Training Step:3075... Training loss:1.9619... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3076... Training loss:1.9751... 0.4368 sec/batch\n",
      "Epoch:16/20... Training Step:3077... Training loss:1.9805... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3078... Training loss:1.9755... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3079... Training loss:2.0088... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:3080... Training loss:1.9725... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3081... Training loss:1.9783... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3082... Training loss:1.9750... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3083... Training loss:1.9786... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:3084... Training loss:1.9713... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3085... Training loss:1.9649... 0.4278 sec/batch\n",
      "Epoch:16/20... Training Step:3086... Training loss:1.9365... 0.4379 sec/batch\n",
      "Epoch:16/20... Training Step:3087... Training loss:1.9784... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3088... Training loss:1.9804... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3089... Training loss:1.9854... 0.4368 sec/batch\n",
      "Epoch:16/20... Training Step:3090... Training loss:1.9759... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3091... Training loss:1.9871... 0.4318 sec/batch\n",
      "Epoch:16/20... Training Step:3092... Training loss:1.9503... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:3093... Training loss:1.9518... 0.4269 sec/batch\n",
      "Epoch:16/20... Training Step:3094... Training loss:2.0040... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3095... Training loss:1.9635... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3096... Training loss:1.9348... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3097... Training loss:1.9926... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3098... Training loss:1.9879... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3099... Training loss:1.9730... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3100... Training loss:1.9734... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3101... Training loss:1.9531... 0.4368 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:16/20... Training Step:3102... Training loss:1.9510... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3103... Training loss:1.9878... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3104... Training loss:1.9871... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3105... Training loss:1.9811... 0.4269 sec/batch\n",
      "Epoch:16/20... Training Step:3106... Training loss:1.9983... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3107... Training loss:1.9851... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3108... Training loss:1.9818... 0.4478 sec/batch\n",
      "Epoch:16/20... Training Step:3109... Training loss:2.0151... 0.4328 sec/batch\n",
      "Epoch:16/20... Training Step:3110... Training loss:1.9602... 0.4328 sec/batch\n",
      "Epoch:16/20... Training Step:3111... Training loss:1.9994... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3112... Training loss:1.9685... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3113... Training loss:1.9757... 0.4298 sec/batch\n",
      "Epoch:16/20... Training Step:3114... Training loss:1.9707... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3115... Training loss:1.9572... 0.4498 sec/batch\n",
      "Epoch:16/20... Training Step:3116... Training loss:1.9906... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3117... Training loss:1.9902... 0.4478 sec/batch\n",
      "Epoch:16/20... Training Step:3118... Training loss:2.0019... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3119... Training loss:1.9693... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3120... Training loss:1.9567... 0.4339 sec/batch\n",
      "Epoch:16/20... Training Step:3121... Training loss:1.9656... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3122... Training loss:2.0080... 0.4458 sec/batch\n",
      "Epoch:16/20... Training Step:3123... Training loss:1.9805... 0.4309 sec/batch\n",
      "Epoch:16/20... Training Step:3124... Training loss:1.9914... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3125... Training loss:1.9674... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3126... Training loss:1.9707... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3127... Training loss:1.9790... 0.4518 sec/batch\n",
      "Epoch:16/20... Training Step:3128... Training loss:1.9778... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3129... Training loss:1.9473... 0.4329 sec/batch\n",
      "Epoch:16/20... Training Step:3130... Training loss:2.0098... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3131... Training loss:1.9997... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3132... Training loss:1.9709... 0.4438 sec/batch\n",
      "Epoch:16/20... Training Step:3133... Training loss:2.0006... 0.4308 sec/batch\n",
      "Epoch:16/20... Training Step:3134... Training loss:1.9911... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3135... Training loss:1.9696... 0.4508 sec/batch\n",
      "Epoch:16/20... Training Step:3136... Training loss:1.9626... 0.4458 sec/batch\n",
      "Epoch:16/20... Training Step:3137... Training loss:1.9759... 0.4338 sec/batch\n",
      "Epoch:16/20... Training Step:3138... Training loss:2.0189... 0.4498 sec/batch\n",
      "Epoch:16/20... Training Step:3139... Training loss:1.9716... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3140... Training loss:1.9608... 0.4339 sec/batch\n",
      "Epoch:16/20... Training Step:3141... Training loss:1.9597... 0.4428 sec/batch\n",
      "Epoch:16/20... Training Step:3142... Training loss:1.9712... 0.4598 sec/batch\n",
      "Epoch:16/20... Training Step:3143... Training loss:1.9966... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3144... Training loss:1.9906... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3145... Training loss:1.9932... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3146... Training loss:1.9853... 0.4269 sec/batch\n",
      "Epoch:16/20... Training Step:3147... Training loss:1.9671... 0.4348 sec/batch\n",
      "Epoch:16/20... Training Step:3148... Training loss:1.9926... 0.4518 sec/batch\n",
      "Epoch:16/20... Training Step:3149... Training loss:1.9541... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3150... Training loss:1.9458... 0.4388 sec/batch\n",
      "Epoch:16/20... Training Step:3151... Training loss:1.9507... 0.4358 sec/batch\n",
      "Epoch:16/20... Training Step:3152... Training loss:1.9831... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3153... Training loss:1.9776... 0.4478 sec/batch\n",
      "Epoch:16/20... Training Step:3154... Training loss:2.0026... 0.4957 sec/batch\n",
      "Epoch:16/20... Training Step:3155... Training loss:1.9902... 0.4638 sec/batch\n",
      "Epoch:16/20... Training Step:3156... Training loss:1.9548... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:3157... Training loss:1.9683... 0.4448 sec/batch\n",
      "Epoch:16/20... Training Step:3158... Training loss:1.9581... 0.4408 sec/batch\n",
      "Epoch:16/20... Training Step:3159... Training loss:1.9615... 0.4378 sec/batch\n",
      "Epoch:16/20... Training Step:3160... Training loss:1.9657... 0.4468 sec/batch\n",
      "Epoch:16/20... Training Step:3161... Training loss:1.9893... 0.4318 sec/batch\n",
      "Epoch:16/20... Training Step:3162... Training loss:1.9401... 0.4568 sec/batch\n",
      "Epoch:16/20... Training Step:3163... Training loss:1.9811... 0.4269 sec/batch\n",
      "Epoch:16/20... Training Step:3164... Training loss:1.9598... 0.4418 sec/batch\n",
      "Epoch:16/20... Training Step:3165... Training loss:1.9448... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3166... Training loss:1.9797... 0.4398 sec/batch\n",
      "Epoch:16/20... Training Step:3167... Training loss:1.9711... 0.4478 sec/batch\n",
      "Epoch:16/20... Training Step:3168... Training loss:1.9580... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3169... Training loss:2.1352... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3170... Training loss:1.9484... 0.4448 sec/batch\n",
      "Epoch:17/20... Training Step:3171... Training loss:1.9503... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3172... Training loss:1.9567... 0.4518 sec/batch\n",
      "Epoch:17/20... Training Step:3173... Training loss:1.9570... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3174... Training loss:1.9405... 0.4329 sec/batch\n",
      "Epoch:17/20... Training Step:3175... Training loss:1.9704... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3176... Training loss:1.9550... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3177... Training loss:1.9960... 0.4298 sec/batch\n",
      "Epoch:17/20... Training Step:3178... Training loss:1.9590... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3179... Training loss:1.9453... 0.4318 sec/batch\n",
      "Epoch:17/20... Training Step:3180... Training loss:1.9488... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3181... Training loss:1.9614... 0.4358 sec/batch\n",
      "Epoch:17/20... Training Step:3182... Training loss:2.0017... 0.4309 sec/batch\n",
      "Epoch:17/20... Training Step:3183... Training loss:1.9535... 0.4518 sec/batch\n",
      "Epoch:17/20... Training Step:3184... Training loss:1.9533... 0.4279 sec/batch\n",
      "Epoch:17/20... Training Step:3185... Training loss:1.9696... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3186... Training loss:2.0095... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3187... Training loss:1.9716... 0.4349 sec/batch\n",
      "Epoch:17/20... Training Step:3188... Training loss:1.9770... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3189... Training loss:1.9605... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3190... Training loss:1.9997... 0.4478 sec/batch\n",
      "Epoch:17/20... Training Step:3191... Training loss:1.9717... 0.4508 sec/batch\n",
      "Epoch:17/20... Training Step:3192... Training loss:1.9620... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3193... Training loss:1.9651... 0.4329 sec/batch\n",
      "Epoch:17/20... Training Step:3194... Training loss:1.9348... 0.4488 sec/batch\n",
      "Epoch:17/20... Training Step:3195... Training loss:1.9433... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3196... Training loss:1.9727... 0.4508 sec/batch\n",
      "Epoch:17/20... Training Step:3197... Training loss:1.9955... 0.4318 sec/batch\n",
      "Epoch:17/20... Training Step:3198... Training loss:1.9827... 0.4298 sec/batch\n",
      "Epoch:17/20... Training Step:3199... Training loss:1.9714... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3200... Training loss:1.9451... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3201... Training loss:1.9741... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3202... Training loss:1.9990... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3203... Training loss:1.9408... 0.4518 sec/batch\n",
      "Epoch:17/20... Training Step:3204... Training loss:1.9604... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3205... Training loss:1.9633... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3206... Training loss:1.9249... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3207... Training loss:1.9225... 0.4378 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/20... Training Step:3208... Training loss:1.9397... 0.4498 sec/batch\n",
      "Epoch:17/20... Training Step:3209... Training loss:1.9450... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3210... Training loss:1.9728... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3211... Training loss:1.9426... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3212... Training loss:1.9359... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3213... Training loss:1.9714... 0.4338 sec/batch\n",
      "Epoch:17/20... Training Step:3214... Training loss:1.9125... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3215... Training loss:1.9639... 0.4508 sec/batch\n",
      "Epoch:17/20... Training Step:3216... Training loss:1.9440... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3217... Training loss:1.9584... 0.4298 sec/batch\n",
      "Epoch:17/20... Training Step:3218... Training loss:2.0036... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3219... Training loss:1.9215... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3220... Training loss:2.0022... 0.4468 sec/batch\n",
      "Epoch:17/20... Training Step:3221... Training loss:1.9516... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3222... Training loss:1.9483... 0.4528 sec/batch\n",
      "Epoch:17/20... Training Step:3223... Training loss:1.9465... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3224... Training loss:1.9755... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3225... Training loss:1.9711... 0.4548 sec/batch\n",
      "Epoch:17/20... Training Step:3226... Training loss:1.9469... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3227... Training loss:1.9369... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3228... Training loss:1.9919... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3229... Training loss:1.9505... 0.4318 sec/batch\n",
      "Epoch:17/20... Training Step:3230... Training loss:2.0016... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3231... Training loss:1.9929... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3232... Training loss:1.9653... 0.4448 sec/batch\n",
      "Epoch:17/20... Training Step:3233... Training loss:1.9492... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3234... Training loss:1.9803... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3235... Training loss:1.9700... 0.4887 sec/batch\n",
      "Epoch:17/20... Training Step:3236... Training loss:1.9315... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3237... Training loss:1.9379... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3238... Training loss:1.9544... 0.4338 sec/batch\n",
      "Epoch:17/20... Training Step:3239... Training loss:1.9918... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3240... Training loss:1.9613... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3241... Training loss:1.9859... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3242... Training loss:1.9358... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3243... Training loss:1.9579... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3244... Training loss:1.9948... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3245... Training loss:1.9598... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3246... Training loss:1.9714... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3247... Training loss:1.9271... 0.4289 sec/batch\n",
      "Epoch:17/20... Training Step:3248... Training loss:1.9484... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3249... Training loss:1.9151... 0.4558 sec/batch\n",
      "Epoch:17/20... Training Step:3250... Training loss:1.9758... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3251... Training loss:1.9198... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3252... Training loss:1.9438... 0.4319 sec/batch\n",
      "Epoch:17/20... Training Step:3253... Training loss:1.9196... 0.4328 sec/batch\n",
      "Epoch:17/20... Training Step:3254... Training loss:1.9402... 0.4359 sec/batch\n",
      "Epoch:17/20... Training Step:3255... Training loss:1.9447... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3256... Training loss:1.9381... 0.4379 sec/batch\n",
      "Epoch:17/20... Training Step:3257... Training loss:1.9124... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3258... Training loss:1.9683... 0.4658 sec/batch\n",
      "Epoch:17/20... Training Step:3259... Training loss:1.9304... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3260... Training loss:1.9463... 0.4677 sec/batch\n",
      "Epoch:17/20... Training Step:3261... Training loss:1.9159... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3262... Training loss:1.9256... 0.4548 sec/batch\n",
      "Epoch:17/20... Training Step:3263... Training loss:1.9379... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3264... Training loss:1.9454... 0.4299 sec/batch\n",
      "Epoch:17/20... Training Step:3265... Training loss:1.9462... 0.4299 sec/batch\n",
      "Epoch:17/20... Training Step:3266... Training loss:1.9344... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3267... Training loss:1.9280... 0.4349 sec/batch\n",
      "Epoch:17/20... Training Step:3268... Training loss:1.9094... 0.4318 sec/batch\n",
      "Epoch:17/20... Training Step:3269... Training loss:1.9605... 0.4518 sec/batch\n",
      "Epoch:17/20... Training Step:3270... Training loss:1.9528... 0.4338 sec/batch\n",
      "Epoch:17/20... Training Step:3271... Training loss:1.9477... 0.4278 sec/batch\n",
      "Epoch:17/20... Training Step:3272... Training loss:1.9496... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3273... Training loss:1.9279... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3274... Training loss:1.9487... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3275... Training loss:1.9397... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3276... Training loss:1.9425... 0.4538 sec/batch\n",
      "Epoch:17/20... Training Step:3277... Training loss:1.9697... 0.4339 sec/batch\n",
      "Epoch:17/20... Training Step:3278... Training loss:1.9531... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3279... Training loss:1.9386... 0.4359 sec/batch\n",
      "Epoch:17/20... Training Step:3280... Training loss:1.9393... 0.4338 sec/batch\n",
      "Epoch:17/20... Training Step:3281... Training loss:1.9487... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3282... Training loss:1.9408... 0.4358 sec/batch\n",
      "Epoch:17/20... Training Step:3283... Training loss:1.9411... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3284... Training loss:1.9137... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3285... Training loss:1.9464... 0.4318 sec/batch\n",
      "Epoch:17/20... Training Step:3286... Training loss:1.9409... 0.4349 sec/batch\n",
      "Epoch:17/20... Training Step:3287... Training loss:1.9480... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3288... Training loss:1.9382... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3289... Training loss:1.9639... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3290... Training loss:1.9231... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3291... Training loss:1.9324... 0.4358 sec/batch\n",
      "Epoch:17/20... Training Step:3292... Training loss:1.9724... 0.4478 sec/batch\n",
      "Epoch:17/20... Training Step:3293... Training loss:1.9380... 0.4259 sec/batch\n",
      "Epoch:17/20... Training Step:3294... Training loss:1.9067... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3295... Training loss:1.9628... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3296... Training loss:1.9626... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3297... Training loss:1.9476... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3298... Training loss:1.9521... 0.4379 sec/batch\n",
      "Epoch:17/20... Training Step:3299... Training loss:1.9319... 0.4448 sec/batch\n",
      "Epoch:17/20... Training Step:3300... Training loss:1.9147... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3301... Training loss:1.9617... 0.4329 sec/batch\n",
      "Epoch:17/20... Training Step:3302... Training loss:1.9537... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3303... Training loss:1.9467... 0.4448 sec/batch\n",
      "Epoch:17/20... Training Step:3304... Training loss:1.9580... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3305... Training loss:1.9544... 0.4478 sec/batch\n",
      "Epoch:17/20... Training Step:3306... Training loss:1.9553... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3307... Training loss:1.9854... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3308... Training loss:1.9379... 0.4328 sec/batch\n",
      "Epoch:17/20... Training Step:3309... Training loss:1.9732... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3310... Training loss:1.9411... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3311... Training loss:1.9430... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3312... Training loss:1.9540... 0.4289 sec/batch\n",
      "Epoch:17/20... Training Step:3313... Training loss:1.9288... 0.4308 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:17/20... Training Step:3314... Training loss:1.9670... 0.4528 sec/batch\n",
      "Epoch:17/20... Training Step:3315... Training loss:1.9621... 0.4328 sec/batch\n",
      "Epoch:17/20... Training Step:3316... Training loss:1.9651... 0.4359 sec/batch\n",
      "Epoch:17/20... Training Step:3317... Training loss:1.9589... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3318... Training loss:1.9396... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3319... Training loss:1.9326... 0.4488 sec/batch\n",
      "Epoch:17/20... Training Step:3320... Training loss:1.9815... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3321... Training loss:1.9572... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3322... Training loss:1.9625... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3323... Training loss:1.9423... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3324... Training loss:1.9453... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3325... Training loss:1.9503... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3326... Training loss:1.9422... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3327... Training loss:1.9239... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3328... Training loss:1.9830... 0.4289 sec/batch\n",
      "Epoch:17/20... Training Step:3329... Training loss:1.9802... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3330... Training loss:1.9470... 0.4348 sec/batch\n",
      "Epoch:17/20... Training Step:3331... Training loss:1.9618... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3332... Training loss:1.9576... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3333... Training loss:1.9472... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3334... Training loss:1.9456... 0.4328 sec/batch\n",
      "Epoch:17/20... Training Step:3335... Training loss:1.9615... 0.4428 sec/batch\n",
      "Epoch:17/20... Training Step:3336... Training loss:1.9881... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3337... Training loss:1.9447... 0.4388 sec/batch\n",
      "Epoch:17/20... Training Step:3338... Training loss:1.9393... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3339... Training loss:1.9257... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3340... Training loss:1.9368... 0.4398 sec/batch\n",
      "Epoch:17/20... Training Step:3341... Training loss:1.9650... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3342... Training loss:1.9650... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3343... Training loss:1.9497... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3344... Training loss:1.9555... 0.4498 sec/batch\n",
      "Epoch:17/20... Training Step:3345... Training loss:1.9449... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3346... Training loss:1.9653... 0.4508 sec/batch\n",
      "Epoch:17/20... Training Step:3347... Training loss:1.9166... 0.4308 sec/batch\n",
      "Epoch:17/20... Training Step:3348... Training loss:1.9223... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3349... Training loss:1.9248... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3350... Training loss:1.9613... 0.4369 sec/batch\n",
      "Epoch:17/20... Training Step:3351... Training loss:1.9576... 0.4438 sec/batch\n",
      "Epoch:17/20... Training Step:3352... Training loss:1.9718... 0.4418 sec/batch\n",
      "Epoch:17/20... Training Step:3353... Training loss:1.9510... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3354... Training loss:1.9391... 0.4408 sec/batch\n",
      "Epoch:17/20... Training Step:3355... Training loss:1.9431... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3356... Training loss:1.9392... 0.4368 sec/batch\n",
      "Epoch:17/20... Training Step:3357... Training loss:1.9424... 0.4328 sec/batch\n",
      "Epoch:17/20... Training Step:3358... Training loss:1.9466... 0.4468 sec/batch\n",
      "Epoch:17/20... Training Step:3359... Training loss:1.9632... 0.4378 sec/batch\n",
      "Epoch:17/20... Training Step:3360... Training loss:1.9230... 0.4329 sec/batch\n",
      "Epoch:17/20... Training Step:3361... Training loss:1.9511... 0.4478 sec/batch\n",
      "Epoch:17/20... Training Step:3362... Training loss:1.9303... 0.4369 sec/batch\n",
      "Epoch:17/20... Training Step:3363... Training loss:1.9228... 0.4338 sec/batch\n",
      "Epoch:17/20... Training Step:3364... Training loss:1.9453... 0.4458 sec/batch\n",
      "Epoch:17/20... Training Step:3365... Training loss:1.9479... 0.4498 sec/batch\n",
      "Epoch:17/20... Training Step:3366... Training loss:1.9405... 0.4538 sec/batch\n",
      "Epoch:18/20... Training Step:3367... Training loss:2.1033... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3368... Training loss:1.9207... 0.4279 sec/batch\n",
      "Epoch:18/20... Training Step:3369... Training loss:1.9261... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3370... Training loss:1.9330... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3371... Training loss:1.9273... 0.4508 sec/batch\n",
      "Epoch:18/20... Training Step:3372... Training loss:1.9047... 0.4349 sec/batch\n",
      "Epoch:18/20... Training Step:3373... Training loss:1.9474... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3374... Training loss:1.9292... 0.4498 sec/batch\n",
      "Epoch:18/20... Training Step:3375... Training loss:1.9652... 0.4308 sec/batch\n",
      "Epoch:18/20... Training Step:3376... Training loss:1.9344... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3377... Training loss:1.9179... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3378... Training loss:1.9168... 0.4538 sec/batch\n",
      "Epoch:18/20... Training Step:3379... Training loss:1.9307... 0.4309 sec/batch\n",
      "Epoch:18/20... Training Step:3380... Training loss:1.9771... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3381... Training loss:1.9248... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3382... Training loss:1.9187... 0.4299 sec/batch\n",
      "Epoch:18/20... Training Step:3383... Training loss:1.9300... 0.4349 sec/batch\n",
      "Epoch:18/20... Training Step:3384... Training loss:1.9802... 0.4369 sec/batch\n",
      "Epoch:18/20... Training Step:3385... Training loss:1.9412... 0.4548 sec/batch\n",
      "Epoch:18/20... Training Step:3386... Training loss:1.9476... 0.4488 sec/batch\n",
      "Epoch:18/20... Training Step:3387... Training loss:1.9210... 0.4309 sec/batch\n",
      "Epoch:18/20... Training Step:3388... Training loss:1.9754... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3389... Training loss:1.9450... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3390... Training loss:1.9321... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3391... Training loss:1.9326... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3392... Training loss:1.9094... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3393... Training loss:1.9185... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3394... Training loss:1.9490... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3395... Training loss:1.9657... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3396... Training loss:1.9492... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3397... Training loss:1.9413... 0.4308 sec/batch\n",
      "Epoch:18/20... Training Step:3398... Training loss:1.9105... 0.4468 sec/batch\n",
      "Epoch:18/20... Training Step:3399... Training loss:1.9389... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3400... Training loss:1.9790... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3401... Training loss:1.9216... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3402... Training loss:1.9332... 0.4478 sec/batch\n",
      "Epoch:18/20... Training Step:3403... Training loss:1.9252... 0.4488 sec/batch\n",
      "Epoch:18/20... Training Step:3404... Training loss:1.8980... 0.4458 sec/batch\n",
      "Epoch:18/20... Training Step:3405... Training loss:1.8945... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3406... Training loss:1.9056... 0.4328 sec/batch\n",
      "Epoch:18/20... Training Step:3407... Training loss:1.9131... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3408... Training loss:1.9440... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3409... Training loss:1.9148... 0.4299 sec/batch\n",
      "Epoch:18/20... Training Step:3410... Training loss:1.8921... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3411... Training loss:1.9467... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3412... Training loss:1.8799... 0.4338 sec/batch\n",
      "Epoch:18/20... Training Step:3413... Training loss:1.9433... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3414... Training loss:1.9106... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3415... Training loss:1.9242... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3416... Training loss:1.9686... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3417... Training loss:1.9001... 0.4349 sec/batch\n",
      "Epoch:18/20... Training Step:3418... Training loss:1.9835... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3419... Training loss:1.9169... 0.4418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/20... Training Step:3420... Training loss:1.9268... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3421... Training loss:1.9159... 0.4309 sec/batch\n",
      "Epoch:18/20... Training Step:3422... Training loss:1.9445... 0.4349 sec/batch\n",
      "Epoch:18/20... Training Step:3423... Training loss:1.9389... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3424... Training loss:1.9220... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3425... Training loss:1.9063... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3426... Training loss:1.9636... 0.4468 sec/batch\n",
      "Epoch:18/20... Training Step:3427... Training loss:1.9292... 0.4269 sec/batch\n",
      "Epoch:18/20... Training Step:3428... Training loss:1.9666... 0.4278 sec/batch\n",
      "Epoch:18/20... Training Step:3429... Training loss:1.9598... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3430... Training loss:1.9447... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3431... Training loss:1.9221... 0.4528 sec/batch\n",
      "Epoch:18/20... Training Step:3432... Training loss:1.9508... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3433... Training loss:1.9377... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3434... Training loss:1.9036... 0.4368 sec/batch\n",
      "Epoch:18/20... Training Step:3435... Training loss:1.9094... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3436... Training loss:1.9250... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3437... Training loss:1.9685... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3438... Training loss:1.9401... 0.4548 sec/batch\n",
      "Epoch:18/20... Training Step:3439... Training loss:1.9635... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3440... Training loss:1.9194... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3441... Training loss:1.9359... 0.4478 sec/batch\n",
      "Epoch:18/20... Training Step:3442... Training loss:1.9656... 0.4359 sec/batch\n",
      "Epoch:18/20... Training Step:3443... Training loss:1.9353... 0.4308 sec/batch\n",
      "Epoch:18/20... Training Step:3444... Training loss:1.9332... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3445... Training loss:1.8979... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3446... Training loss:1.9205... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3447... Training loss:1.9043... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3448... Training loss:1.9400... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3449... Training loss:1.8860... 0.4329 sec/batch\n",
      "Epoch:18/20... Training Step:3450... Training loss:1.9239... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3451... Training loss:1.8827... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3452... Training loss:1.9208... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3453... Training loss:1.9272... 0.4379 sec/batch\n",
      "Epoch:18/20... Training Step:3454... Training loss:1.9021... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3455... Training loss:1.8986... 0.4359 sec/batch\n",
      "Epoch:18/20... Training Step:3456... Training loss:1.9334... 0.4528 sec/batch\n",
      "Epoch:18/20... Training Step:3457... Training loss:1.9024... 0.4258 sec/batch\n",
      "Epoch:18/20... Training Step:3458... Training loss:1.9163... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3459... Training loss:1.8929... 0.4379 sec/batch\n",
      "Epoch:18/20... Training Step:3460... Training loss:1.9067... 0.4308 sec/batch\n",
      "Epoch:18/20... Training Step:3461... Training loss:1.9028... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3462... Training loss:1.9144... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3463... Training loss:1.9098... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3464... Training loss:1.8997... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3465... Training loss:1.9006... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3466... Training loss:1.8808... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3467... Training loss:1.9292... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3468... Training loss:1.9247... 0.4309 sec/batch\n",
      "Epoch:18/20... Training Step:3469... Training loss:1.9078... 0.4309 sec/batch\n",
      "Epoch:18/20... Training Step:3470... Training loss:1.9146... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3471... Training loss:1.9096... 0.4338 sec/batch\n",
      "Epoch:18/20... Training Step:3472... Training loss:1.9108... 0.4468 sec/batch\n",
      "Epoch:18/20... Training Step:3473... Training loss:1.9174... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3474... Training loss:1.9180... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3475... Training loss:1.9409... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3476... Training loss:1.9275... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3477... Training loss:1.9257... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3478... Training loss:1.9130... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3479... Training loss:1.9204... 0.4468 sec/batch\n",
      "Epoch:18/20... Training Step:3480... Training loss:1.9152... 0.4299 sec/batch\n",
      "Epoch:18/20... Training Step:3481... Training loss:1.9034... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3482... Training loss:1.8786... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3483... Training loss:1.9269... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3484... Training loss:1.9152... 0.4288 sec/batch\n",
      "Epoch:18/20... Training Step:3485... Training loss:1.9219... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3486... Training loss:1.9082... 0.4498 sec/batch\n",
      "Epoch:18/20... Training Step:3487... Training loss:1.9328... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3488... Training loss:1.9016... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3489... Training loss:1.8959... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3490... Training loss:1.9507... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3491... Training loss:1.9175... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3492... Training loss:1.8673... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3493... Training loss:1.9458... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3494... Training loss:1.9338... 0.4279 sec/batch\n",
      "Epoch:18/20... Training Step:3495... Training loss:1.9197... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3496... Training loss:1.9266... 0.4359 sec/batch\n",
      "Epoch:18/20... Training Step:3497... Training loss:1.8970... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3498... Training loss:1.8875... 0.4478 sec/batch\n",
      "Epoch:18/20... Training Step:3499... Training loss:1.9413... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3500... Training loss:1.9302... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3501... Training loss:1.9308... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3502... Training loss:1.9411... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3503... Training loss:1.9378... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3504... Training loss:1.9299... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3505... Training loss:1.9665... 0.4368 sec/batch\n",
      "Epoch:18/20... Training Step:3506... Training loss:1.9187... 0.4478 sec/batch\n",
      "Epoch:18/20... Training Step:3507... Training loss:1.9487... 0.4438 sec/batch\n",
      "Epoch:18/20... Training Step:3508... Training loss:1.9139... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3509... Training loss:1.9182... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3510... Training loss:1.9208... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3511... Training loss:1.9039... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3512... Training loss:1.9295... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3513... Training loss:1.9353... 0.4468 sec/batch\n",
      "Epoch:18/20... Training Step:3514... Training loss:1.9515... 0.4298 sec/batch\n",
      "Epoch:18/20... Training Step:3515... Training loss:1.9263... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3516... Training loss:1.9107... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3517... Training loss:1.9122... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3518... Training loss:1.9556... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3519... Training loss:1.9315... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3520... Training loss:1.9331... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3521... Training loss:1.9066... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3522... Training loss:1.9125... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3523... Training loss:1.9239... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3524... Training loss:1.9250... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3525... Training loss:1.8914... 0.4309 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:18/20... Training Step:3526... Training loss:1.9609... 0.4269 sec/batch\n",
      "Epoch:18/20... Training Step:3527... Training loss:1.9488... 0.4358 sec/batch\n",
      "Epoch:18/20... Training Step:3528... Training loss:1.9188... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3529... Training loss:1.9346... 0.4458 sec/batch\n",
      "Epoch:18/20... Training Step:3530... Training loss:1.9288... 0.4289 sec/batch\n",
      "Epoch:18/20... Training Step:3531... Training loss:1.9263... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3532... Training loss:1.9128... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3533... Training loss:1.9254... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3534... Training loss:1.9629... 0.4368 sec/batch\n",
      "Epoch:18/20... Training Step:3535... Training loss:1.9222... 0.4369 sec/batch\n",
      "Epoch:18/20... Training Step:3536... Training loss:1.9186... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3537... Training loss:1.9015... 0.4478 sec/batch\n",
      "Epoch:18/20... Training Step:3538... Training loss:1.9085... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3539... Training loss:1.9443... 0.4289 sec/batch\n",
      "Epoch:18/20... Training Step:3540... Training loss:1.9350... 0.4528 sec/batch\n",
      "Epoch:18/20... Training Step:3541... Training loss:1.9322... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3542... Training loss:1.9258... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3543... Training loss:1.9102... 0.4398 sec/batch\n",
      "Epoch:18/20... Training Step:3544... Training loss:1.9300... 0.4488 sec/batch\n",
      "Epoch:18/20... Training Step:3545... Training loss:1.8963... 0.4448 sec/batch\n",
      "Epoch:18/20... Training Step:3546... Training loss:1.9003... 0.4369 sec/batch\n",
      "Epoch:18/20... Training Step:3547... Training loss:1.9050... 0.4508 sec/batch\n",
      "Epoch:18/20... Training Step:3548... Training loss:1.9367... 0.4299 sec/batch\n",
      "Epoch:18/20... Training Step:3549... Training loss:1.9195... 0.4328 sec/batch\n",
      "Epoch:18/20... Training Step:3550... Training loss:1.9447... 0.4428 sec/batch\n",
      "Epoch:18/20... Training Step:3551... Training loss:1.9301... 0.4368 sec/batch\n",
      "Epoch:18/20... Training Step:3552... Training loss:1.9064... 0.4418 sec/batch\n",
      "Epoch:18/20... Training Step:3553... Training loss:1.9114... 0.4349 sec/batch\n",
      "Epoch:18/20... Training Step:3554... Training loss:1.8978... 0.4388 sec/batch\n",
      "Epoch:18/20... Training Step:3555... Training loss:1.9048... 0.4508 sec/batch\n",
      "Epoch:18/20... Training Step:3556... Training loss:1.9185... 0.4298 sec/batch\n",
      "Epoch:18/20... Training Step:3557... Training loss:1.9367... 0.4348 sec/batch\n",
      "Epoch:18/20... Training Step:3558... Training loss:1.8963... 0.4318 sec/batch\n",
      "Epoch:18/20... Training Step:3559... Training loss:1.9243... 0.4359 sec/batch\n",
      "Epoch:18/20... Training Step:3560... Training loss:1.9006... 0.4408 sec/batch\n",
      "Epoch:18/20... Training Step:3561... Training loss:1.8981... 0.4578 sec/batch\n",
      "Epoch:18/20... Training Step:3562... Training loss:1.9172... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3563... Training loss:1.9126... 0.4378 sec/batch\n",
      "Epoch:18/20... Training Step:3564... Training loss:1.9004... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3565... Training loss:2.0683... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3566... Training loss:1.8991... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3567... Training loss:1.9022... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3568... Training loss:1.9057... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3569... Training loss:1.9062... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3570... Training loss:1.8785... 0.4428 sec/batch\n",
      "Epoch:19/20... Training Step:3571... Training loss:1.9183... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3572... Training loss:1.9066... 0.4278 sec/batch\n",
      "Epoch:19/20... Training Step:3573... Training loss:1.9398... 0.4328 sec/batch\n",
      "Epoch:19/20... Training Step:3574... Training loss:1.9040... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3575... Training loss:1.9004... 0.4508 sec/batch\n",
      "Epoch:19/20... Training Step:3576... Training loss:1.8939... 0.4269 sec/batch\n",
      "Epoch:19/20... Training Step:3577... Training loss:1.9109... 0.4339 sec/batch\n",
      "Epoch:19/20... Training Step:3578... Training loss:1.9476... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3579... Training loss:1.9012... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3580... Training loss:1.8982... 0.4897 sec/batch\n",
      "Epoch:19/20... Training Step:3581... Training loss:1.9057... 0.4538 sec/batch\n",
      "Epoch:19/20... Training Step:3582... Training loss:1.9597... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3583... Training loss:1.9221... 0.4349 sec/batch\n",
      "Epoch:19/20... Training Step:3584... Training loss:1.9284... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3585... Training loss:1.8983... 0.4308 sec/batch\n",
      "Epoch:19/20... Training Step:3586... Training loss:1.9397... 0.4289 sec/batch\n",
      "Epoch:19/20... Training Step:3587... Training loss:1.9156... 0.4349 sec/batch\n",
      "Epoch:19/20... Training Step:3588... Training loss:1.9036... 0.4518 sec/batch\n",
      "Epoch:19/20... Training Step:3589... Training loss:1.9074... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3590... Training loss:1.8907... 0.4328 sec/batch\n",
      "Epoch:19/20... Training Step:3591... Training loss:1.8908... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3592... Training loss:1.9211... 0.4428 sec/batch\n",
      "Epoch:19/20... Training Step:3593... Training loss:1.9445... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3594... Training loss:1.9302... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3595... Training loss:1.9217... 0.4478 sec/batch\n",
      "Epoch:19/20... Training Step:3596... Training loss:1.8890... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3597... Training loss:1.9227... 0.4428 sec/batch\n",
      "Epoch:19/20... Training Step:3598... Training loss:1.9500... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3599... Training loss:1.9016... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3600... Training loss:1.9077... 0.4508 sec/batch\n",
      "Epoch:19/20... Training Step:3601... Training loss:1.9089... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3602... Training loss:1.8707... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3603... Training loss:1.8712... 0.4598 sec/batch\n",
      "Epoch:19/20... Training Step:3604... Training loss:1.8827... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3605... Training loss:1.8911... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3606... Training loss:1.9132... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3607... Training loss:1.8903... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3608... Training loss:1.8695... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3609... Training loss:1.9199... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3610... Training loss:1.8513... 0.4578 sec/batch\n",
      "Epoch:19/20... Training Step:3611... Training loss:1.9059... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3612... Training loss:1.8898... 0.4299 sec/batch\n",
      "Epoch:19/20... Training Step:3613... Training loss:1.8940... 0.4598 sec/batch\n",
      "Epoch:19/20... Training Step:3614... Training loss:1.9474... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3615... Training loss:1.8727... 0.4598 sec/batch\n",
      "Epoch:19/20... Training Step:3616... Training loss:1.9479... 0.4488 sec/batch\n",
      "Epoch:19/20... Training Step:3617... Training loss:1.9062... 0.4338 sec/batch\n",
      "Epoch:19/20... Training Step:3618... Training loss:1.8951... 0.4319 sec/batch\n",
      "Epoch:19/20... Training Step:3619... Training loss:1.8892... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3620... Training loss:1.9146... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3621... Training loss:1.9220... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3622... Training loss:1.9011... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3623... Training loss:1.8859... 0.4328 sec/batch\n",
      "Epoch:19/20... Training Step:3624... Training loss:1.9375... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3625... Training loss:1.8917... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3626... Training loss:1.9444... 0.4318 sec/batch\n",
      "Epoch:19/20... Training Step:3627... Training loss:1.9322... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3628... Training loss:1.9121... 0.4299 sec/batch\n",
      "Epoch:19/20... Training Step:3629... Training loss:1.8971... 0.4349 sec/batch\n",
      "Epoch:19/20... Training Step:3630... Training loss:1.9223... 0.4329 sec/batch\n",
      "Epoch:19/20... Training Step:3631... Training loss:1.9230... 0.4349 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/20... Training Step:3632... Training loss:1.8741... 0.4318 sec/batch\n",
      "Epoch:19/20... Training Step:3633... Training loss:1.8880... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3634... Training loss:1.9000... 0.4329 sec/batch\n",
      "Epoch:19/20... Training Step:3635... Training loss:1.9399... 0.4299 sec/batch\n",
      "Epoch:19/20... Training Step:3636... Training loss:1.9114... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3637... Training loss:1.9243... 0.4428 sec/batch\n",
      "Epoch:19/20... Training Step:3638... Training loss:1.8907... 0.4508 sec/batch\n",
      "Epoch:19/20... Training Step:3639... Training loss:1.8997... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3640... Training loss:1.9422... 0.4488 sec/batch\n",
      "Epoch:19/20... Training Step:3641... Training loss:1.9104... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3642... Training loss:1.9030... 0.4369 sec/batch\n",
      "Epoch:19/20... Training Step:3643... Training loss:1.8665... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3644... Training loss:1.8887... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3645... Training loss:1.8756... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3646... Training loss:1.9190... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3647... Training loss:1.8725... 0.4478 sec/batch\n",
      "Epoch:19/20... Training Step:3648... Training loss:1.9097... 0.4308 sec/batch\n",
      "Epoch:19/20... Training Step:3649... Training loss:1.8626... 0.4298 sec/batch\n",
      "Epoch:19/20... Training Step:3650... Training loss:1.8893... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3651... Training loss:1.8916... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3652... Training loss:1.8724... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3653... Training loss:1.8630... 0.4278 sec/batch\n",
      "Epoch:19/20... Training Step:3654... Training loss:1.9189... 0.4538 sec/batch\n",
      "Epoch:19/20... Training Step:3655... Training loss:1.8788... 0.4468 sec/batch\n",
      "Epoch:19/20... Training Step:3656... Training loss:1.8919... 0.4299 sec/batch\n",
      "Epoch:19/20... Training Step:3657... Training loss:1.8705... 0.4498 sec/batch\n",
      "Epoch:19/20... Training Step:3658... Training loss:1.8745... 0.4269 sec/batch\n",
      "Epoch:19/20... Training Step:3659... Training loss:1.8790... 0.4308 sec/batch\n",
      "Epoch:19/20... Training Step:3660... Training loss:1.9028... 0.4289 sec/batch\n",
      "Epoch:19/20... Training Step:3661... Training loss:1.8969... 0.4548 sec/batch\n",
      "Epoch:19/20... Training Step:3662... Training loss:1.8721... 0.4468 sec/batch\n",
      "Epoch:19/20... Training Step:3663... Training loss:1.8763... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3664... Training loss:1.8603... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3665... Training loss:1.9079... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3666... Training loss:1.8961... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3667... Training loss:1.8825... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3668... Training loss:1.8885... 0.4518 sec/batch\n",
      "Epoch:19/20... Training Step:3669... Training loss:1.8749... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3670... Training loss:1.8863... 0.4369 sec/batch\n",
      "Epoch:19/20... Training Step:3671... Training loss:1.8881... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3672... Training loss:1.9011... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3673... Training loss:1.9126... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3674... Training loss:1.9065... 0.4329 sec/batch\n",
      "Epoch:19/20... Training Step:3675... Training loss:1.8814... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3676... Training loss:1.8846... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3677... Training loss:1.8950... 0.4478 sec/batch\n",
      "Epoch:19/20... Training Step:3678... Training loss:1.8874... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3679... Training loss:1.8762... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3680... Training loss:1.8551... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3681... Training loss:1.9015... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3682... Training loss:1.8870... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3683... Training loss:1.8999... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3684... Training loss:1.8859... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3685... Training loss:1.9102... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3686... Training loss:1.8673... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3687... Training loss:1.8704... 0.4478 sec/batch\n",
      "Epoch:19/20... Training Step:3688... Training loss:1.9116... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3689... Training loss:1.8883... 0.4518 sec/batch\n",
      "Epoch:19/20... Training Step:3690... Training loss:1.8536... 0.4318 sec/batch\n",
      "Epoch:19/20... Training Step:3691... Training loss:1.9135... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3692... Training loss:1.9135... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3693... Training loss:1.8924... 0.4438 sec/batch\n",
      "Epoch:19/20... Training Step:3694... Training loss:1.9014... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3695... Training loss:1.8642... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3696... Training loss:1.8728... 0.4298 sec/batch\n",
      "Epoch:19/20... Training Step:3697... Training loss:1.9118... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3698... Training loss:1.9007... 0.4328 sec/batch\n",
      "Epoch:19/20... Training Step:3699... Training loss:1.9009... 0.4308 sec/batch\n",
      "Epoch:19/20... Training Step:3700... Training loss:1.9143... 0.4318 sec/batch\n",
      "Epoch:19/20... Training Step:3701... Training loss:1.9138... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3702... Training loss:1.9111... 0.4458 sec/batch\n",
      "Epoch:19/20... Training Step:3703... Training loss:1.9347... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3704... Training loss:1.8832... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3705... Training loss:1.9196... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3706... Training loss:1.8864... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3707... Training loss:1.8992... 0.4448 sec/batch\n",
      "Epoch:19/20... Training Step:3708... Training loss:1.9029... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3709... Training loss:1.8868... 0.4508 sec/batch\n",
      "Epoch:19/20... Training Step:3710... Training loss:1.9151... 0.4308 sec/batch\n",
      "Epoch:19/20... Training Step:3711... Training loss:1.9160... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3712... Training loss:1.9389... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3713... Training loss:1.9120... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3714... Training loss:1.8942... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3715... Training loss:1.8773... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3716... Training loss:1.9307... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3717... Training loss:1.9045... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3718... Training loss:1.9121... 0.4348 sec/batch\n",
      "Epoch:19/20... Training Step:3719... Training loss:1.8952... 0.4518 sec/batch\n",
      "Epoch:19/20... Training Step:3720... Training loss:1.8858... 0.4349 sec/batch\n",
      "Epoch:19/20... Training Step:3721... Training loss:1.9114... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3722... Training loss:1.9155... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3723... Training loss:1.8663... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3724... Training loss:1.9298... 0.4458 sec/batch\n",
      "Epoch:19/20... Training Step:3725... Training loss:1.9269... 0.4278 sec/batch\n",
      "Epoch:19/20... Training Step:3726... Training loss:1.9040... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3727... Training loss:1.9182... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3728... Training loss:1.9061... 0.4318 sec/batch\n",
      "Epoch:19/20... Training Step:3729... Training loss:1.9090... 0.4498 sec/batch\n",
      "Epoch:19/20... Training Step:3730... Training loss:1.8858... 0.4288 sec/batch\n",
      "Epoch:19/20... Training Step:3731... Training loss:1.9077... 0.4388 sec/batch\n",
      "Epoch:19/20... Training Step:3732... Training loss:1.9552... 0.4328 sec/batch\n",
      "Epoch:19/20... Training Step:3733... Training loss:1.9035... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3734... Training loss:1.8994... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3735... Training loss:1.8739... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3736... Training loss:1.8839... 0.4458 sec/batch\n",
      "Epoch:19/20... Training Step:3737... Training loss:1.9232... 0.4319 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:19/20... Training Step:3738... Training loss:1.9003... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3739... Training loss:1.9078... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3740... Training loss:1.9100... 0.4468 sec/batch\n",
      "Epoch:19/20... Training Step:3741... Training loss:1.8879... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3742... Training loss:1.9171... 0.4418 sec/batch\n",
      "Epoch:19/20... Training Step:3743... Training loss:1.8723... 0.4488 sec/batch\n",
      "Epoch:19/20... Training Step:3744... Training loss:1.8727... 0.4528 sec/batch\n",
      "Epoch:19/20... Training Step:3745... Training loss:1.8746... 0.4398 sec/batch\n",
      "Epoch:19/20... Training Step:3746... Training loss:1.9040... 0.4518 sec/batch\n",
      "Epoch:19/20... Training Step:3747... Training loss:1.9019... 0.4498 sec/batch\n",
      "Epoch:19/20... Training Step:3748... Training loss:1.9230... 0.4359 sec/batch\n",
      "Epoch:19/20... Training Step:3749... Training loss:1.9031... 0.4458 sec/batch\n",
      "Epoch:19/20... Training Step:3750... Training loss:1.8765... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3751... Training loss:1.8848... 0.4478 sec/batch\n",
      "Epoch:19/20... Training Step:3752... Training loss:1.8716... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3753... Training loss:1.8893... 0.4378 sec/batch\n",
      "Epoch:19/20... Training Step:3754... Training loss:1.8947... 0.4379 sec/batch\n",
      "Epoch:19/20... Training Step:3755... Training loss:1.9075... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3756... Training loss:1.8656... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3757... Training loss:1.8952... 0.4408 sec/batch\n",
      "Epoch:19/20... Training Step:3758... Training loss:1.8707... 0.4369 sec/batch\n",
      "Epoch:19/20... Training Step:3759... Training loss:1.8752... 0.4358 sec/batch\n",
      "Epoch:19/20... Training Step:3760... Training loss:1.8926... 0.4428 sec/batch\n",
      "Epoch:19/20... Training Step:3761... Training loss:1.8955... 0.4368 sec/batch\n",
      "Epoch:19/20... Training Step:3762... Training loss:1.8740... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3763... Training loss:2.0481... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3764... Training loss:1.8743... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3765... Training loss:1.8795... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3766... Training loss:1.8864... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3767... Training loss:1.8895... 0.4458 sec/batch\n",
      "Epoch:20/20... Training Step:3768... Training loss:1.8470... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3769... Training loss:1.8971... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3770... Training loss:1.8786... 0.4498 sec/batch\n",
      "Epoch:20/20... Training Step:3771... Training loss:1.9219... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3772... Training loss:1.8849... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3773... Training loss:1.8754... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3774... Training loss:1.8744... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3775... Training loss:1.8850... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3776... Training loss:1.9154... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3777... Training loss:1.8727... 0.4548 sec/batch\n",
      "Epoch:20/20... Training Step:3778... Training loss:1.8743... 0.4308 sec/batch\n",
      "Epoch:20/20... Training Step:3779... Training loss:1.8785... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3780... Training loss:1.9270... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3781... Training loss:1.8886... 0.4428 sec/batch\n",
      "Epoch:20/20... Training Step:3782... Training loss:1.8976... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3783... Training loss:1.8793... 0.4508 sec/batch\n",
      "Epoch:20/20... Training Step:3784... Training loss:1.9134... 0.4498 sec/batch\n",
      "Epoch:20/20... Training Step:3785... Training loss:1.8853... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3786... Training loss:1.8838... 0.4488 sec/batch\n",
      "Epoch:20/20... Training Step:3787... Training loss:1.8910... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3788... Training loss:1.8654... 0.4498 sec/batch\n",
      "Epoch:20/20... Training Step:3789... Training loss:1.8651... 0.4279 sec/batch\n",
      "Epoch:20/20... Training Step:3790... Training loss:1.8917... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3791... Training loss:1.9201... 0.4558 sec/batch\n",
      "Epoch:20/20... Training Step:3792... Training loss:1.8950... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3793... Training loss:1.8824... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3794... Training loss:1.8530... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3795... Training loss:1.8939... 0.4458 sec/batch\n",
      "Epoch:20/20... Training Step:3796... Training loss:1.9058... 0.4498 sec/batch\n",
      "Epoch:20/20... Training Step:3797... Training loss:1.8773... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3798... Training loss:1.8830... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3799... Training loss:1.8734... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3800... Training loss:1.8412... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3801... Training loss:1.8408... 0.4598 sec/batch\n",
      "Epoch:20/20... Training Step:3802... Training loss:1.8578... 0.4518 sec/batch\n",
      "Epoch:20/20... Training Step:3803... Training loss:1.8609... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3804... Training loss:1.8795... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3805... Training loss:1.8654... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3806... Training loss:1.8585... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3807... Training loss:1.9009... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3808... Training loss:1.8303... 0.4268 sec/batch\n",
      "Epoch:20/20... Training Step:3809... Training loss:1.8869... 0.4508 sec/batch\n",
      "Epoch:20/20... Training Step:3810... Training loss:1.8713... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3811... Training loss:1.8795... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3812... Training loss:1.9219... 0.4318 sec/batch\n",
      "Epoch:20/20... Training Step:3813... Training loss:1.8506... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3814... Training loss:1.9422... 0.4318 sec/batch\n",
      "Epoch:20/20... Training Step:3815... Training loss:1.8695... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3816... Training loss:1.8770... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3817... Training loss:1.8708... 0.4349 sec/batch\n",
      "Epoch:20/20... Training Step:3818... Training loss:1.8911... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3819... Training loss:1.9046... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3820... Training loss:1.8749... 0.4359 sec/batch\n",
      "Epoch:20/20... Training Step:3821... Training loss:1.8629... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3822... Training loss:1.9216... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3823... Training loss:1.8922... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3824... Training loss:1.9236... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3825... Training loss:1.9114... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3826... Training loss:1.8921... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3827... Training loss:1.8689... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3828... Training loss:1.9040... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3829... Training loss:1.8981... 0.4309 sec/batch\n",
      "Epoch:20/20... Training Step:3830... Training loss:1.8529... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3831... Training loss:1.8745... 0.4518 sec/batch\n",
      "Epoch:20/20... Training Step:3832... Training loss:1.8751... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3833... Training loss:1.9240... 0.4308 sec/batch\n",
      "Epoch:20/20... Training Step:3834... Training loss:1.8915... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3835... Training loss:1.9113... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3836... Training loss:1.8728... 0.4538 sec/batch\n",
      "Epoch:20/20... Training Step:3837... Training loss:1.8780... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3838... Training loss:1.9182... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3839... Training loss:1.8877... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3840... Training loss:1.8877... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3841... Training loss:1.8551... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3842... Training loss:1.8677... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3843... Training loss:1.8336... 0.4418 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/20... Training Step:3844... Training loss:1.8940... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3845... Training loss:1.8445... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3846... Training loss:1.8706... 0.4319 sec/batch\n",
      "Epoch:20/20... Training Step:3847... Training loss:1.8388... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3848... Training loss:1.8625... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3849... Training loss:1.8683... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3850... Training loss:1.8454... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3851... Training loss:1.8385... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3852... Training loss:1.8884... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3853... Training loss:1.8537... 0.4318 sec/batch\n",
      "Epoch:20/20... Training Step:3854... Training loss:1.8718... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3855... Training loss:1.8361... 0.4289 sec/batch\n",
      "Epoch:20/20... Training Step:3856... Training loss:1.8648... 0.4428 sec/batch\n",
      "Epoch:20/20... Training Step:3857... Training loss:1.8470... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3858... Training loss:1.8707... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3859... Training loss:1.8650... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3860... Training loss:1.8512... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3861... Training loss:1.8532... 0.4458 sec/batch\n",
      "Epoch:20/20... Training Step:3862... Training loss:1.8252... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3863... Training loss:1.8755... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3864... Training loss:1.8739... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3865... Training loss:1.8534... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3866... Training loss:1.8652... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3867... Training loss:1.8552... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3868... Training loss:1.8698... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3869... Training loss:1.8673... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3870... Training loss:1.8648... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3871... Training loss:1.8911... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3872... Training loss:1.8792... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3873... Training loss:1.8690... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3874... Training loss:1.8592... 0.4349 sec/batch\n",
      "Epoch:20/20... Training Step:3875... Training loss:1.8651... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3876... Training loss:1.8708... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3877... Training loss:1.8543... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3878... Training loss:1.8296... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3879... Training loss:1.8767... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3880... Training loss:1.8689... 0.4308 sec/batch\n",
      "Epoch:20/20... Training Step:3881... Training loss:1.8654... 0.4518 sec/batch\n",
      "Epoch:20/20... Training Step:3882... Training loss:1.8704... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3883... Training loss:1.8755... 0.4498 sec/batch\n",
      "Epoch:20/20... Training Step:3884... Training loss:1.8416... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3885... Training loss:1.8497... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3886... Training loss:1.9019... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3887... Training loss:1.8636... 0.4308 sec/batch\n",
      "Epoch:20/20... Training Step:3888... Training loss:1.8235... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3889... Training loss:1.8893... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3890... Training loss:1.8935... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3891... Training loss:1.8656... 0.4488 sec/batch\n",
      "Epoch:20/20... Training Step:3892... Training loss:1.8727... 0.4298 sec/batch\n",
      "Epoch:20/20... Training Step:3893... Training loss:1.8417... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3894... Training loss:1.8367... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3895... Training loss:1.8852... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3896... Training loss:1.8797... 0.4289 sec/batch\n",
      "Epoch:20/20... Training Step:3897... Training loss:1.8698... 0.4428 sec/batch\n",
      "Epoch:20/20... Training Step:3898... Training loss:1.8748... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3899... Training loss:1.8769... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3900... Training loss:1.8769... 0.4319 sec/batch\n",
      "Epoch:20/20... Training Step:3901... Training loss:1.9065... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3902... Training loss:1.8613... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3903... Training loss:1.8971... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3904... Training loss:1.8585... 0.4458 sec/batch\n",
      "Epoch:20/20... Training Step:3905... Training loss:1.8712... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3906... Training loss:1.8680... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3907... Training loss:1.8569... 0.4298 sec/batch\n",
      "Epoch:20/20... Training Step:3908... Training loss:1.8786... 0.4349 sec/batch\n",
      "Epoch:20/20... Training Step:3909... Training loss:1.8825... 0.4269 sec/batch\n",
      "Epoch:20/20... Training Step:3910... Training loss:1.8970... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3911... Training loss:1.8809... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3912... Training loss:1.8696... 0.4398 sec/batch\n",
      "Epoch:20/20... Training Step:3913... Training loss:1.8561... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3914... Training loss:1.8983... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3915... Training loss:1.8843... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3916... Training loss:1.8905... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3917... Training loss:1.8626... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3918... Training loss:1.8617... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3919... Training loss:1.8739... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3920... Training loss:1.8683... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3921... Training loss:1.8436... 0.4478 sec/batch\n",
      "Epoch:20/20... Training Step:3922... Training loss:1.8943... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3923... Training loss:1.8980... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3924... Training loss:1.8679... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3925... Training loss:1.8909... 0.4628 sec/batch\n",
      "Epoch:20/20... Training Step:3926... Training loss:1.8841... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3927... Training loss:1.8775... 0.4368 sec/batch\n",
      "Epoch:20/20... Training Step:3928... Training loss:1.8635... 0.4369 sec/batch\n",
      "Epoch:20/20... Training Step:3929... Training loss:1.8902... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3930... Training loss:1.9257... 0.4438 sec/batch\n",
      "Epoch:20/20... Training Step:3931... Training loss:1.8772... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3932... Training loss:1.8618... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3933... Training loss:1.8532... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3934... Training loss:1.8537... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3935... Training loss:1.9007... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3936... Training loss:1.8848... 0.4309 sec/batch\n",
      "Epoch:20/20... Training Step:3937... Training loss:1.8850... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3938... Training loss:1.8786... 0.4418 sec/batch\n",
      "Epoch:20/20... Training Step:3939... Training loss:1.8656... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3940... Training loss:1.8897... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3941... Training loss:1.8451... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3942... Training loss:1.8367... 0.4358 sec/batch\n",
      "Epoch:20/20... Training Step:3943... Training loss:1.8561... 0.4408 sec/batch\n",
      "Epoch:20/20... Training Step:3944... Training loss:1.8764... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3945... Training loss:1.8737... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3946... Training loss:1.8909... 0.4448 sec/batch\n",
      "Epoch:20/20... Training Step:3947... Training loss:1.8665... 0.4428 sec/batch\n",
      "Epoch:20/20... Training Step:3948... Training loss:1.8592... 0.4289 sec/batch\n",
      "Epoch:20/20... Training Step:3949... Training loss:1.8561... 0.4289 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:20/20... Training Step:3950... Training loss:1.8547... 0.4488 sec/batch\n",
      "Epoch:20/20... Training Step:3951... Training loss:1.8625... 0.4328 sec/batch\n",
      "Epoch:20/20... Training Step:3952... Training loss:1.8632... 0.4428 sec/batch\n",
      "Epoch:20/20... Training Step:3953... Training loss:1.8789... 0.4388 sec/batch\n",
      "Epoch:20/20... Training Step:3954... Training loss:1.8453... 0.4348 sec/batch\n",
      "Epoch:20/20... Training Step:3955... Training loss:1.8742... 0.4468 sec/batch\n",
      "Epoch:20/20... Training Step:3956... Training loss:1.8431... 0.4378 sec/batch\n",
      "Epoch:20/20... Training Step:3957... Training loss:1.8517... 0.4299 sec/batch\n",
      "Epoch:20/20... Training Step:3958... Training loss:1.8744... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3959... Training loss:1.8656... 0.4338 sec/batch\n",
      "Epoch:20/20... Training Step:3960... Training loss:1.8703... 0.4388 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# 每200步保存一个checkpoint\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps, \n",
    "               lstm_size=lstm_size, num_layers=num_layers, \n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)#  the maximum number of recent checkpoint files to keep.  As new files are created, older files are deleted.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss,\n",
    "                                                 model.final_state,\n",
    "                                                 model.optimizer],\n",
    "                                                 feed_dict=feed)\n",
    "            end = time.time()\n",
    "            print('Epoch:{}/{}...'.format(e+1, epochs),\n",
    "                  'Training Step:{}...'.format(counter),\n",
    "                  'Training loss:{:.4f}...'.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, 'checkpoints/i{}.ckpt'.format(counter))\n",
    "\n",
    "    saver.save(sess, 'checkpoints/i{}.ckpt'.format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3960.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3600.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3800.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3960.ckpt\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]]=0\n",
    "    p = p / np.sum(p)\n",
    "    char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime='The '):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction,\n",
    "                                         model.final_state],\n",
    "                                         feed_dict=feed)\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                x[0,0] = c\n",
    "                feed = {model.inputs: x,\n",
    "                        model.keep_prob: 1.,\n",
    "                        model.initial_state: new_state}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "                c = pick_top_n(preds, len(vocab))\n",
    "                samples.append(int_to_vocab[c])\n",
    "\n",
    "    return ''.join(samples)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3960.ckpt\n",
      "Theelexd, \"so dered in the read.\n",
      "\n",
      "\"That's mo be worling,\" thing the samaning of the ceating had the carre\n",
      "tarking a thate hord foret all his wathed and to though, andwerind his\n",
      "higenes to showed. They is as impentions were her the sand that to dean\n",
      "hould to bece he his and at try wing, and shild neathing, and him not\n",
      "she said to to the steress.\"\n",
      "\n",
      "\"Ahan I shale in the roomen theme't that?.\"\n",
      "\n",
      "\"Now, Ivin't anly to the seare of. I was santing that,\" she said a mint\n",
      "thit wist were and ham bathir hes ant hid homese, and and what to shy\n",
      "seeting and has seale an to bechines. But he her that has hissenfed the\n",
      "sorter the soncesting, and the sungers it he cented thit hes sone and she\n",
      "what had he did not so le to his woth and ant selfed and tomented, and the\n",
      "would and all at one hin and himsenfe and with instechiog, and her hor\n",
      "sampation with whome he hed soild to her she said, and and he hed a\n",
      "tone a dowing. And he tanded herserf, his wane and sert an that he cousd\n",
      "her to ster at to be shish thought whet she sead on the ster the shows.\n",
      "\n",
      "She had been they alling the searter on him that the saress firlowe her\n",
      "hand hisherf forting of with his all stroug to the fealted of the comeninco\n",
      "her began and tread the sent out onse to har his and her triline on\n",
      "the serpaning of the cant anow at a to though to how. Seexel Alknayevitch\n",
      "a sunting.\n",
      "\n",
      "She wert trough her ther ataraning the more to thoughe. \n",
      "Alexand Alanovov him his suplition. He was dorne weald at in a lowner, who\n",
      "had shidern histly. To than he hed ther, all that he wound hevend his\n",
      "alother, and sele that he had but oncon is the parstirnss, at hig to\n",
      "her to say if her hered to he went with a mantes.\"\n",
      "\n",
      "\"As at in it a troul the rust and wist the sees of the mass ware ofe thas\n",
      "and wish the porsten with the rasing tratk alons and he west of in the\n",
      "seally,, she wan so deling of she heald bat in of the risters, was\n",
      "sonedsing at the poiss of that's bulle a the some of to her, she wis\n",
      "ound to the browe,\" he said, himpllady and with har had er, he coused\n",
      "tint tom time woth him, trought his, and as a would, and she her seed\n",
      "not has he ceners of the menere wase to sting then has were sald and\n",
      "he have to ter a mis would been while at he doon to so the mater.\n",
      "\n",
      "\"Wall, at aton a same ane her as the cold thene wo doon to he heas stat the\n",
      "sardione of a light as if alo seaning on her able and himsent.\"\n",
      "\"\n",
      "hum he had not to the baring, and the said wat her tile som it,\" and\n",
      "was as him, what he sould the canting her thomenter ham a said to\n",
      "andoss woored and to the mertare, him and hid ant they and a chatrow\n",
      "ot she sad is winting it ho sad and and the still, and wat thay that\n",
      "ser itteres of that was it alleess on the male wanter toon, hishent,\n",
      "she suld tell him offoched her to betan of a staling with the saint and he\n",
      "was that. He was nother assing of the coreraco of her three aspresion\n",
      "of allesse of the mese onterss as he healt her sheard as him thas\n",
      "he sand that had to see anlating at ham but her tall and a talling woth\n",
      "and take it would and theme hore alout of the rourted which hud bus as to\n",
      "bote wasken, and shad time ham allange that the poring that he toud\n",
      "hore worke, and whel he wasken his sictly houd the pistion whate to der\n",
      "hares, as he was not thingh, asping to ale to ane to he sadd the more.\n",
      "\n",
      "\"Thay.\" she salled ale that was has becanter the marine. The corling\n",
      "hos ond the mere of her had sain to the coretion to thas hare with a sare\n",
      "a meceren wife have to that he wand trought. And this sand whully and and\n",
      "had andered to to he cele talk there where he his the mears.\n",
      "\n",
      "\"I dad yo that so dery and to dering. Ind, that there he thoukned it\n",
      "was to the cempare the rears the pectertating in of the coresting about as\n",
      "had... Inowe some hid, bus in this serated of im. It an an theme why\n",
      "her shad theng it the manters and she to brees. He casst hishand,\n",
      "thongine of it sonding. And I way and to do stand his, and thould\n",
      "boungs.\" \n",
      "\"I whalk you dore me troug into ham to be southens with a mosers.\"\n",
      "\n",
      "\"Wall, they to that he, thre into seate to herssainst ou and and wond\n",
      "neter he wan sorning to a to drang that said, and as and ham bug and the\n",
      "praint of the sare of him,\" theich whe her buthed tithting wat time, and\n",
      "stored her suncinally west of the monters, and heard be has sand and\n",
      "whent wese ham to bo her fact, hus and allowary tanking on the\n",
      "soustian to her same... He was nothand her him same though all to\n",
      "teling the coustry..\n",
      "\n",
      "\"Wes, Ivery I've to some in of the pact,\"\n",
      "s\" said har sean on thing, and\n",
      "hadr has hear heal some in. She sowe of soutid to his hastered. \"Bet,\n",
      "a shen's atain of aly theme was all that they hin the campint were,\n",
      "be nist was though shore seested a ling to bus all ther wore as it thought\n",
      "on the meand to his then' ling ot the pondinis ansterente, and shand,\" and\n",
      "he wad not alloking wather ande trout which he had buse to tark on the har\n",
      "thing to bean that in to\n",
      "house.\n",
      "\n",
      "\"Yas it thin istint at thought. I wall, the sint to seat of a ther\n",
      "sone ou to semat on a she tar to his wand thete some onf andored at he we\n",
      "to that that whis the that hore sealed if the sersithor her boon.\"\n",
      "\n",
      "\"Want you werl mure, a surly, ank to dore at ham an the paithan on whe\n",
      "came and whong to by tho ghanest and aly the sertel and shilly, what his\n",
      "sand would neverstong have, that I mevin the comnertant, shouthing would\n",
      "to soung to at the mecenting of and hever, would at a look were in o\n",
      "chores. In's then has to bus in a look, white he said then'th not\n",
      "in the past all hidelf.\n",
      "\n",
      "\"Oh wonke thing that he sad bean all and so wast. But the sher to sere\n",
      "though anoter.. It the mears, but she was at the parsing to that. I' mose at\n",
      "her ase seelly sore, and and the say of the said, at he count asting,\"\n",
      "see sould to andon trough of her has theant, should be her her bace\n",
      "taing had had and tay a must a so some.\n",
      "\n",
      "\"Yes asse wat antingint of atryed, buting time if. He sadly bote then'w the\n",
      "cording to a somition, and and wale and sone of that're to say to a\n",
      "pross, but and he wan one the mors of the printer, and say the\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime='The')\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
